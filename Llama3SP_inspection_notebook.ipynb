{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama3SP - Model Inspection Notebook\n",
    "\n",
    "#### Hello!  Welcome to the model inspection notebook, we provides the scripts for model loading and model inference on testing data of model training process for all of the models mentioned in our experiments in the paper. \n",
    "\n",
    "##### Attention!!!\n",
    "##### Before interacting with this notebook, you may want to install a few dependencies [HERE](#dependencies).\n",
    "##### Also, make sure to run the [Static Methods](#static-method) cell, then you are good to go\n",
    "#### The models are categorized by the experiment scenario, please follow the link as follows to reach the specific section\n",
    "\n",
    "### 1. [Within Project Models](#within_project)\n",
    "### 2. [Cross Project Models](#cross_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dependencies'></a>\n",
    "## Dependencies Installation\n",
    "#### run the cell below to install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%!pip install transformers\n",
    "%!pip install peft\n",
    "%!pip install torch\n",
    "%!pip install tokenizers\n",
    "%!pip install captum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='static-method'></a>\n",
    "## Static Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from Llama3SP import LlamaForSequenceClassification as Llama3SP\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    XLNetTokenizer,\n",
    "    BertTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Tokenizer Mapping\n",
    "TOKENIZER_MAPPING = {\"#0\": \"llama3\", \"#00\": \"llama3\", \"#000\": \"llama3\",\n",
    "                     \"#2\": \"sp_word_level\", \"#22\": \"sp_word_level\", \"#222\": \"sp_word_level\",\n",
    "                     \"#6\": \"wordpiece_sp\", \"#66\": \"wordpiece_sp\", \"#666\": \"wordpiece_sp\",\n",
    "                     \"#7\": \"sentencepiece_sp\", \"#77\": \"sentencepiece_sp\", \"#777\": \"sentencepiece_sp\"}\n",
    "\n",
    "# pad token ID mapping\n",
    "PAD_TOKEN_ID_MAPPING = {\"llama3\": 128001, \"sp_word_level\": 3, \"wordpiece_sp\": 0, \"sentencepiece_sp\": 0}\n",
    "# static global vars\n",
    "global DYNAMIC_BATCH, DEVICE\n",
    "DYNAMIC_BATCH = True\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    # set up to release cache memory when possible\n",
    "    torch.cuda.empty_cache()\n",
    "    # set up more conservative memory limits  \n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)  # Use only 80% of GPU memory\n",
    "SEQUENCE_LEN = 20\n",
    "# dynamic global vars\n",
    "global PAD_TOKEN_ID, BATCH_SIZE_RATIO, BATCH_SIZE, TOKENIZER, WITHIN_PROJECT, TEXT, KEY, TOK, MODEL, PROJECT_ID\n",
    "PAD_TOKEN_ID = None\n",
    "BATCH_SIZE_RATIO = None\n",
    "BATCH_SIZE = None\n",
    "TOKENIZER = None\n",
    "WITHIN_PROJECT = None\n",
    "TEXT = None\n",
    "KEY = None\n",
    "TOK = None\n",
    "MODEL = None\n",
    "PROJECT_ID = None\n",
    "\n",
    "\n",
    "def tokenization(text_list, path):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL, TOK, PAD_TOKEN_ID\n",
    "    \n",
    "    if TOKENIZER == 'llama3':\n",
    "        print('using llama3 tokenizer!')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        MODEL.config.pad_token_id = MODEL.config.eos_token_id \n",
    "        \n",
    "        # ensure that no sequence exceeds SEQUENCE_LEN\n",
    "        encoded_dict = tokenizer.batch_encode_plus(\n",
    "            text_list,\n",
    "            max_length=SEQUENCE_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        \n",
    "        # Verify dimensions\n",
    "        if encoded_dict['input_ids'].shape[1] > SEQUENCE_LEN:\n",
    "            print(f\"Warning: Truncating sequences to {SEQUENCE_LEN} tokens\")\n",
    "            encoded_dict['input_ids'] = encoded_dict['input_ids'][:, :SEQUENCE_LEN]\n",
    "            if 'attention_mask' in encoded_dict:\n",
    "                encoded_dict['attention_mask'] = encoded_dict['attention_mask'][:, :SEQUENCE_LEN]\n",
    "        \n",
    "        TOK = tokenizer\n",
    "        return encoded_dict\n",
    "    elif TOKENIZER == 'sp_word_level':\n",
    "        print('using word-level tokenizer!')\n",
    "        tokenizer = Tokenizer.from_pretrained(path)\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        tokenizer.pad_token_id = PAD_TOKEN_ID\n",
    "        MODEL.config.pad_token_id = PAD_TOKEN_ID\n",
    "\n",
    "        TOK = tokenizer\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'sentencepiece_sp':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sp_sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        # update some model configs\n",
    "        # must use .cache = False as below or it crashes from my experience\n",
    "        MODEL.config.pad_token_id = tokenizer.pad_token_id\n",
    "        MODEL.config.use_cache = False\n",
    "        MODEL.config.pretraining_tp = 1\n",
    "        return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "    elif TOKENIZER == 'wordpiece_sp':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/sp_word_piece/vocab.txt')\n",
    "\n",
    "        MODEL.config.pad_token_id = tokenizer.pad_token_id\n",
    "        MODEL.config.use_cache = False\n",
    "        MODEL.config.pretraining_tp = 1\n",
    "        return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "    d = {'text': data['title'], 'label': data['storypoint'], 'issuekey': data['issuekey']}\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def load_trained_model(model_id, project_name):\n",
    "    global WITHIN_PROJECT, MODEL, PROJECT_ID\n",
    "\n",
    "    local = False\n",
    "    try:\n",
    "        int(model_id[1:])\n",
    "        local = False\n",
    "    except:\n",
    "        local = True\n",
    "\n",
    "    if local:\n",
    "        print(\"Loading model from local...\")\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "            lora_dropout=0.1,\n",
    "            bias='none',\n",
    "            task_type='SEQ_CLS'\n",
    "        )\n",
    "        HF_MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "        MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "            HF_MODEL_NAME,\n",
    "            quantization_config=quantization_config,\n",
    "            num_labels=1,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto',\n",
    "            low_cpu_mem_usage=True,\n",
    "            pad_token_id=PAD_TOKEN_ID,\n",
    "        )\n",
    "        MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "        MODEL = get_peft_model(MODEL, lora_config)\n",
    "        MODEL.gradient_checkpointing_enable()\n",
    "        MODEL.enable_input_require_grads()\n",
    "\n",
    "        state_dict = torch.load(model_id, map_location=DEVICE, weights_only=True)\n",
    "        MODEL.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    else:\n",
    "        print(\"Loading model from Hugging Face...\")\n",
    "\n",
    "        if WITHIN_PROJECT:\n",
    "            path = \"DEVCamiloSepulveda/\" + model_id[1:] + \"-LLAMA3SP-\" + project_name\n",
    "        else:\n",
    "            path = \"DEVCamiloSepulveda/\" + model_id[1:] + \"-LLAMA3SP-\" + project_name.split(\"_\")[0] + \"-\" + project_name.split(\"_\")[1]\n",
    "        PROJECT_ID = path\n",
    "\n",
    "        # Load the model configuration\n",
    "        config = PeftConfig.from_pretrained(path)\n",
    "\n",
    "        # Load the original base model\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            config.base_model_name_or_path,\n",
    "            num_labels=1,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map='auto'\n",
    "        )\n",
    "\n",
    "        # Load the LoRA adapters\n",
    "        MODEL = PeftModel.from_pretrained(base_model, path)\n",
    "    \n",
    "    return MODEL\n",
    "\n",
    "\n",
    "def prepare_test_dataloader(file_name, model_id, project_name):\n",
    "    global WITHIN_PROJECT, BATCH_SIZE, BATCH_SIZE_RATIO, TEXT, KEY, MODEL\n",
    "\n",
    "    global WITHIN_PROJECT, MODEL\n",
    "    if WITHIN_PROJECT:\n",
    "        path = \"DEVCamiloSepulveda/\" + model_id[1:] + \"-LLAMA3SP-\" + project_name\n",
    "    else:\n",
    "        path = \"DEVCamiloSepulveda/\" + model_id[1:] + \"-LLAMA3SP-\" + project_name.split(\"_\")[0] + \"-\" + project_name.split(\"_\")[1]\n",
    "    path = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "    if WITHIN_PROJECT:\n",
    "        # calculate the batch size\n",
    "        df = prepare_dataframe(file_name)\n",
    "        BATCH_SIZE = min(int(int(len(df['text'][:int(len(df)*0.6)])) * BATCH_SIZE_RATIO), 512)\n",
    "        print(\"Batch Size: \", BATCH_SIZE)\n",
    "        # prepare testing data\n",
    "        test_text = df['text'][int(len(df)*0.8):]\n",
    "        TEXT = test_text\n",
    "        KEY = df['issuekey'][int(len(df)*0.8):]\n",
    "        test_labels = df['label'][int(len(df)*0.8):]\n",
    "        tokens_test = tokenization(test_text.tolist(), path)\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "    else:\n",
    "        # calculate the batch size based on training data\n",
    "        df = prepare_dataframe('sp_dataset/marked_data/' + file_name[0] + '.csv')\n",
    "        BATCH_SIZE = min(int(int(len(df['text'][:int(len(df)*0.6)])) * BATCH_SIZE_RATIO), 512)\n",
    "        # prepare testing data\n",
    "        df = prepare_dataframe('sp_dataset/marked_data/' + file_name[1] + '.csv')\n",
    "        test_text = df['text']\n",
    "        TEXT = test_text\n",
    "        KEY = df['issuekey']\n",
    "        test_labels = df['label']\n",
    "        tokens_test = tokenization(test_text.tolist(), path)\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')         \n",
    "    return test_dataloader\n",
    " \n",
    "\n",
    "def do_inference(trained_model, test_dataloader):\n",
    "    global TEXT, KEY\n",
    "    global XAI\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    try:\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(DEVICE) for t in batch)\n",
    "            b_input_ids, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                logits = trained_model(b_input_ids)\n",
    "            logits = logits['logits'].detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "        # Calculate errors\n",
    "        distance_records = []\n",
    "        for i in range(len(predictions)):\n",
    "            for j in range(len(predictions[i])):\n",
    "                # Calculate absolute difference between prediction and true value\n",
    "                distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                distance_records.append(distance)\n",
    "        \n",
    "        # Convert to numpy array for efficient calculation\n",
    "        distance_array = np.array(distance_records)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        MAE = np.mean(distance_array)\n",
    "        MdAE = np.median(distance_array)\n",
    "\n",
    "        return MAE, MdAE\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in do_inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def main(model_id, project_name):\n",
    "    global WITHIN_PROJECT, BATCH_SIZE_RATIO, TOKENIZER, PAD_TOKEN_ID, MODEL\n",
    "    # define tokenizer based on model ID\n",
    "    try:\n",
    "        TOKENIZER = TOKENIZER_MAPPING[model_id]\n",
    "    except:\n",
    "        TOKENIZER = 'llama3'\n",
    "    PAD_TOKEN_ID = PAD_TOKEN_ID_MAPPING[TOKENIZER]\n",
    "\n",
    "    if len(project_name.split('_')) == 1:\n",
    "        WITHIN_PROJECT = True\n",
    "        BATCH_SIZE_RATIO = 0.3\n",
    "        print('within project inference using model ' + model_id + ' for project ' + project_name)\n",
    "        file_name = 'sp_dataset/marked_data/' + project_name + '.csv'\n",
    "    else:\n",
    "        WITHIN_PROJECT = False\n",
    "        BATCH_SIZE_RATIO = 0.4\n",
    "        training_project = project_name.split('_')[0]\n",
    "        testing_project = project_name.split('_')[1]\n",
    "        print('cross project inference using model ' + model_id + ' trained on ' + training_project \n",
    "              + ' for project ' + testing_project)\n",
    "        file_name = (training_project, testing_project)\n",
    "    trained_model = load_trained_model(model_id, project_name)\n",
    "    \n",
    "    trained_model.to(DEVICE)\n",
    "    trained_model.eval()\n",
    "    \n",
    "    test_dataloader = prepare_test_dataloader(file_name, model_id, project_name)\n",
    "    predictions = do_inference(trained_model, test_dataloader)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def clean_hf_cache(model_id=None):\n",
    "    \"\"\"\n",
    "    Cleans the cache of files downloaded from Hugging Face.\n",
    "    \n",
    "    Args:\n",
    "        model_id (str, optional): Specific model ID to clean.\n",
    "                                  If not specified, cleans the entire cache.\n",
    "    \"\"\"\n",
    "    # Use the correct cache path\n",
    "    cache_path = os.path.join(str(Path.home()), '.cache', 'huggingface', 'hub')\n",
    "    \n",
    "    if not os.path.exists(cache_path):\n",
    "        print(f\"Cache directory not found at: {cache_path}\")\n",
    "        return\n",
    "        \n",
    "    if model_id:\n",
    "        # If a model is specified, look for its specific directory\n",
    "        model_path = os.path.join(cache_path, 'models--' + model_id.replace('/', '--'))\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"Deleting cache for model: {model_id}\")\n",
    "            try:\n",
    "                shutil.rmtree(model_path)\n",
    "                print(f\"Cache successfully deleted for: {model_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting cache: {e}\")\n",
    "        else:\n",
    "            print(f\"Cache not found for model: {model_id}\")\n",
    "    else:\n",
    "        # Clean the entire cache\n",
    "        print(f\"Deleting all Hugging Face cache at: {cache_path}\")\n",
    "        try:\n",
    "            for item in os.listdir(cache_path):\n",
    "                item_path = os.path.join(cache_path, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    shutil.rmtree(item_path)\n",
    "                else:\n",
    "                    os.remove(item_path)\n",
    "            print(\"Cache completely deleted\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting cache: {e}\")\n",
    "\n",
    "\n",
    "def clean_gpu_memory():\n",
    "    \"\"\"\n",
    "    Clean GPU memory by releasing cache memory and unused tensors\n",
    "    \"\"\"\n",
    "    global MODEL\n",
    "\n",
    "    del MODEL\n",
    "\n",
    "    # Release PyTorch cache memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Release tensor memory\n",
    "    if torch.cuda.is_available():\n",
    "        # Get the current device\n",
    "        device = torch.cuda.current_device()\n",
    "        \n",
    "        # Synchronize the device to ensure all operations are complete\n",
    "        torch.cuda.synchronize(device)\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Release all tensors assigned to the device\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reset all CUDA devices\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        torch.cuda.reset_accumulated_memory_stats(device)\n",
    "\n",
    "\n",
    "def local_model_inference(models):\n",
    "    maes = []\n",
    "    for i, model in enumerate(models):\n",
    "        train_project = model['train']\n",
    "        test_project = model['test']\n",
    "        model_name = f\"{train_project}_{test_project}\"\n",
    "        # Open the file in results to upload the model\n",
    "        with open(f\"./results/{train_project}_{test_project}.txt\", \"r\") as f:\n",
    "            model_results = f.read()\n",
    "            mae, mdae, training_time, epochs, batch_size = (\n",
    "                float(re.search(r\"MAE:\\s*([\\d.]+)\", model_results).group(1)),\n",
    "                float(re.search(r\"MdAE:\\s*([\\d.]+)\", model_results).group(1)),\n",
    "                float(re.search(r\"training time:\\s*([\\d.]+)\", model_results).group(1)),\n",
    "                int(re.search(r\"Epochs:\\s*(\\d+)\", model_results).group(1)),\n",
    "                int(re.search(r\"batch size:\\s*(\\d+)\", model_results).group(1))\n",
    "            )\n",
    "\n",
    "        mae, mdae = main(f\"./models/{model_name}_epo_{epochs}\", train_project)\n",
    "        mae = round(mae, 2)\n",
    "        maes.append(mae)\n",
    "\n",
    "        print(f\"Model: {model_name}, MAE: {mae}, MdAE: {mdae}\")\n",
    "\n",
    "        clean_gpu_memory()\n",
    "    return maes\n",
    "\n",
    "\n",
    "def create_df(maes, models):\n",
    "    data = [\n",
    "        {'Project': train_proj, 'MAE': mae} \n",
    "        if (train_proj := model['train']) == (test_proj := model['test'])\n",
    "        else {'Train': train_proj, 'Test': test_proj, 'MAE': mae}\n",
    "        for model, mae in zip(models, maes)\n",
    "    ]\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)\n",
    "\n",
    "PROJECTS = [\n",
    "    [\n",
    "        {'train': 'appceleratorstudio', 'test': 'appceleratorstudio'},\n",
    "        {'train': 'aptanastudio', 'test': 'aptanastudio'},\n",
    "        {'train': 'bamboo', 'test': 'bamboo'},\n",
    "        {'train': 'clover', 'test': 'clover'},\n",
    "        {'train': 'datamanagement', 'test': 'datamanagement'},\n",
    "        {'train': 'duracloud', 'test': 'duracloud'},\n",
    "        {'train': 'jirasoftware', 'test': 'jirasoftware'},\n",
    "        {'train': 'mesos', 'test': 'mesos'},\n",
    "        {'train': 'moodle', 'test': 'moodle'},\n",
    "        {'train': 'mule', 'test': 'mule'},\n",
    "        {'train': 'mulestudio', 'test': 'mulestudio'},\n",
    "        {'train': 'springxd', 'test': 'springxd'},\n",
    "        {'train': 'talenddataquality', 'test': 'talenddataquality'},\n",
    "        {'train': 'talendesb', 'test': 'talendesb'},\n",
    "        {'train': 'titanium', 'test': 'titanium'},\n",
    "        {'train': 'usergrid', 'test': 'usergrid'},\n",
    "    ],\n",
    "    [\n",
    "        {'train': 'mesos', 'test': 'usergrid'},\n",
    "        {'train': 'usergrid', 'test': 'mesos'},\n",
    "        {'train': 'appceleratorstudio', 'test': 'aptanastudio'},\n",
    "        {'train': 'appceleratorstudio', 'test': 'titanium'},\n",
    "        {'train': 'titanium', 'test': 'appceleratorstudio'},\n",
    "        {'train': 'aptanastudio', 'test': 'titanium'},\n",
    "        {'train': 'mule', 'test': 'mulestudio'},\n",
    "        {'train': 'mulestudio', 'test': 'mule'}\n",
    "    ],\n",
    "    [\n",
    "        {'train': 'clover', 'test': 'usergrid'},\n",
    "        {'train': 'talendesb', 'test': 'mesos'},\n",
    "        {'train': 'talenddataquality', 'test': 'aptanastudio'},\n",
    "        {'train': 'mule', 'test': 'titanium'},\n",
    "        {'train': 'talenddataquality', 'test': 'appceleratorstudio'},\n",
    "        {'train': 'mulestudio', 'test': 'titanium'},\n",
    "        {'train': 'appceleratorstudio', 'test': 'mulestudio'},\n",
    "        {'train': 'appceleratorstudio', 'test': 'mule'}\n",
    "    ]\n",
    "]\n",
    "\n",
    "MODELS = {\n",
    "    \"#0\": \"Llama3.2\",\n",
    "    \"#2\": \"Llama3.2+SPWordLevel\",\n",
    "    \"#6\": \"Llama3.2+SPWordPiece\",\n",
    "    \"#7\": \"Llama3.2+SPSentencePiece\"\n",
    "}\n",
    "\n",
    "WITHIN_PROJECTS = PROJECTS[0]\n",
    "CROSS_PROJECTS = PROJECTS[1:]\n",
    "ALL_CROSS_PROJECTS = [item for sublist in PROJECTS[1:] for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='within_project'></a>\n",
    "## Within Projects Models\n",
    "\n",
    "#### There are two parts under Within Project Model section, follow the link to reach the section:\n",
    "#### 1. [Training Process Inspection](#within_project_tb)\n",
    "#### 2. [Model Testing](#within_project_model_testing)\n",
    "\n",
    "#### Different models are available for cross project estimation as follows: \n",
    "\n",
    "#### #0 - Llama3.2 Auto Tokenizer + Llama3.2\n",
    "#### #2 - Word-level Story Point Tokenizer + Llama3.2\n",
    "#### #6 - WordPiece Story Point Tokenizer + Llama3.2\n",
    "#### #7 - SentencePiece Story Point Tokenizer + Llama3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='within_project_model_testing'></a>\n",
    "### Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the cell below to do inference on all testing datasets using all the uploaded models on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\AppData\\Local\\Temp\\ipykernel_4704\\1479069591.py:1: FutureWarning: Could not cast to float64, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
      "  llama3_within_df = pd.DataFrame({'Project': [item['train'] for item in WITHIN_PROJECTS]}, dtype='float64')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for project: appceleratorstudio\n",
      "within project inference using model #0 for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  525\n",
      "using llama3 tokenizer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\AppData\\Local\\Temp\\ipykernel_4704\\1818555241.py:246: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_seq = torch.tensor(tokens_test['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for project appceleratorstudio: 1.650390625\n",
      "Running inference for project: aptanastudio\n",
      "within project inference using model #0 for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  149\n",
      "using llama3 tokenizer!\n",
      "MAE for project aptanastudio: 3.740234375\n",
      "Running inference for project: bamboo\n",
      "within project inference using model #0 for project bamboo\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  93\n",
      "using llama3 tokenizer!\n",
      "MAE for project bamboo: 1.099609375\n",
      "Running inference for project: clover\n",
      "within project inference using model #0 for project clover\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  69\n",
      "using llama3 tokenizer!\n",
      "MAE for project clover: 4.078125\n",
      "Running inference for project: datamanagement\n",
      "within project inference using model #0 for project datamanagement\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  840\n",
      "using llama3 tokenizer!\n",
      "MAE for project datamanagement: 6.44921875\n",
      "Running inference for project: duracloud\n",
      "within project inference using model #0 for project duracloud\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  119\n",
      "using llama3 tokenizer!\n",
      "MAE for project duracloud: 1.0498046875\n",
      "Running inference for project: jirasoftware\n",
      "within project inference using model #0 for project jirasoftware\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  63\n",
      "using llama3 tokenizer!\n",
      "MAE for project jirasoftware: 2.05078125\n",
      "Running inference for project: mesos\n",
      "within project inference using model #0 for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  302\n",
      "using llama3 tokenizer!\n",
      "MAE for project mesos: 1.3798828125\n",
      "Running inference for project: moodle\n",
      "within project inference using model #0 for project moodle\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  209\n",
      "using llama3 tokenizer!\n",
      "MAE for project moodle: 11.609375\n",
      "Running inference for project: mule\n",
      "within project inference using model #0 for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  159\n",
      "using llama3 tokenizer!\n",
      "MAE for project mule: 2.650390625\n",
      "Running inference for project: mulestudio\n",
      "within project inference using model #0 for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  131\n",
      "using llama3 tokenizer!\n",
      "MAE for project mulestudio: 3.69921875\n",
      "Running inference for project: springxd\n",
      "within project inference using model #0 for project springxd\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  634\n",
      "using llama3 tokenizer!\n",
      "MAE for project springxd: 2.0703125\n",
      "Running inference for project: talenddataquality\n",
      "within project inference using model #0 for project talenddataquality\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  248\n",
      "using llama3 tokenizer!\n",
      "MAE for project talenddataquality: 3.7890625\n",
      "Running inference for project: talendesb\n",
      "within project inference using model #0 for project talendesb\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  156\n",
      "using llama3 tokenizer!\n",
      "MAE for project talendesb: 1.080078125\n",
      "Running inference for project: titanium\n",
      "within project inference using model #0 for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  405\n",
      "using llama3 tokenizer!\n",
      "MAE for project titanium: 2.4609375\n",
      "Running inference for project: usergrid\n",
      "within project inference using model #0 for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  86\n",
      "using llama3 tokenizer!\n",
      "MAE for project usergrid: 1.509765625\n",
      "Running inference for project: appceleratorstudio\n",
      "within project inference using model #2 for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  525\n",
      "using word-level tokenizer!\n",
      "MAE for project appceleratorstudio: 1.5\n",
      "Running inference for project: aptanastudio\n",
      "within project inference using model #2 for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  149\n",
      "using word-level tokenizer!\n",
      "MAE for project aptanastudio: 3.759765625\n",
      "Running inference for project: bamboo\n",
      "within project inference using model #2 for project bamboo\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  93\n",
      "using word-level tokenizer!\n",
      "MAE for project bamboo: 1.4296875\n",
      "Running inference for project: clover\n",
      "within project inference using model #2 for project clover\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  69\n",
      "using word-level tokenizer!\n",
      "MAE for project clover: 4.8984375\n",
      "Running inference for project: datamanagement\n",
      "within project inference using model #2 for project datamanagement\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  840\n",
      "using word-level tokenizer!\n",
      "MAE for project datamanagement: 16.78125\n",
      "Running inference for project: duracloud\n",
      "within project inference using model #2 for project duracloud\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  119\n",
      "using word-level tokenizer!\n",
      "MAE for project duracloud: 1.3603515625\n",
      "Running inference for project: jirasoftware\n",
      "within project inference using model #2 for project jirasoftware\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  63\n",
      "using word-level tokenizer!\n",
      "MAE for project jirasoftware: 2.3203125\n",
      "Running inference for project: mesos\n",
      "within project inference using model #2 for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  302\n",
      "using word-level tokenizer!\n",
      "MAE for project mesos: 1.509765625\n",
      "Running inference for project: moodle\n",
      "within project inference using model #2 for project moodle\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  209\n",
      "using word-level tokenizer!\n",
      "MAE for project moodle: 11.640625\n",
      "Running inference for project: mule\n",
      "within project inference using model #2 for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  159\n",
      "using word-level tokenizer!\n",
      "MAE for project mule: 2.689453125\n",
      "Running inference for project: mulestudio\n",
      "within project inference using model #2 for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  131\n",
      "using word-level tokenizer!\n",
      "MAE for project mulestudio: 3.560546875\n",
      "Running inference for project: springxd\n",
      "within project inference using model #2 for project springxd\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  634\n",
      "using word-level tokenizer!\n",
      "MAE for project springxd: 1.9296875\n",
      "Running inference for project: talenddataquality\n",
      "within project inference using model #2 for project talenddataquality\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  248\n",
      "using word-level tokenizer!\n",
      "MAE for project talenddataquality: 4.671875\n",
      "Running inference for project: talendesb\n",
      "within project inference using model #2 for project talendesb\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  156\n",
      "using word-level tokenizer!\n",
      "MAE for project talendesb: 1.7099609375\n",
      "Running inference for project: titanium\n",
      "within project inference using model #2 for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  405\n",
      "using word-level tokenizer!\n",
      "MAE for project titanium: 2.33984375\n",
      "Running inference for project: usergrid\n",
      "within project inference using model #2 for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  86\n",
      "using word-level tokenizer!\n",
      "MAE for project usergrid: 1.51953125\n",
      "Running inference for project: appceleratorstudio\n",
      "within project inference using model #6 for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  525\n",
      "using wordpiece tokenizer!\n",
      "MAE for project appceleratorstudio: 1.669921875\n",
      "Running inference for project: aptanastudio\n",
      "within project inference using model #6 for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  149\n",
      "using wordpiece tokenizer!\n",
      "MAE for project aptanastudio: 4.0390625\n",
      "Running inference for project: bamboo\n",
      "within project inference using model #6 for project bamboo\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  93\n",
      "using wordpiece tokenizer!\n",
      "MAE for project bamboo: 1.169921875\n",
      "Running inference for project: clover\n",
      "within project inference using model #6 for project clover\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  69\n",
      "using wordpiece tokenizer!\n",
      "MAE for project clover: 3.83984375\n",
      "Running inference for project: datamanagement\n",
      "within project inference using model #6 for project datamanagement\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  840\n",
      "using wordpiece tokenizer!\n",
      "MAE for project datamanagement: 6.78125\n",
      "Running inference for project: duracloud\n",
      "within project inference using model #6 for project duracloud\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  119\n",
      "using wordpiece tokenizer!\n",
      "MAE for project duracloud: 1.2900390625\n",
      "Running inference for project: jirasoftware\n",
      "within project inference using model #6 for project jirasoftware\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  63\n",
      "using wordpiece tokenizer!\n",
      "MAE for project jirasoftware: 2.130859375\n",
      "Running inference for project: mesos\n",
      "within project inference using model #6 for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  302\n",
      "using wordpiece tokenizer!\n",
      "MAE for project mesos: 1.1904296875\n",
      "Running inference for project: moodle\n",
      "within project inference using model #6 for project moodle\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  209\n",
      "using wordpiece tokenizer!\n",
      "MAE for project moodle: 13.796875\n",
      "Running inference for project: mule\n",
      "within project inference using model #6 for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  159\n",
      "using wordpiece tokenizer!\n",
      "MAE for project mule: 2.80078125\n",
      "Running inference for project: mulestudio\n",
      "within project inference using model #6 for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  131\n",
      "using wordpiece tokenizer!\n",
      "MAE for project mulestudio: 3.83984375\n",
      "Running inference for project: springxd\n",
      "within project inference using model #6 for project springxd\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  634\n",
      "using wordpiece tokenizer!\n",
      "MAE for project springxd: 1.849609375\n",
      "Running inference for project: talenddataquality\n",
      "within project inference using model #6 for project talenddataquality\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  248\n",
      "using wordpiece tokenizer!\n",
      "MAE for project talenddataquality: 3.759765625\n",
      "Running inference for project: talendesb\n",
      "within project inference using model #6 for project talendesb\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  156\n",
      "using wordpiece tokenizer!\n",
      "MAE for project talendesb: 1.0498046875\n",
      "Running inference for project: titanium\n",
      "within project inference using model #6 for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  405\n",
      "using wordpiece tokenizer!\n",
      "MAE for project titanium: 2.470703125\n",
      "Running inference for project: usergrid\n",
      "within project inference using model #6 for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  86\n",
      "using wordpiece tokenizer!\n",
      "MAE for project usergrid: 1.3701171875\n",
      "Running inference for project: appceleratorstudio\n",
      "within project inference using model #7 for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  525\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project appceleratorstudio: 1.6396484375\n",
      "Running inference for project: aptanastudio\n",
      "within project inference using model #7 for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  149\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project aptanastudio: 3.740234375\n",
      "Running inference for project: bamboo\n",
      "within project inference using model #7 for project bamboo\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  93\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project bamboo: 0.97021484375\n",
      "Running inference for project: clover\n",
      "within project inference using model #7 for project clover\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  69\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project clover: 3.8203125\n",
      "Running inference for project: datamanagement\n",
      "within project inference using model #7 for project datamanagement\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  840\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project datamanagement: 7.171875\n",
      "Running inference for project: duracloud\n",
      "within project inference using model #7 for project duracloud\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  119\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project duracloud: 1.099609375\n",
      "Running inference for project: jirasoftware\n",
      "within project inference using model #7 for project jirasoftware\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  63\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project jirasoftware: 1.900390625\n",
      "Running inference for project: mesos\n",
      "within project inference using model #7 for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  302\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project mesos: 1.2900390625\n",
      "Running inference for project: moodle\n",
      "within project inference using model #7 for project moodle\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  209\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project moodle: 11.21875\n",
      "Running inference for project: mule\n",
      "within project inference using model #7 for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  159\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project mule: 2.509765625\n",
      "Running inference for project: mulestudio\n",
      "within project inference using model #7 for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  131\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project mulestudio: 3.810546875\n",
      "Running inference for project: springxd\n",
      "within project inference using model #7 for project springxd\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  634\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project springxd: 1.7001953125\n",
      "Running inference for project: talenddataquality\n",
      "within project inference using model #7 for project talenddataquality\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  248\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project talenddataquality: 3.919921875\n",
      "Running inference for project: talendesb\n",
      "within project inference using model #7 for project talendesb\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  156\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project talendesb: 1.0\n",
      "Running inference for project: titanium\n",
      "within project inference using model #7 for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  405\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project titanium: 2.3203125\n",
      "Running inference for project: usergrid\n",
      "within project inference using model #7 for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  86\n",
      "using sentencepiece tokenizer!\n",
      "MAE for project usergrid: 1.3701171875\n",
      "All projects processed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project</th>\n",
       "      <th>Llama3.2</th>\n",
       "      <th>Llama3.2+SPWordLevel</th>\n",
       "      <th>Llama3.2+SPWordPiece</th>\n",
       "      <th>Llama3.2+SPSentencePiece</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>1.65</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.76</td>\n",
       "      <td>4.04</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bamboo</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clover</td>\n",
       "      <td>4.08</td>\n",
       "      <td>4.90</td>\n",
       "      <td>3.84</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datamanagement</td>\n",
       "      <td>6.45</td>\n",
       "      <td>16.78</td>\n",
       "      <td>6.78</td>\n",
       "      <td>7.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>duracloud</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jirasoftware</td>\n",
       "      <td>2.05</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mesos</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>moodle</td>\n",
       "      <td>11.61</td>\n",
       "      <td>11.64</td>\n",
       "      <td>13.80</td>\n",
       "      <td>11.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mule</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mulestudio</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3.84</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>springxd</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.93</td>\n",
       "      <td>1.85</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>talenddataquality</td>\n",
       "      <td>3.79</td>\n",
       "      <td>4.67</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>talendesb</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>titanium</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>usergrid</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Project  Llama3.2  Llama3.2+SPWordLevel  Llama3.2+SPWordPiece  \\\n",
       "0   appceleratorstudio      1.65                  1.50                  1.67   \n",
       "1         aptanastudio      3.74                  3.76                  4.04   \n",
       "2               bamboo      1.10                  1.43                  1.17   \n",
       "3               clover      4.08                  4.90                  3.84   \n",
       "4       datamanagement      6.45                 16.78                  6.78   \n",
       "5            duracloud      1.05                  1.36                  1.29   \n",
       "6         jirasoftware      2.05                  2.32                  2.13   \n",
       "7                mesos      1.38                  1.51                  1.19   \n",
       "8               moodle     11.61                 11.64                 13.80   \n",
       "9                 mule      2.65                  2.69                  2.80   \n",
       "10          mulestudio      3.70                  3.56                  3.84   \n",
       "11            springxd      2.07                  1.93                  1.85   \n",
       "12   talenddataquality      3.79                  4.67                  3.76   \n",
       "13           talendesb      1.08                  1.71                  1.05   \n",
       "14            titanium      2.46                  2.34                  2.47   \n",
       "15            usergrid      1.51                  1.52                  1.37   \n",
       "\n",
       "    Llama3.2+SPSentencePiece  \n",
       "0                       1.64  \n",
       "1                       3.74  \n",
       "2                       0.97  \n",
       "3                       3.82  \n",
       "4                       7.17  \n",
       "5                       1.10  \n",
       "6                       1.90  \n",
       "7                       1.29  \n",
       "8                      11.22  \n",
       "9                       2.51  \n",
       "10                      3.81  \n",
       "11                      1.70  \n",
       "12                      3.92  \n",
       "13                      1.00  \n",
       "14                      2.32  \n",
       "15                      1.37  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_within_df = pd.DataFrame({'Project': [item['train'] for item in WITHIN_PROJECTS]}, dtype='float64')\n",
    "\n",
    "for model in MODELS:\n",
    "    maes = []\n",
    "    for project in WITHIN_PROJECTS:\n",
    "        print(f\"Running inference for project: {project['train']}\")\n",
    "        mae, mdae = main(model, project['train'])\n",
    "        mae = round(mae, 2)\n",
    "        maes.append(mae)\n",
    "        print(f\"MAE for project {project['train']}: {mae}\")\n",
    "        model_id = f\"DEVCamiloSepulveda/{model[1:]}-LLAMA3SP-{project['train']}\"\n",
    "        # clean_hf_cache(model_id)\n",
    "        clean_gpu_memory()\n",
    "    llama3_within_df[MODELS[model]] = maes\n",
    "    llama3_within_df[MODELS[model]] = llama3_within_df[MODELS[model]].apply(lambda x: round(x, 2))\n",
    "\n",
    "print(\"All projects processed\")\n",
    "\n",
    "llama3_within_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional: Save the results\n",
    "\n",
    "You can save the results of the DataFrame to a CSV file for further analysis or record-keeping. Run the following script to save the `llama3_within_df` DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'Llama3_within_results.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "llama3_within_df.to_csv('./data_model_analysis/Llama3_within_results.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'Llama3_within_results.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the cell below to do inference on testing dataset using **local** trained model on all within projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from local on Within projects inference...\n",
      "within project inference using model ./models/appceleratorstudio_appceleratorstudio_epo_1 for project appceleratorstudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  512\n",
      "using llama3 tokenizer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\AppData\\Local\\Temp\\ipykernel_4704\\3307862746.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_seq = torch.tensor(tokens_test['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: appceleratorstudio_appceleratorstudio, MAE: 1.590000033378601, MdAE: 1.3363306522369385\n",
      "within project inference using model ./models/aptanastudio_aptanastudio_epo_12 for project aptanastudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  149\n",
      "using llama3 tokenizer!\n",
      "Model: aptanastudio_aptanastudio, MAE: 3.890000104904175, MdAE: 3.0960545539855957\n",
      "within project inference using model ./models/bamboo_bamboo_epo_17 for project bamboo\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  93\n",
      "using llama3 tokenizer!\n",
      "Model: bamboo_bamboo, MAE: 1.1100000143051147, MdAE: 1.0073730945587158\n",
      "within project inference using model ./models/clover_clover_epo_2 for project clover\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  69\n",
      "using llama3 tokenizer!\n",
      "Model: clover_clover, MAE: 3.990000009536743, MdAE: 2.606503963470459\n",
      "within project inference using model ./models/datamanagement_datamanagement_epo_1 for project datamanagement\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  512\n",
      "using llama3 tokenizer!\n",
      "Model: datamanagement_datamanagement, MAE: 5.840000152587891, MdAE: 2.9360532760620117\n",
      "within project inference using model ./models/duracloud_duracloud_epo_3 for project duracloud\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  119\n",
      "using llama3 tokenizer!\n",
      "Model: duracloud_duracloud, MAE: 1.0099999904632568, MdAE: 0.7023202776908875\n",
      "within project inference using model ./models/jirasoftware_jirasoftware_epo_3 for project jirasoftware\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  63\n",
      "using llama3 tokenizer!\n",
      "Model: jirasoftware_jirasoftware, MAE: 2.380000114440918, MdAE: 1.8219866752624512\n",
      "within project inference using model ./models/mesos_mesos_epo_0 for project mesos\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  302\n",
      "using llama3 tokenizer!\n",
      "Model: mesos_mesos, MAE: 1.2699999809265137, MdAE: 1.0253846645355225\n",
      "within project inference using model ./models/moodle_moodle_epo_17 for project moodle\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  209\n",
      "using llama3 tokenizer!\n",
      "Model: moodle_moodle, MAE: 13.899999618530273, MdAE: 12.567741394042969\n",
      "within project inference using model ./models/mule_mule_epo_15 for project mule\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  159\n",
      "using llama3 tokenizer!\n",
      "Model: mule_mule, MAE: 2.759999990463257, MdAE: 2.5467729568481445\n",
      "within project inference using model ./models/mulestudio_mulestudio_epo_1 for project mulestudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  131\n",
      "using llama3 tokenizer!\n",
      "Model: mulestudio_mulestudio, MAE: 3.9100000858306885, MdAE: 3.3148765563964844\n",
      "within project inference using model ./models/springxd_springxd_epo_6 for project springxd\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  512\n",
      "using llama3 tokenizer!\n",
      "Model: springxd_springxd, MAE: 1.8700000047683716, MdAE: 1.6935579776763916\n",
      "within project inference using model ./models/talenddataquality_talenddataquality_epo_7 for project talenddataquality\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  248\n",
      "using llama3 tokenizer!\n",
      "Model: talenddataquality_talenddataquality, MAE: 4.539999961853027, MdAE: 4.284450531005859\n",
      "within project inference using model ./models/talendesb_talendesb_epo_7 for project talendesb\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  156\n",
      "using llama3 tokenizer!\n",
      "Model: talendesb_talendesb, MAE: 1.059999942779541, MdAE: 0.805655300617218\n",
      "within project inference using model ./models/titanium_titanium_epo_1 for project titanium\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  405\n",
      "using llama3 tokenizer!\n",
      "Model: titanium_titanium, MAE: 2.5999999046325684, MdAE: 1.9763915538787842\n",
      "within project inference using model ./models/usergrid_usergrid_epo_14 for project usergrid\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  86\n",
      "using llama3 tokenizer!\n",
      "Model: usergrid_usergrid, MAE: 1.4700000286102295, MdAE: 1.0582633018493652\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bamboo</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clover</td>\n",
       "      <td>3.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datamanagement</td>\n",
       "      <td>5.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>duracloud</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jirasoftware</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mesos</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>moodle</td>\n",
       "      <td>13.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mule</td>\n",
       "      <td>2.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mulestudio</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>springxd</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>talenddataquality</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>talendesb</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>titanium</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>usergrid</td>\n",
       "      <td>1.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Project    MAE\n",
       "0   appceleratorstudio   1.59\n",
       "1         aptanastudio   3.89\n",
       "2               bamboo   1.11\n",
       "3               clover   3.99\n",
       "4       datamanagement   5.84\n",
       "5            duracloud   1.01\n",
       "6         jirasoftware   2.38\n",
       "7                mesos   1.27\n",
       "8               moodle  13.90\n",
       "9                 mule   2.76\n",
       "10          mulestudio   3.91\n",
       "11            springxd   1.87\n",
       "12   talenddataquality   4.54\n",
       "13           talendesb   1.06\n",
       "14            titanium   2.60\n",
       "15            usergrid   1.47"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading models from local on Within projects inference...\")\n",
    "        \n",
    "within_maes = local_model_inference(WITHIN_PROJECTS)\n",
    "within_df = create_df(within_maes, WITHIN_PROJECTS)\n",
    "\n",
    "within_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional: Save the results\n",
    "\n",
    "You can save the results of the DataFrame to a CSV file for further analysis or record-keeping. Run the following script to save the `within_df` DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'within_project_results.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "within_df.to_csv('./data_model_analysis/Llama3SP_within_results.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'within_project_results.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross_project'></a>\n",
    "## Cross Projects Models\n",
    "\n",
    "#### There are two parts under Cross Project Model section, follow the link to reach the section:\n",
    "#### 1. [Training Process Inspection](#cross_project_tb)\n",
    "#### 2. [Model Testing](#cross_project_model_testing)\n",
    "\n",
    "##### Different models are available for cross project estimation as follows:\n",
    "\n",
    "### Cross project - within repository models\n",
    "#### #00 - Llama3.2 Auto Tokenizer\n",
    "#### #22 - Word-level Story Point Tokenizer + Llama3.2\n",
    "#### #66 - WordPiece Story Point Tokenizer + Llama3.2\n",
    "#### #77 - SentencePiece Story Point Tokenizer + Llama3.2  \n",
    "\n",
    "### Cross project - cross repository models\n",
    "#### #000 - Llama3.2 Auto Tokenizer\n",
    "#### #222 - Word-level Story Point Tokenizer + Llama3.2\n",
    "#### #666 - WordPiece Story Point Tokenizer + Llama3.2\n",
    "#### #777 - SentencePiece Story Point Tokenizer + Llama3.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross_project_model_testing'></a>\n",
    "### Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the cell below to do inference on all testing datasets using all the uploaded models on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\AppData\\Local\\Temp\\ipykernel_5236\\3501689558.py:1: FutureWarning: Could not cast to float64, falling back to object. This behavior is deprecated. In a future version, when a dtype is passed to 'DataFrame', either all columns will be cast to that dtype, or a TypeError will be raised\n",
      "  llama3_cross_df = pd.DataFrame(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for project trained on mesos and tested on usergrid\n",
      "cross project inference using model #00 trained on mesos for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\AppData\\Local\\Temp\\ipykernel_5236\\533683151.py:259: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_seq = torch.tensor(tokens_test['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for project mesos: 1.33984375\n",
      "Running inference for project trained on usergrid and tested on mesos\n",
      "cross project inference using model #00 trained on usergrid for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project usergrid: 1.740234375\n",
      "Running inference for project trained on appceleratorstudio and tested on aptanastudio\n",
      "cross project inference using model #00 trained on appceleratorstudio for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project appceleratorstudio: 4.359375\n",
      "Running inference for project trained on appceleratorstudio and tested on titanium\n",
      "cross project inference using model #00 trained on appceleratorstudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project appceleratorstudio: 3.359375\n",
      "Running inference for project trained on titanium and tested on appceleratorstudio\n",
      "cross project inference using model #00 trained on titanium for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project titanium: 2.55078125\n",
      "Running inference for project trained on aptanastudio and tested on titanium\n",
      "cross project inference using model #00 trained on aptanastudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project aptanastudio: 3.779296875\n",
      "Running inference for project trained on mule and tested on mulestudio\n",
      "cross project inference using model #00 trained on mule for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project mule: 3.5703125\n",
      "Running inference for project trained on mulestudio and tested on mule\n",
      "cross project inference using model #00 trained on mulestudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project mulestudio: 2.98046875\n",
      "Running inference for project trained on clover and tested on usergrid\n",
      "cross project inference using model #000 trained on clover for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project clover: 2.01953125\n",
      "Running inference for project trained on talendesb and tested on mesos\n",
      "cross project inference using model #000 trained on talendesb for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project talendesb: 1.51953125\n",
      "Running inference for project trained on talenddataquality and tested on aptanastudio\n",
      "cross project inference using model #000 trained on talenddataquality for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project talenddataquality: 4.53125\n",
      "Running inference for project trained on mule and tested on titanium\n",
      "cross project inference using model #000 trained on mule for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project mule: 3.48046875\n",
      "Running inference for project trained on talenddataquality and tested on appceleratorstudio\n",
      "cross project inference using model #000 trained on talenddataquality for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project talenddataquality: 2.720703125\n",
      "Running inference for project trained on mulestudio and tested on titanium\n",
      "cross project inference using model #000 trained on mulestudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project mulestudio: 3.73046875\n",
      "Running inference for project trained on appceleratorstudio and tested on mulestudio\n",
      "cross project inference using model #000 trained on appceleratorstudio for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project appceleratorstudio: 3.41015625\n",
      "Running inference for project trained on appceleratorstudio and tested on mule\n",
      "cross project inference using model #000 trained on appceleratorstudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using llama3 tokenizer!\n",
      "MAE for project appceleratorstudio: 2.900390625\n",
      "Running inference for project trained on mesos and tested on usergrid\n",
      "cross project inference using model #22 trained on mesos for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54a7554b0ca427f87636e7c770e3d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159392a0bd00428eb398763f4f138a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project mesos: 1.400390625\n",
      "Running inference for project trained on usergrid and tested on mesos\n",
      "cross project inference using model #22 trained on usergrid for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdaf5be7016443c8bdd42fb77c9615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee4fa0283c549f8babde06eaad57dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project usergrid: 1.73046875\n",
      "Running inference for project trained on appceleratorstudio and tested on aptanastudio\n",
      "cross project inference using model #22 trained on appceleratorstudio for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf955f6a2911421da955fbae6bd621bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d938e430d424dc49bde0698651a6a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project appceleratorstudio: 5.21875\n",
      "Running inference for project trained on appceleratorstudio and tested on titanium\n",
      "cross project inference using model #22 trained on appceleratorstudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190f197f00c54d1081cba195e8fb7889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d136d3a372a4473b85a0df6fa72c727c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project appceleratorstudio: 3.759765625\n",
      "Running inference for project trained on titanium and tested on appceleratorstudio\n",
      "cross project inference using model #22 trained on titanium for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d178000b8ab74fd088c281a461527317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad48adade35849b8b16ac74b16d52aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project titanium: 2.640625\n",
      "Running inference for project trained on aptanastudio and tested on titanium\n",
      "cross project inference using model #22 trained on aptanastudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99745ac8076d41a8a70b165f73b31987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523f24d430f94fee8811183b3c484286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project aptanastudio: 3.640625\n",
      "Running inference for project trained on mule and tested on mulestudio\n",
      "cross project inference using model #22 trained on mule for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5050109a464b9388031c4bef82caf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39a7da67277499791955dc49d622670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project mule: 4.0703125\n",
      "Running inference for project trained on mulestudio and tested on mule\n",
      "cross project inference using model #22 trained on mulestudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f9d345cd6d4409970b37d43345da05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bdbf2485c84f81ae2348d53ec82ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project mulestudio: 3.0703125\n",
      "Running inference for project trained on clover and tested on usergrid\n",
      "cross project inference using model #222 trained on clover for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb60299547c40e6bdf0ab4ca99c26ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a887efd2bfb442ab1d6636962cf2587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project clover: 2.529296875\n",
      "Running inference for project trained on talendesb and tested on mesos\n",
      "cross project inference using model #222 trained on talendesb for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c7fc5458d94c5fa34dfc58522beca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d217ccd8b0c4fb5a305f55f08abe8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project talendesb: 1.9501953125\n",
      "Running inference for project trained on talenddataquality and tested on aptanastudio\n",
      "cross project inference using model #222 trained on talenddataquality for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17aaf892a91d460e8dffdf51e227492d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba05cdc4ab3d497f867b3e511af76fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project talenddataquality: 4.69140625\n",
      "Running inference for project trained on mule and tested on titanium\n",
      "cross project inference using model #222 trained on mule for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5c9f5b7e6442db945ef63c06dcbd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae3f21249ce4a688bfa36bbdbae9225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project mule: 3.6796875\n",
      "Running inference for project trained on talenddataquality and tested on appceleratorstudio\n",
      "cross project inference using model #222 trained on talenddataquality for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2047957ada0b4629a240e0efb497eea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b4b393f83a4eceb8e9ba5a60462900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project talenddataquality: 2.66015625\n",
      "Running inference for project trained on mulestudio and tested on titanium\n",
      "cross project inference using model #222 trained on mulestudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79473f003e846178753899be441b506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91088fd1f5b34a42881c85c5c07a8177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project mulestudio: 3.73046875\n",
      "Running inference for project trained on appceleratorstudio and tested on mulestudio\n",
      "cross project inference using model #222 trained on appceleratorstudio for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed43260cc7934539aace8e690237c077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dc82f7d47842ac862da0427b594512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project appceleratorstudio: 3.5390625\n",
      "Running inference for project trained on appceleratorstudio and tested on mule\n",
      "cross project inference using model #222 trained on appceleratorstudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6461b4b82c534581b80623cb2dc628c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad03ac2e45e4ae897e196914259bc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using word-level tokenizer!\n",
      "MAE for project appceleratorstudio: 2.890625\n",
      "Running inference for project trained on mesos and tested on usergrid\n",
      "cross project inference using model #66 trained on mesos for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf82dcc45349559baaa9b436c138a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70fcef1494542ceb11aa9abb5efc15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project mesos: 1.3798828125\n",
      "Running inference for project trained on usergrid and tested on mesos\n",
      "cross project inference using model #66 trained on usergrid for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045ee2b86de641a5b5d0f759d2607b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810a307f39e441a5a0a8fe34676a03b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project usergrid: 1.599609375\n",
      "Running inference for project trained on appceleratorstudio and tested on aptanastudio\n",
      "cross project inference using model #66 trained on appceleratorstudio for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb722f0863e140789c7dce1c7dd588ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e83afa13f1d45b191c262a7f9d8f6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project appceleratorstudio: 4.28125\n",
      "Running inference for project trained on appceleratorstudio and tested on titanium\n",
      "cross project inference using model #66 trained on appceleratorstudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec84469887d644339efd2f5429d8e158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26da9ac3a8ef4f5f9bd23314ce185e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project appceleratorstudio: 3.400390625\n",
      "Running inference for project trained on titanium and tested on appceleratorstudio\n",
      "cross project inference using model #66 trained on titanium for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227df08b338a481dbb365118cce1f730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216d4a93c89c4bbbb919c6980857ce60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project titanium: 2.359375\n",
      "Running inference for project trained on aptanastudio and tested on titanium\n",
      "cross project inference using model #66 trained on aptanastudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747c20adad4b457b8af749a4bee7c4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129f19ccc3244a969af08057818ad2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project aptanastudio: 4.109375\n",
      "Running inference for project trained on mule and tested on mulestudio\n",
      "cross project inference using model #66 trained on mule for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00046b2d1d4f4d27a7f456ba54bda00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c7e958c31c42ff8de5a41b570ee967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project mule: 3.4296875\n",
      "Running inference for project trained on mulestudio and tested on mule\n",
      "cross project inference using model #66 trained on mulestudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ae1f46cd2b41428e882f5b61b8b1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3abafb3b550425ea1974c2db0596554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project mulestudio: 3.080078125\n",
      "Running inference for project trained on clover and tested on usergrid\n",
      "cross project inference using model #666 trained on clover for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07df3c065dc4180bc298e22e5d47720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23343a202f934db9b708a400e6861d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project clover: 2.0703125\n",
      "Running inference for project trained on talendesb and tested on mesos\n",
      "cross project inference using model #666 trained on talendesb for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a01bcd1e56f41cc87b8aeae43de9be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed115080d5d249e0b9b14f34451f2a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project talendesb: 1.599609375\n",
      "Running inference for project trained on talenddataquality and tested on aptanastudio\n",
      "cross project inference using model #666 trained on talenddataquality for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdef532b9f84a72871385333f4a75ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2614095c6b4d14a10d6a368c3d8057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project talenddataquality: 4.5390625\n",
      "Running inference for project trained on mule and tested on titanium\n",
      "cross project inference using model #666 trained on mule for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4634515a2a7243a8b3d5a56839807390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f77dafa636348b497bf73c374c40a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project mule: 3.33984375\n",
      "Running inference for project trained on talenddataquality and tested on appceleratorstudio\n",
      "cross project inference using model #666 trained on talenddataquality for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d5c5da0ade4555a1025fdad8e376e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78738e9736144424bc7f83756d410695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project talenddataquality: 2.98046875\n",
      "Running inference for project trained on mulestudio and tested on titanium\n",
      "cross project inference using model #666 trained on mulestudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18dce2e83b94dc7bcf4ad2f215530f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b72f35715542be83f144d29cade808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project mulestudio: 3.509765625\n",
      "Running inference for project trained on appceleratorstudio and tested on mulestudio\n",
      "cross project inference using model #666 trained on appceleratorstudio for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "393e2a82055846ce8b1674c86b442257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4b03578f2f49e8886bc194db340819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project appceleratorstudio: 3.390625\n",
      "Running inference for project trained on appceleratorstudio and tested on mule\n",
      "cross project inference using model #666 trained on appceleratorstudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02af1e7a55dc4f54b1e49f12dfafd17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164be8c528cc4215b802b58229791867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using wordpiece tokenizer!\n",
      "MAE for project appceleratorstudio: 3.009765625\n",
      "Running inference for project trained on mesos and tested on usergrid\n",
      "cross project inference using model #77 trained on mesos for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09687fb4f4ea45b6810c1533c5f44af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dacdcb1f83a4733afd9f4e80333120e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project mesos: 1.0498046875\n",
      "Running inference for project trained on usergrid and tested on mesos\n",
      "cross project inference using model #77 trained on usergrid for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7824c1f30f1e4131a3a38606cc5d2464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c03f3bca9ad4d77934ae53162c3b4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project usergrid: 1.5595703125\n",
      "Running inference for project trained on appceleratorstudio and tested on aptanastudio\n",
      "cross project inference using model #77 trained on appceleratorstudio for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6d622ccd6d4bc9ba28ea0b8b15f3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbf6c0f47c743459f8df39b0c97bf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project appceleratorstudio: 4.30859375\n",
      "Running inference for project trained on appceleratorstudio and tested on titanium\n",
      "cross project inference using model #77 trained on appceleratorstudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd44348d1024e53a7f182435add1794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d50aad3f1f43fba1e22002245fe874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project appceleratorstudio: 3.30078125\n",
      "Running inference for project trained on titanium and tested on appceleratorstudio\n",
      "cross project inference using model #77 trained on titanium for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5041a9aedbb4248975c97e7f5542a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62aa0fcc36545bdac120a534f951a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project titanium: 2.359375\n",
      "Running inference for project trained on aptanastudio and tested on titanium\n",
      "cross project inference using model #77 trained on aptanastudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04ff9b889fb44d4a41d43ebca60bda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2751a9cf6ab64ae98f32b22402a91182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project aptanastudio: 3.779296875\n",
      "Running inference for project trained on mule and tested on mulestudio\n",
      "cross project inference using model #77 trained on mule for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72d9b8df3c94aa39f11ed484c1d1247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3377a884920041daa9f701a0c82af4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project mule: 3.650390625\n",
      "Running inference for project trained on mulestudio and tested on mule\n",
      "cross project inference using model #77 trained on mulestudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f588924019ad4ea98791d78c8469aa69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee1e836fecd4291aa2c8f4be1fb2ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project mulestudio: 3.310546875\n",
      "Running inference for project trained on clover and tested on usergrid\n",
      "cross project inference using model #777 trained on clover for project usergrid\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4874c66a134c81990f1b25d542a225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d6c77fd0e545809c73075edea0a2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project clover: 1.48046875\n",
      "Running inference for project trained on talendesb and tested on mesos\n",
      "cross project inference using model #777 trained on talendesb for project mesos\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f4bf33e8d34f9f914f945072d2c6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ab450ac8454f7c80d014cb6939518b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project talendesb: 1.66015625\n",
      "Running inference for project trained on talenddataquality and tested on aptanastudio\n",
      "cross project inference using model #777 trained on talenddataquality for project aptanastudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f16df5da4e47bc89e5c92b19ec784f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0adf34d1a4f4ce7b8b9a9165850640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project talenddataquality: 4.73046875\n",
      "Running inference for project trained on mule and tested on titanium\n",
      "cross project inference using model #777 trained on mule for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56b3cec13e9487ea4560a885276902c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6cabfe874342c98b359478f2ba3a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project mule: 3.310546875\n",
      "Running inference for project trained on talenddataquality and tested on appceleratorstudio\n",
      "cross project inference using model #777 trained on talenddataquality for project appceleratorstudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18da6dc9bb1490ab2146a6a2694bb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b67bea35d544fb584ff74b4ad9d5ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project talenddataquality: 2.490234375\n",
      "Running inference for project trained on mulestudio and tested on titanium\n",
      "cross project inference using model #777 trained on mulestudio for project titanium\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f442ee4223e8428a9ac7cb5638613155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e564bdb08b946e4b6677e1ff057ea57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project mulestudio: 3.970703125\n",
      "Running inference for project trained on appceleratorstudio and tested on mulestudio\n",
      "cross project inference using model #777 trained on appceleratorstudio for project mulestudio\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377a0c05bb2e49f2b418d2374e1daec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735457348eb9420bb37b2f36b51c3b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project appceleratorstudio: 3.529296875\n",
      "Running inference for project trained on appceleratorstudio and tested on mule\n",
      "cross project inference using model #777 trained on appceleratorstudio for project mule\n",
      "Loading model from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999f145baaa0447a9dce9fc75696c836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8509f3b2166949dba06f77ca865afef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using sentencepiece tokenizer!\n",
      "MAE for project appceleratorstudio: 2.76953125\n",
      "All projects processed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>Llama3.2</th>\n",
       "      <th>Llama3.2+SPWordLevel</th>\n",
       "      <th>Llama3.2+SPWordPiece</th>\n",
       "      <th>Llama3.2+SPSentencePiece</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mesos</td>\n",
       "      <td>usergrid</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.38</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usergrid</td>\n",
       "      <td>mesos</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.73</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>4.36</td>\n",
       "      <td>5.22</td>\n",
       "      <td>4.28</td>\n",
       "      <td>4.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>titanium</td>\n",
       "      <td>3.36</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>titanium</td>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>titanium</td>\n",
       "      <td>3.78</td>\n",
       "      <td>3.64</td>\n",
       "      <td>4.11</td>\n",
       "      <td>3.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mule</td>\n",
       "      <td>mulestudio</td>\n",
       "      <td>3.57</td>\n",
       "      <td>4.07</td>\n",
       "      <td>3.43</td>\n",
       "      <td>3.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mulestudio</td>\n",
       "      <td>mule</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.07</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clover</td>\n",
       "      <td>usergrid</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.07</td>\n",
       "      <td>1.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>talendesb</td>\n",
       "      <td>mesos</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>talenddataquality</td>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>4.53</td>\n",
       "      <td>4.69</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mule</td>\n",
       "      <td>titanium</td>\n",
       "      <td>3.48</td>\n",
       "      <td>3.68</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>talenddataquality</td>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.98</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mulestudio</td>\n",
       "      <td>titanium</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.51</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>mulestudio</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.54</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>mule</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.89</td>\n",
       "      <td>3.01</td>\n",
       "      <td>2.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Train                Test  Llama3.2  Llama3.2+SPWordLevel  \\\n",
       "0                mesos            usergrid      1.34                  1.40   \n",
       "1             usergrid               mesos      1.74                  1.73   \n",
       "2   appceleratorstudio        aptanastudio      4.36                  5.22   \n",
       "3   appceleratorstudio            titanium      3.36                  3.76   \n",
       "4             titanium  appceleratorstudio      2.55                  2.64   \n",
       "5         aptanastudio            titanium      3.78                  3.64   \n",
       "6                 mule          mulestudio      3.57                  4.07   \n",
       "7           mulestudio                mule      2.98                  3.07   \n",
       "8               clover            usergrid      2.02                  2.53   \n",
       "9            talendesb               mesos      1.52                  1.95   \n",
       "10   talenddataquality        aptanastudio      4.53                  4.69   \n",
       "11                mule            titanium      3.48                  3.68   \n",
       "12   talenddataquality  appceleratorstudio      2.72                  2.66   \n",
       "13          mulestudio            titanium      3.73                  3.73   \n",
       "14  appceleratorstudio          mulestudio      3.41                  3.54   \n",
       "15  appceleratorstudio                mule      2.90                  2.89   \n",
       "\n",
       "    Llama3.2+SPWordPiece  Llama3.2+SPSentencePiece  \n",
       "0                   1.38                      1.05  \n",
       "1                   1.60                      1.56  \n",
       "2                   4.28                      4.31  \n",
       "3                   3.40                      3.30  \n",
       "4                   2.36                      2.36  \n",
       "5                   4.11                      3.78  \n",
       "6                   3.43                      3.65  \n",
       "7                   3.08                      3.31  \n",
       "8                   2.07                      1.48  \n",
       "9                   1.60                      1.66  \n",
       "10                  4.54                      4.73  \n",
       "11                  3.34                      3.31  \n",
       "12                  2.98                      2.49  \n",
       "13                  3.51                      3.97  \n",
       "14                  3.39                      3.53  \n",
       "15                  3.01                      2.77  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama3_cross_df = pd.DataFrame(\n",
    "    {\n",
    "        'Train': [item['train'] for item in ALL_CROSS_PROJECTS],\n",
    "        'Test': [item['test'] for item in ALL_CROSS_PROJECTS]\n",
    "    }\n",
    "    , dtype='float64'\n",
    ")\n",
    "\n",
    "for model in MODELS:\n",
    "    maes = []\n",
    "    for i, projects in enumerate(CROSS_PROJECTS):\n",
    "        for project in projects:\n",
    "            model_id = model\n",
    "            caracter = model[1:]\n",
    "            caracter = caracter * (i + 2)\n",
    "            model_id = f\"#{caracter}\"\n",
    "\n",
    "            print(f\"Running inference for project trained on {project['train']} and tested on {project['test']}\")\n",
    "            mae, mdae = main(model_id, f\"{project['train']}_{project['test']}\" )\n",
    "            mae = round(mae, 2)\n",
    "            maes.append(mae)\n",
    "            print(f\"MAE for project {project['train']}: {mae}\")\n",
    "            hf_model_id = f\"DEVCamiloSepulveda/{model[1:]}-LLAMA3SP-{project['train']}\"\n",
    "            # clean_hf_cache(hf_model_id)\n",
    "            clean_gpu_memory()\n",
    "    llama3_cross_df[MODELS[model]] = maes\n",
    "    llama3_cross_df[MODELS[model]] = llama3_cross_df[MODELS[model]].apply(lambda x: round(x, 2))\n",
    "\n",
    "print(\"All projects processed\")\n",
    "\n",
    "llama3_cross_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional: Save the results\n",
    "\n",
    "You can save the results of the DataFrame to a CSV file for further analysis or record-keeping. Run the following script to save the `llama3_cross_df` DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'Llama3_cross_results.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrtaFrame to a CSV file\n",
    "llama3_cross_df.to_csv('./data_model_analysis/Llama3_cross_results.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'Llama3_cross_results.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the cell below to do inference on testing dataset using **local** trained model on all cross projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from local on Cross projects inference...\n",
      "within project inference using model ./models/mesos_usergrid_epo_6 for project mesos\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  302\n",
      "using llama3 tokenizer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Camilo\\AppData\\Local\\Temp\\ipykernel_26004\\1345957804.py:245: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_seq = torch.tensor(tokens_test['input_ids'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: mesos_usergrid, MAE: 1.3700000047683716, MdAE: 1.0562742948532104\n",
      "within project inference using model ./models/usergrid_mesos_epo_4 for project usergrid\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  86\n",
      "using llama3 tokenizer!\n",
      "Model: usergrid_mesos, MAE: 1.4500000476837158, MdAE: 1.2023036479949951\n",
      "within project inference using model ./models/appceleratorstudio_aptanastudio_epo_0 for project appceleratorstudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  512\n",
      "using llama3 tokenizer!\n",
      "Model: appceleratorstudio_aptanastudio, MAE: 1.5299999713897705, MdAE: 1.2283222675323486\n",
      "within project inference using model ./models/appceleratorstudio_titanium_epo_0 for project appceleratorstudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  512\n",
      "using llama3 tokenizer!\n",
      "Model: appceleratorstudio_titanium, MAE: 1.7000000476837158, MdAE: 1.4522945880889893\n",
      "within project inference using model ./models/titanium_appceleratorstudio_epo_0 for project titanium\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  405\n",
      "using llama3 tokenizer!\n",
      "Model: titanium_appceleratorstudio, MAE: 2.6500000953674316, MdAE: 2.1921749114990234\n",
      "within project inference using model ./models/aptanastudio_titanium_epo_2 for project aptanastudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  149\n",
      "using llama3 tokenizer!\n",
      "Model: aptanastudio_titanium, MAE: 3.4800000190734863, MdAE: 2.7980830669403076\n",
      "within project inference using model ./models/mule_mulestudio_epo_16 for project mule\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  159\n",
      "using llama3 tokenizer!\n",
      "Model: mule_mulestudio, MAE: 2.7300000190734863, MdAE: 2.5237152576446533\n",
      "within project inference using model ./models/mulestudio_mule_epo_6 for project mulestudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  131\n",
      "using llama3 tokenizer!\n",
      "Model: mulestudio_mule, MAE: 3.869999885559082, MdAE: 3.0381665229797363\n",
      "within project inference using model ./models/clover_usergrid_epo_8 for project clover\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  69\n",
      "using llama3 tokenizer!\n",
      "Model: clover_usergrid, MAE: 4.170000076293945, MdAE: 2.5466246604919434\n",
      "within project inference using model ./models/talendesb_mesos_epo_15 for project talendesb\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  156\n",
      "using llama3 tokenizer!\n",
      "Model: talendesb_mesos, MAE: 1.0700000524520874, MdAE: 0.8679754734039307\n",
      "within project inference using model ./models/talenddataquality_aptanastudio_epo_3 for project talenddataquality\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  248\n",
      "using llama3 tokenizer!\n",
      "Model: talenddataquality_aptanastudio, MAE: 3.7100000381469727, MdAE: 3.401155471801758\n",
      "within project inference using model ./models/mule_titanium_epo_13 for project mule\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  159\n",
      "using llama3 tokenizer!\n",
      "Model: mule_titanium, MAE: 2.680000066757202, MdAE: 2.3301451206207275\n",
      "within project inference using model ./models/talenddataquality_appceleratorstudio_epo_0 for project talenddataquality\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  248\n",
      "using llama3 tokenizer!\n",
      "Model: talenddataquality_appceleratorstudio, MAE: 3.9200000762939453, MdAE: 3.9146008491516113\n",
      "within project inference using model ./models/mulestudio_titanium_epo_1 for project mulestudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  131\n",
      "using llama3 tokenizer!\n",
      "Model: mulestudio_titanium, MAE: 3.8499999046325684, MdAE: 3.1191020011901855\n",
      "within project inference using model ./models/appceleratorstudio_mulestudio_epo_0 for project appceleratorstudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  512\n",
      "using llama3 tokenizer!\n",
      "Model: appceleratorstudio_mulestudio, MAE: 1.7400000095367432, MdAE: 1.5412847995758057\n",
      "within project inference using model ./models/appceleratorstudio_mule_epo_2 for project appceleratorstudio\n",
      "Loading model from local...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size:  512\n",
      "using llama3 tokenizer!\n",
      "Model: appceleratorstudio_mule, MAE: 1.909999966621399, MdAE: 1.7131648063659668\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train Project</th>\n",
       "      <th>Test Project</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mesos</td>\n",
       "      <td>usergrid</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usergrid</td>\n",
       "      <td>mesos</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>titanium</td>\n",
       "      <td>1.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>titanium</td>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>titanium</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mule</td>\n",
       "      <td>mulestudio</td>\n",
       "      <td>2.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mulestudio</td>\n",
       "      <td>mule</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clover</td>\n",
       "      <td>usergrid</td>\n",
       "      <td>4.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>talendesb</td>\n",
       "      <td>mesos</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>talenddataquality</td>\n",
       "      <td>aptanastudio</td>\n",
       "      <td>3.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mule</td>\n",
       "      <td>titanium</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>talenddataquality</td>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>3.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mulestudio</td>\n",
       "      <td>titanium</td>\n",
       "      <td>3.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>mulestudio</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>appceleratorstudio</td>\n",
       "      <td>mule</td>\n",
       "      <td>1.91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Train Project        Test Project   MAE\n",
       "0                mesos            usergrid  1.37\n",
       "1             usergrid               mesos  1.45\n",
       "2   appceleratorstudio        aptanastudio  1.53\n",
       "3   appceleratorstudio            titanium  1.70\n",
       "4             titanium  appceleratorstudio  2.65\n",
       "5         aptanastudio            titanium  3.48\n",
       "6                 mule          mulestudio  2.73\n",
       "7           mulestudio                mule  3.87\n",
       "8               clover            usergrid  4.17\n",
       "9            talendesb               mesos  1.07\n",
       "10   talenddataquality        aptanastudio  3.71\n",
       "11                mule            titanium  2.68\n",
       "12   talenddataquality  appceleratorstudio  3.92\n",
       "13          mulestudio            titanium  3.85\n",
       "14  appceleratorstudio          mulestudio  1.74\n",
       "15  appceleratorstudio                mule  1.91"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading models from local on Cross projects inference...\")\n",
    "\n",
    "cross_projects = [item for sublist in PROJECTS[1:] for item in sublist]\n",
    "cross_maes = local_model_inference(cross_projects)\n",
    "cross_df = create_df(cross_maes, cross_projects)\n",
    "\n",
    "cross_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional: Save the results\n",
    "\n",
    "You can save the results of the DataFrame to a CSV file for further analysis or record-keeping. Run the following script to save the `cross_df` DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'cross_project_results.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrtaFrame to a CSV file\n",
    "cross_df.to_csv('./data_model_analysis/Llama3SP_cross_results.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'Llama3SP_cross_results.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
