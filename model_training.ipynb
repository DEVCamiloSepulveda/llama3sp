{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Llama3SP import LlamaForSequenceClassification as LLAMA3SP\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    XLNetTokenizer,\n",
    "    BertTokenizer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to huggingface hub to put your Llama token so we can access Llama 3.2 1B Param Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME, DEVICE\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.3 # within proj: 0.3 / cross proj: 0.4\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "TOKENIZER = 'qwen' # available: llama3, wordlevel, sentencepiece, wordpiece, gpt, deepseek, qwen\n",
    "MODEL_NAME = 'qwen' # available: llama3, llama3sp, deepseek, qwen\n",
    "# HF_MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "# HF_MODEL_NAME = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "HF_MODEL_NAME = 'Qwen/Qwen3-1.7B'\n",
    "\n",
    "# define device\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    # set up to release cache memory when possible\n",
    "    torch.cuda.empty_cache()\n",
    "    # set up more conservative memory limits  \n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)  # Use only 80% of GPU memory\n",
    "\n",
    "# define files to be used\n",
    "global DATA_PATH \n",
    "DATA_PATH = './sp_dataset/marked_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dynamic memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Methods and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = ''\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "\n",
    "def optimize_memory():\n",
    "    \"\"\"Aux function to optimize memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def data_processing(file_pair):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH, MODEL_NAME\n",
    "\n",
    "    optimize_memory()\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
    "    for train_file_name in file_pair['train']:\n",
    "        fname = DATA_PATH + train_file_name + '.csv'\n",
    "        df = prepare_dataframe(fname)\n",
    "        train_data = train_data.append(df)\n",
    "        \n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
    "    else:\n",
    "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
    "    # define batch size dynamically based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        # BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "        BATCH_SIZE = min(int(len(train_text) * BATCH_SIZE_RATIO), 32)\n",
    "\n",
    "    optimize_memory()\n",
    "\n",
    "    # process data in chunks for tokenization\n",
    "    def process_in_chunks(texts, chunk_size=1000):\n",
    "        all_tokens = {'input_ids': []}\n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i + chunk_size].tolist()\n",
    "            tokens = tokenization(chunk)\n",
    "            all_tokens['input_ids'].extend(tokens['input_ids'])\n",
    "            optimize_memory()\n",
    "        return all_tokens\n",
    "    \n",
    "    # tokenization\n",
    "    tokens_train = process_in_chunks(train_text)\n",
    "    tokens_val = process_in_chunks(val_text)\n",
    " \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "    \n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    test_file_names = []\n",
    "\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = process_in_chunks(test_text)\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(file_pair['test'][0])\n",
    "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "    for test_file_name in file_pair['test']:\n",
    "        fname = DATA_PATH + test_file_name + '.csv'\n",
    "        test_data = prepare_dataframe(fname)\n",
    "\n",
    "        test_text = test_data['text']\n",
    "        test_labels = test_data['label']\n",
    "\n",
    "        # tokenization\n",
    "        tokens_test = process_in_chunks(test_text)\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(test_file_name)\n",
    "\n",
    "        optimize_memory()\n",
    "    print('cross project data processing!')\n",
    "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "\n",
    "def train_val_split(data, split_ratio):\n",
    "    print('cross project split!')\n",
    "    split_point = int(len(data) * split_ratio)\n",
    "    train_text = data['text'][:split_point]\n",
    "    train_labels = data['label'][:split_point]\n",
    "    val_text = data['text'][split_point:]\n",
    "    val_labels = data['label'][split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
    "\n",
    "    if TOKENIZER == 'wordpiece':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/sp_word_piece/vocab.txt')\n",
    "    elif TOKENIZER == 'sentencepiece':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sp_sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    elif TOKENIZER == 'wordlevel':\n",
    "        print('using wordlevel tokenizer!')\n",
    "        tokenizer = Tokenizer.from_file('all_tokenizers/sp_word_level/wordlevel.json')\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'llama3':\n",
    "        print('using pretrained llama3 tokenizer')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME, add_prefix_space=True)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    elif TOKENIZER == 'deepseek':\n",
    "        print('using pretrained DeepSeek tokenizer')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    elif TOKENIZER == 'qwen':\n",
    "        print('using pretrained Qwen tokenizer')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # update some model configs\n",
    "    # must use .cache = False as below or it crashes from my experience\n",
    "    MODEL.config.pad_token_id = tokenizer.pad_token_id\n",
    "    MODEL.config.use_cache = False\n",
    "    MODEL.config.pretraining_tp = 1\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "    d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_text = data['text'][:train_val_split_point]\n",
    "    train_labels = data['label'][:train_val_split_point]\n",
    "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "    test_text = data['text'][val_test_split_point:]\n",
    "    test_labels = data['label'][val_test_split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels, test_text, test_labels   \n",
    "\n",
    "\n",
    "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
    "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
    "    optimizer = torch.optim.AdamW(MODEL.parameters(), lr=LEARNING_RATE)    \n",
    "    # total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(\"Start training for \", file_pair, \".....\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    \n",
    "    # vars for model selection\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    \n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        optimize_memory()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):            \n",
    "            b_input_ids = batch[0].to(torch.long).to(DEVICE)\n",
    "            b_labels = batch[1].to(torch.float).to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "            optimize_memory()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "        optimize_memory()\n",
    "        \n",
    "        time_records.append(time.time() - start_time)\n",
    "        \n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:            \n",
    "            b_input_ids = batch[0].to(torch.long).to(DEVICE)\n",
    "            b_labels = batch[1].to(torch.float).to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_eval_loss += loss.item()  \n",
    "            # clean memory\n",
    "            del b_input_ids, b_labels, batch, result, loss, logits\n",
    "            optimize_memory()\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "\n",
    "        optimize_memory()\n",
    "        \n",
    "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        optimize_memory()\n",
    "        # save model state to dict\n",
    "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "        print(\"===============================\")\n",
    "        \n",
    "        # testing on holdout data\n",
    "        index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            test_file_name = test_file_names[index]\n",
    "            index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits['logits'].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            distance_records = []\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    distance_records.append(distance)\n",
    "\n",
    "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "            MAE = np.mean(np.array(distance_records)) \n",
    "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "            MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "            global OUTPUT\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # select model\n",
    "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "              'models/' + str(file_pair['train'][0]) + '_' \n",
    "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "    # del unwanted models\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            os.remove(\"models/epo_\" + str(i))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "    print('all done for one project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1147633d1295484ebba461b269939c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 18.92\n",
      "-\n",
      " Average eval MAE loss: 4.23\n",
      "===============================\n",
      "MAE:  1.5512632\n",
      "MdAE:  1.3481796\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 12.13\n",
      "-\n",
      " Average eval MAE loss: 12.15\n",
      "===============================\n",
      "MAE:  2.6329756\n",
      "MdAE:  2.3386428\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 7.94\n",
      "-\n",
      " Average eval MAE loss: 11.59\n",
      "===============================\n",
      "MAE:  2.699457\n",
      "MdAE:  2.414291\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 5.17\n",
      "-\n",
      " Average eval MAE loss: 9.57\n",
      "===============================\n",
      "MAE:  2.4589581\n",
      "MdAE:  2.2552195\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.55\n",
      "-\n",
      " Average eval MAE loss: 10.43\n",
      "===============================\n",
      "MAE:  2.5690935\n",
      "MdAE:  2.3332527\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.41\n",
      "-\n",
      " Average eval MAE loss: 7.08\n",
      "===============================\n",
      "MAE:  2.056425\n",
      "MdAE:  1.7325592\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.31\n",
      "-\n",
      " Average eval MAE loss: 5.80\n",
      "===============================\n",
      "MAE:  1.8765509\n",
      "MdAE:  1.5960782\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.77\n",
      "-\n",
      " Average eval MAE loss: 7.57\n",
      "===============================\n",
      "MAE:  2.131484\n",
      "MdAE:  1.8435113\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.39\n",
      "-\n",
      " Average eval MAE loss: 7.11\n",
      "===============================\n",
      "MAE:  2.0469687\n",
      "MdAE:  1.7589958\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 6.50\n",
      "===============================\n",
      "MAE:  1.9686418\n",
      "MdAE:  1.7671096\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.02\n",
      "-\n",
      " Average eval MAE loss: 7.90\n",
      "===============================\n",
      "MAE:  2.1724143\n",
      "MdAE:  1.8704698\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.61\n",
      "-\n",
      " Average eval MAE loss: 8.01\n",
      "===============================\n",
      "MAE:  2.168139\n",
      "MdAE:  1.8441889\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.47\n",
      "-\n",
      " Average eval MAE loss: 6.29\n",
      "===============================\n",
      "MAE:  1.9478258\n",
      "MdAE:  1.6945057\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.32\n",
      "-\n",
      " Average eval MAE loss: 7.07\n",
      "===============================\n",
      "MAE:  2.062282\n",
      "MdAE:  1.8159871\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.29\n",
      "-\n",
      " Average eval MAE loss: 6.77\n",
      "===============================\n",
      "MAE:  2.0082796\n",
      "MdAE:  1.7276909\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 7.57\n",
      "===============================\n",
      "MAE:  2.1213052\n",
      "MdAE:  1.7894816\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.15\n",
      "-\n",
      " Average eval MAE loss: 6.88\n",
      "===============================\n",
      "MAE:  2.0348506\n",
      "MdAE:  1.7732806\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 6.68\n",
      "===============================\n",
      "MAE:  1.9942361\n",
      "MdAE:  1.7160299\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 6.83\n",
      "===============================\n",
      "MAE:  2.0213077\n",
      "MdAE:  1.7453814\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 6.73\n",
      "===============================\n",
      "MAE:  2.004332\n",
      "MdAE:  1.6921289\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffdc5a983474791abc4501eeb7f4172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['aptanastudio'], 'test': ['aptanastudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 41.75\n",
      "-\n",
      " Average eval MAE loss: 35.50\n",
      "===============================\n",
      "MAE:  3.364114\n",
      "MdAE:  2.1809506\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 28.55\n",
      "-\n",
      " Average eval MAE loss: 29.84\n",
      "===============================\n",
      "MAE:  3.927094\n",
      "MdAE:  3.0819707\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 16.12\n",
      "-\n",
      " Average eval MAE loss: 33.18\n",
      "===============================\n",
      "MAE:  4.0182986\n",
      "MdAE:  3.2475832\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 8.79\n",
      "-\n",
      " Average eval MAE loss: 36.86\n",
      "===============================\n",
      "MAE:  3.7999246\n",
      "MdAE:  2.8548074\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 4.96\n",
      "-\n",
      " Average eval MAE loss: 34.69\n",
      "===============================\n",
      "MAE:  4.178519\n",
      "MdAE:  3.4144874\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 4.04\n",
      "-\n",
      " Average eval MAE loss: 35.23\n",
      "===============================\n",
      "MAE:  3.712961\n",
      "MdAE:  2.8498752\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.57\n",
      "-\n",
      " Average eval MAE loss: 33.52\n",
      "===============================\n",
      "MAE:  3.844384\n",
      "MdAE:  3.0900888\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.67\n",
      "-\n",
      " Average eval MAE loss: 33.74\n",
      "===============================\n",
      "MAE:  3.9570475\n",
      "MdAE:  3.246397\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.85\n",
      "-\n",
      " Average eval MAE loss: 35.41\n",
      "===============================\n",
      "MAE:  3.694151\n",
      "MdAE:  2.715026\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.63\n",
      "-\n",
      " Average eval MAE loss: 34.38\n",
      "===============================\n",
      "MAE:  4.0785317\n",
      "MdAE:  3.2003222\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.97\n",
      "-\n",
      " Average eval MAE loss: 34.80\n",
      "===============================\n",
      "MAE:  3.8793666\n",
      "MdAE:  2.9544373\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 33.07\n",
      "===============================\n",
      "MAE:  3.8519933\n",
      "MdAE:  2.8473334\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.95\n",
      "-\n",
      " Average eval MAE loss: 34.03\n",
      "===============================\n",
      "MAE:  3.9031284\n",
      "MdAE:  2.910899\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.62\n",
      "-\n",
      " Average eval MAE loss: 33.94\n",
      "===============================\n",
      "MAE:  3.7669072\n",
      "MdAE:  2.7130625\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.36\n",
      "-\n",
      " Average eval MAE loss: 33.16\n",
      "===============================\n",
      "MAE:  3.9350681\n",
      "MdAE:  3.122549\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.42\n",
      "-\n",
      " Average eval MAE loss: 33.50\n",
      "===============================\n",
      "MAE:  3.9402046\n",
      "MdAE:  3.1199794\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.59\n",
      "-\n",
      " Average eval MAE loss: 33.60\n",
      "===============================\n",
      "MAE:  3.923184\n",
      "MdAE:  3.0169172\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.17\n",
      "-\n",
      " Average eval MAE loss: 34.40\n",
      "===============================\n",
      "MAE:  3.8134239\n",
      "MdAE:  2.8363483\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 34.01\n",
      "===============================\n",
      "MAE:  3.8425314\n",
      "MdAE:  2.9320784\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 34.15\n",
      "===============================\n",
      "MAE:  3.8398192\n",
      "MdAE:  2.9524212\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6b2f61032a49c8b2c924044e0d29cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['bamboo'], 'test': ['bamboo']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 8.10\n",
      "-\n",
      " Average eval MAE loss: 3.62\n",
      "===============================\n",
      "MAE:  1.689991\n",
      "MdAE:  1.5455232\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 4.86\n",
      "-\n",
      " Average eval MAE loss: 3.25\n",
      "===============================\n",
      "MAE:  1.5603386\n",
      "MdAE:  1.3236208\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 2.73\n",
      "-\n",
      " Average eval MAE loss: 2.06\n",
      "===============================\n",
      "MAE:  1.1810333\n",
      "MdAE:  0.94262314\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.05\n",
      "-\n",
      " Average eval MAE loss: 2.31\n",
      "===============================\n",
      "MAE:  1.1823512\n",
      "MdAE:  0.92555666\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.65\n",
      "-\n",
      " Average eval MAE loss: 2.95\n",
      "===============================\n",
      "MAE:  1.3102189\n",
      "MdAE:  1.0478227\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 0.60\n",
      "-\n",
      " Average eval MAE loss: 2.26\n",
      "===============================\n",
      "MAE:  1.1314809\n",
      "MdAE:  0.9572675\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.59\n",
      "-\n",
      " Average eval MAE loss: 1.96\n",
      "===============================\n",
      "MAE:  1.1032604\n",
      "MdAE:  0.9220009\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.45\n",
      "-\n",
      " Average eval MAE loss: 1.81\n",
      "===============================\n",
      "MAE:  1.063382\n",
      "MdAE:  0.7861018\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 2.33\n",
      "===============================\n",
      "MAE:  1.1798993\n",
      "MdAE:  0.9258704\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.61\n",
      "-\n",
      " Average eval MAE loss: 2.20\n",
      "===============================\n",
      "MAE:  1.1639695\n",
      "MdAE:  0.9444231\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.48\n",
      "-\n",
      " Average eval MAE loss: 2.18\n",
      "===============================\n",
      "MAE:  1.1638575\n",
      "MdAE:  0.91737044\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.38\n",
      "-\n",
      " Average eval MAE loss: 1.90\n",
      "===============================\n",
      "MAE:  1.0919553\n",
      "MdAE:  0.7752986\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.14\n",
      "-\n",
      " Average eval MAE loss: 1.85\n",
      "===============================\n",
      "MAE:  1.089613\n",
      "MdAE:  0.8405771\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.22\n",
      "-\n",
      " Average eval MAE loss: 1.84\n",
      "===============================\n",
      "MAE:  1.0655614\n",
      "MdAE:  0.78655076\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.14\n",
      "-\n",
      " Average eval MAE loss: 1.99\n",
      "===============================\n",
      "MAE:  1.1033213\n",
      "MdAE:  0.866318\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.06\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  1.099574\n",
      "MdAE:  0.8892299\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 1.88\n",
      "===============================\n",
      "MAE:  1.0823379\n",
      "MdAE:  0.8274462\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 1.89\n",
      "===============================\n",
      "MAE:  1.0847396\n",
      "MdAE:  0.8603927\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.06\n",
      "===============================\n",
      "MAE:  1.1331372\n",
      "MdAE:  0.914695\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.01\n",
      "===============================\n",
      "MAE:  1.1191212\n",
      "MdAE:  0.87094355\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ec84e7e83b4dfc92fa8298a7487400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['clover'], 'test': ['clover']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 57.58\n",
      "-\n",
      " Average eval MAE loss: 31.64\n",
      "===============================\n",
      "MAE:  5.496256\n",
      "MdAE:  4.4075813\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 29.38\n",
      "-\n",
      " Average eval MAE loss: 21.87\n",
      "===============================\n",
      "MAE:  3.8125405\n",
      "MdAE:  1.917032\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 19.34\n",
      "-\n",
      " Average eval MAE loss: 34.91\n",
      "===============================\n",
      "MAE:  5.5954533\n",
      "MdAE:  4.2779965\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 9.98\n",
      "-\n",
      " Average eval MAE loss: 23.96\n",
      "===============================\n",
      "MAE:  4.0238843\n",
      "MdAE:  2.1857357\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.28\n",
      "-\n",
      " Average eval MAE loss: 23.64\n",
      "===============================\n",
      "MAE:  4.1501102\n",
      "MdAE:  2.1706867\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.75\n",
      "-\n",
      " Average eval MAE loss: 27.01\n",
      "===============================\n",
      "MAE:  4.76079\n",
      "MdAE:  3.3562832\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.37\n",
      "-\n",
      " Average eval MAE loss: 23.97\n",
      "===============================\n",
      "MAE:  4.370663\n",
      "MdAE:  2.7399158\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.23\n",
      "-\n",
      " Average eval MAE loss: 24.33\n",
      "===============================\n",
      "MAE:  4.395175\n",
      "MdAE:  2.8121548\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.67\n",
      "-\n",
      " Average eval MAE loss: 24.76\n",
      "===============================\n",
      "MAE:  4.363153\n",
      "MdAE:  2.8369298\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.75\n",
      "-\n",
      " Average eval MAE loss: 24.23\n",
      "===============================\n",
      "MAE:  4.311122\n",
      "MdAE:  2.6765084\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.83\n",
      "-\n",
      " Average eval MAE loss: 23.53\n",
      "===============================\n",
      "MAE:  4.2903843\n",
      "MdAE:  2.4895887\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.74\n",
      "-\n",
      " Average eval MAE loss: 24.84\n",
      "===============================\n",
      "MAE:  4.5179715\n",
      "MdAE:  2.8325024\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.50\n",
      "-\n",
      " Average eval MAE loss: 23.63\n",
      "===============================\n",
      "MAE:  4.1922536\n",
      "MdAE:  2.4894822\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 24.30\n",
      "===============================\n",
      "MAE:  4.349603\n",
      "MdAE:  2.5199509\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 25.39\n",
      "===============================\n",
      "MAE:  4.5447636\n",
      "MdAE:  2.942843\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.35\n",
      "-\n",
      " Average eval MAE loss: 23.37\n",
      "===============================\n",
      "MAE:  4.157977\n",
      "MdAE:  2.4452646\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.18\n",
      "-\n",
      " Average eval MAE loss: 25.20\n",
      "===============================\n",
      "MAE:  4.5842557\n",
      "MdAE:  3.09478\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.19\n",
      "-\n",
      " Average eval MAE loss: 24.27\n",
      "===============================\n",
      "MAE:  4.371688\n",
      "MdAE:  2.763383\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 23.96\n",
      "===============================\n",
      "MAE:  4.2979608\n",
      "MdAE:  2.6229577\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 24.38\n",
      "===============================\n",
      "MAE:  4.380017\n",
      "MdAE:  2.769249\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc18859fc624e03bdce05f8f807c66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['datamanagement'], 'test': ['datamanagement']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 230.20\n",
      "-\n",
      " Average eval MAE loss: 179.87\n",
      "===============================\n",
      "MAE:  9.184349\n",
      "MdAE:  7.2387123\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 153.41\n",
      "-\n",
      " Average eval MAE loss: 157.36\n",
      "===============================\n",
      "MAE:  6.7086096\n",
      "MdAE:  4.193161\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 111.37\n",
      "-\n",
      " Average eval MAE loss: 162.96\n",
      "===============================\n",
      "MAE:  7.6410913\n",
      "MdAE:  5.050146\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 83.22\n",
      "-\n",
      " Average eval MAE loss: 200.14\n",
      "===============================\n",
      "MAE:  6.993478\n",
      "MdAE:  3.4593217\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 65.18\n",
      "-\n",
      " Average eval MAE loss: 172.21\n",
      "===============================\n",
      "MAE:  5.812704\n",
      "MdAE:  2.500825\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 46.35\n",
      "-\n",
      " Average eval MAE loss: 165.18\n",
      "===============================\n",
      "MAE:  6.608508\n",
      "MdAE:  3.3993607\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 36.04\n",
      "-\n",
      " Average eval MAE loss: 158.60\n",
      "===============================\n",
      "MAE:  6.36197\n",
      "MdAE:  3.0115743\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 25.30\n",
      "-\n",
      " Average eval MAE loss: 177.18\n",
      "===============================\n",
      "MAE:  6.8280644\n",
      "MdAE:  3.211924\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 19.58\n",
      "-\n",
      " Average eval MAE loss: 158.31\n",
      "===============================\n",
      "MAE:  6.6723843\n",
      "MdAE:  3.39835\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 15.90\n",
      "-\n",
      " Average eval MAE loss: 162.25\n",
      "===============================\n",
      "MAE:  6.397623\n",
      "MdAE:  3.1526453\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 13.26\n",
      "-\n",
      " Average eval MAE loss: 165.21\n",
      "===============================\n",
      "MAE:  6.872473\n",
      "MdAE:  3.5080686\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 9.51\n",
      "-\n",
      " Average eval MAE loss: 162.94\n",
      "===============================\n",
      "MAE:  6.404444\n",
      "MdAE:  3.17838\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 7.86\n",
      "-\n",
      " Average eval MAE loss: 149.98\n",
      "===============================\n",
      "MAE:  6.438182\n",
      "MdAE:  3.3004508\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 6.80\n",
      "-\n",
      " Average eval MAE loss: 160.63\n",
      "===============================\n",
      "MAE:  6.623504\n",
      "MdAE:  3.4674644\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 5.36\n",
      "-\n",
      " Average eval MAE loss: 154.52\n",
      "===============================\n",
      "MAE:  6.3543687\n",
      "MdAE:  3.277987\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 4.78\n",
      "-\n",
      " Average eval MAE loss: 158.47\n",
      "===============================\n",
      "MAE:  6.5433593\n",
      "MdAE:  3.2499337\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 3.52\n",
      "-\n",
      " Average eval MAE loss: 157.50\n",
      "===============================\n",
      "MAE:  6.5724916\n",
      "MdAE:  3.20262\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 2.95\n",
      "-\n",
      " Average eval MAE loss: 155.88\n",
      "===============================\n",
      "MAE:  6.483372\n",
      "MdAE:  3.2161508\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 2.48\n",
      "-\n",
      " Average eval MAE loss: 155.47\n",
      "===============================\n",
      "MAE:  6.5597525\n",
      "MdAE:  3.375096\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 2.18\n",
      "-\n",
      " Average eval MAE loss: 156.18\n",
      "===============================\n",
      "MAE:  6.5478797\n",
      "MdAE:  3.2856264\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6cf29b51524b4cba93d2959389bc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['duracloud'], 'test': ['duracloud']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 10.83\n",
      "-\n",
      " Average eval MAE loss: 1.82\n",
      "===============================\n",
      "MAE:  1.0864797\n",
      "MdAE:  1.001685\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 3.96\n",
      "-\n",
      " Average eval MAE loss: 1.75\n",
      "===============================\n",
      "MAE:  1.1094776\n",
      "MdAE:  0.94037944\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.86\n",
      "-\n",
      " Average eval MAE loss: 1.72\n",
      "===============================\n",
      "MAE:  1.1561805\n",
      "MdAE:  0.94638956\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 1.02\n",
      "-\n",
      " Average eval MAE loss: 1.97\n",
      "===============================\n",
      "MAE:  1.1160784\n",
      "MdAE:  0.924518\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 1.97\n",
      "===============================\n",
      "MAE:  1.2421608\n",
      "MdAE:  0.9990084\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 0.62\n",
      "-\n",
      " Average eval MAE loss: 1.80\n",
      "===============================\n",
      "MAE:  1.0950247\n",
      "MdAE:  0.83456296\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.48\n",
      "-\n",
      " Average eval MAE loss: 2.00\n",
      "===============================\n",
      "MAE:  1.176124\n",
      "MdAE:  0.9104851\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.34\n",
      "-\n",
      " Average eval MAE loss: 2.03\n",
      "===============================\n",
      "MAE:  1.1863711\n",
      "MdAE:  0.94811493\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.32\n",
      "-\n",
      " Average eval MAE loss: 2.11\n",
      "===============================\n",
      "MAE:  1.26769\n",
      "MdAE:  0.9471631\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.29\n",
      "-\n",
      " Average eval MAE loss: 1.94\n",
      "===============================\n",
      "MAE:  1.1040901\n",
      "MdAE:  0.8500625\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.14\n",
      "-\n",
      " Average eval MAE loss: 1.90\n",
      "===============================\n",
      "MAE:  1.0882071\n",
      "MdAE:  0.85449743\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 1.88\n",
      "===============================\n",
      "MAE:  1.0903136\n",
      "MdAE:  0.8592526\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  1.0885797\n",
      "MdAE:  0.8504964\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 1.92\n",
      "===============================\n",
      "MAE:  1.1247183\n",
      "MdAE:  0.8692109\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 1.90\n",
      "===============================\n",
      "MAE:  1.1298262\n",
      "MdAE:  0.8849895\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  1.1408842\n",
      "MdAE:  0.8569246\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 1.94\n",
      "===============================\n",
      "MAE:  1.119337\n",
      "MdAE:  0.81653196\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 1.93\n",
      "===============================\n",
      "MAE:  1.1170439\n",
      "MdAE:  0.8292428\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  1.1230154\n",
      "MdAE:  0.8405487\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 1.95\n",
      "===============================\n",
      "MAE:  1.1209275\n",
      "MdAE:  0.8371574\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8aa7130045649b4849235a9eb01291d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['jirasoftware'], 'test': ['jirasoftware']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 25.57\n",
      "-\n",
      " Average eval MAE loss: 20.31\n",
      "===============================\n",
      "MAE:  3.331905\n",
      "MdAE:  2.9716892\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 11.83\n",
      "-\n",
      " Average eval MAE loss: 8.41\n",
      "===============================\n",
      "MAE:  2.018072\n",
      "MdAE:  1.3974485\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 5.65\n",
      "-\n",
      " Average eval MAE loss: 12.04\n",
      "===============================\n",
      "MAE:  2.3102365\n",
      "MdAE:  1.6827383\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.89\n",
      "-\n",
      " Average eval MAE loss: 8.91\n",
      "===============================\n",
      "MAE:  2.0530078\n",
      "MdAE:  1.6691737\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.07\n",
      "-\n",
      " Average eval MAE loss: 9.65\n",
      "===============================\n",
      "MAE:  2.1932287\n",
      "MdAE:  1.8365936\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.12\n",
      "-\n",
      " Average eval MAE loss: 13.11\n",
      "===============================\n",
      "MAE:  2.4435115\n",
      "MdAE:  2.3060722\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.37\n",
      "-\n",
      " Average eval MAE loss: 8.58\n",
      "===============================\n",
      "MAE:  1.9691124\n",
      "MdAE:  1.223979\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.92\n",
      "-\n",
      " Average eval MAE loss: 11.96\n",
      "===============================\n",
      "MAE:  2.287818\n",
      "MdAE:  1.9688034\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.57\n",
      "-\n",
      " Average eval MAE loss: 9.89\n",
      "===============================\n",
      "MAE:  2.0705726\n",
      "MdAE:  1.3633115\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.36\n",
      "-\n",
      " Average eval MAE loss: 9.89\n",
      "===============================\n",
      "MAE:  2.0916748\n",
      "MdAE:  1.395957\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.49\n",
      "-\n",
      " Average eval MAE loss: 11.75\n",
      "===============================\n",
      "MAE:  2.2396164\n",
      "MdAE:  1.7137203\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.48\n",
      "-\n",
      " Average eval MAE loss: 9.90\n",
      "===============================\n",
      "MAE:  2.0743945\n",
      "MdAE:  1.2129877\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.25\n",
      "-\n",
      " Average eval MAE loss: 11.13\n",
      "===============================\n",
      "MAE:  2.1902983\n",
      "MdAE:  1.6259251\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.27\n",
      "-\n",
      " Average eval MAE loss: 10.17\n",
      "===============================\n",
      "MAE:  2.1087067\n",
      "MdAE:  1.3417873\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.14\n",
      "-\n",
      " Average eval MAE loss: 10.20\n",
      "===============================\n",
      "MAE:  2.1413214\n",
      "MdAE:  1.4078789\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 10.79\n",
      "===============================\n",
      "MAE:  2.1708229\n",
      "MdAE:  1.4519892\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.10\n",
      "-\n",
      " Average eval MAE loss: 10.52\n",
      "===============================\n",
      "MAE:  2.1321752\n",
      "MdAE:  1.3837886\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 10.65\n",
      "===============================\n",
      "MAE:  2.1506984\n",
      "MdAE:  1.4218538\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 10.66\n",
      "===============================\n",
      "MAE:  2.151759\n",
      "MdAE:  1.4180918\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 10.60\n",
      "===============================\n",
      "MAE:  2.1465857\n",
      "MdAE:  1.4161272\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e649779ec946cf84a731af0e28f625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['mesos'], 'test': ['mesos']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 7.46\n",
      "-\n",
      " Average eval MAE loss: 6.49\n",
      "===============================\n",
      "MAE:  1.9931749\n",
      "MdAE:  1.8402349\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 5.56\n",
      "-\n",
      " Average eval MAE loss: 4.45\n",
      "===============================\n",
      "MAE:  1.5542097\n",
      "MdAE:  1.362766\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 3.44\n",
      "-\n",
      " Average eval MAE loss: 4.26\n",
      "===============================\n",
      "MAE:  1.4155613\n",
      "MdAE:  1.0332493\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.10\n",
      "-\n",
      " Average eval MAE loss: 4.38\n",
      "===============================\n",
      "MAE:  1.433119\n",
      "MdAE:  1.1728595\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.54\n",
      "-\n",
      " Average eval MAE loss: 4.26\n",
      "===============================\n",
      "MAE:  1.3974905\n",
      "MdAE:  1.0901616\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.36\n",
      "-\n",
      " Average eval MAE loss: 5.20\n",
      "===============================\n",
      "MAE:  1.6077414\n",
      "MdAE:  1.3160386\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.02\n",
      "-\n",
      " Average eval MAE loss: 4.34\n",
      "===============================\n",
      "MAE:  1.3738711\n",
      "MdAE:  1.0683196\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.83\n",
      "-\n",
      " Average eval MAE loss: 4.14\n",
      "===============================\n",
      "MAE:  1.4184734\n",
      "MdAE:  1.1136403\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.79\n",
      "-\n",
      " Average eval MAE loss: 4.27\n",
      "===============================\n",
      "MAE:  1.4446789\n",
      "MdAE:  1.1628673\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.76\n",
      "-\n",
      " Average eval MAE loss: 4.83\n",
      "===============================\n",
      "MAE:  1.5255294\n",
      "MdAE:  1.2567729\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.38\n",
      "-\n",
      " Average eval MAE loss: 4.12\n",
      "===============================\n",
      "MAE:  1.3581176\n",
      "MdAE:  1.0866377\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.25\n",
      "-\n",
      " Average eval MAE loss: 4.13\n",
      "===============================\n",
      "MAE:  1.361523\n",
      "MdAE:  1.0968223\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.33\n",
      "-\n",
      " Average eval MAE loss: 4.33\n",
      "===============================\n",
      "MAE:  1.3867781\n",
      "MdAE:  1.0987728\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.18\n",
      "-\n",
      " Average eval MAE loss: 4.06\n",
      "===============================\n",
      "MAE:  1.3459135\n",
      "MdAE:  1.0229259\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 4.22\n",
      "===============================\n",
      "MAE:  1.3818973\n",
      "MdAE:  1.0922766\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 4.29\n",
      "===============================\n",
      "MAE:  1.3792145\n",
      "MdAE:  1.0707978\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 4.10\n",
      "===============================\n",
      "MAE:  1.3493809\n",
      "MdAE:  1.0574361\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 4.07\n",
      "===============================\n",
      "MAE:  1.3374472\n",
      "MdAE:  1.0269291\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 4.05\n",
      "===============================\n",
      "MAE:  1.3356636\n",
      "MdAE:  1.0352007\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 4.11\n",
      "===============================\n",
      "MAE:  1.3462214\n",
      "MdAE:  1.0368247\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274edef4efe04bf9b0bc9d822d66d059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['moodle'], 'test': ['moodle']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 571.79\n",
      "-\n",
      " Average eval MAE loss: 573.52\n",
      "===============================\n",
      "MAE:  12.598942\n",
      "MdAE:  13.75243\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 445.73\n",
      "-\n",
      " Average eval MAE loss: 516.86\n",
      "===============================\n",
      "MAE:  11.779709\n",
      "MdAE:  11.292009\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 293.29\n",
      "-\n",
      " Average eval MAE loss: 443.59\n",
      "===============================\n",
      "MAE:  12.859807\n",
      "MdAE:  9.830744\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 138.43\n",
      "-\n",
      " Average eval MAE loss: 460.76\n",
      "===============================\n",
      "MAE:  10.142724\n",
      "MdAE:  8.125706\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 68.22\n",
      "-\n",
      " Average eval MAE loss: 419.50\n",
      "===============================\n",
      "MAE:  14.187761\n",
      "MdAE:  12.099871\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 53.60\n",
      "-\n",
      " Average eval MAE loss: 453.85\n",
      "===============================\n",
      "MAE:  14.844414\n",
      "MdAE:  12.949636\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 32.11\n",
      "-\n",
      " Average eval MAE loss: 446.51\n",
      "===============================\n",
      "MAE:  15.07636\n",
      "MdAE:  12.175335\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 30.94\n",
      "-\n",
      " Average eval MAE loss: 473.30\n",
      "===============================\n",
      "MAE:  11.135092\n",
      "MdAE:  8.952373\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 20.57\n",
      "-\n",
      " Average eval MAE loss: 450.94\n",
      "===============================\n",
      "MAE:  13.147221\n",
      "MdAE:  10.283126\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 17.38\n",
      "-\n",
      " Average eval MAE loss: 455.07\n",
      "===============================\n",
      "MAE:  13.316929\n",
      "MdAE:  10.566034\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 9.59\n",
      "-\n",
      " Average eval MAE loss: 460.73\n",
      "===============================\n",
      "MAE:  12.033217\n",
      "MdAE:  9.963704\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 9.15\n",
      "-\n",
      " Average eval MAE loss: 450.42\n",
      "===============================\n",
      "MAE:  14.105757\n",
      "MdAE:  12.256676\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 5.36\n",
      "-\n",
      " Average eval MAE loss: 452.16\n",
      "===============================\n",
      "MAE:  12.523419\n",
      "MdAE:  10.983824\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 5.87\n",
      "-\n",
      " Average eval MAE loss: 446.88\n",
      "===============================\n",
      "MAE:  14.86035\n",
      "MdAE:  12.884811\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 4.11\n",
      "-\n",
      " Average eval MAE loss: 454.48\n",
      "===============================\n",
      "MAE:  12.824676\n",
      "MdAE:  10.995361\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 1.88\n",
      "-\n",
      " Average eval MAE loss: 447.87\n",
      "===============================\n",
      "MAE:  13.1010475\n",
      "MdAE:  11.154657\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 2.22\n",
      "-\n",
      " Average eval MAE loss: 450.91\n",
      "===============================\n",
      "MAE:  13.3379135\n",
      "MdAE:  11.255883\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 1.20\n",
      "-\n",
      " Average eval MAE loss: 451.75\n",
      "===============================\n",
      "MAE:  12.777026\n",
      "MdAE:  11.18927\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.83\n",
      "-\n",
      " Average eval MAE loss: 447.65\n",
      "===============================\n",
      "MAE:  13.049773\n",
      "MdAE:  11.268808\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.55\n",
      "-\n",
      " Average eval MAE loss: 449.10\n",
      "===============================\n",
      "MAE:  13.006788\n",
      "MdAE:  11.357025\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29b5ebcb4504a0ca06a0f38103abae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['mule'], 'test': ['mule']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 15.10\n",
      "-\n",
      " Average eval MAE loss: 9.55\n",
      "===============================\n",
      "MAE:  2.7322893\n",
      "MdAE:  2.6263456\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 9.06\n",
      "-\n",
      " Average eval MAE loss: 9.84\n",
      "===============================\n",
      "MAE:  2.5648608\n",
      "MdAE:  2.3239179\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 5.03\n",
      "-\n",
      " Average eval MAE loss: 12.85\n",
      "===============================\n",
      "MAE:  3.1131797\n",
      "MdAE:  2.8565195\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.61\n",
      "-\n",
      " Average eval MAE loss: 11.05\n",
      "===============================\n",
      "MAE:  2.6314797\n",
      "MdAE:  2.257971\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.91\n",
      "-\n",
      " Average eval MAE loss: 11.86\n",
      "===============================\n",
      "MAE:  2.8244817\n",
      "MdAE:  2.637557\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.30\n",
      "-\n",
      " Average eval MAE loss: 11.38\n",
      "===============================\n",
      "MAE:  2.741017\n",
      "MdAE:  2.559682\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.70\n",
      "-\n",
      " Average eval MAE loss: 11.42\n",
      "===============================\n",
      "MAE:  2.8554783\n",
      "MdAE:  2.5979981\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.82\n",
      "-\n",
      " Average eval MAE loss: 10.59\n",
      "===============================\n",
      "MAE:  2.699478\n",
      "MdAE:  2.486221\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.94\n",
      "-\n",
      " Average eval MAE loss: 10.52\n",
      "===============================\n",
      "MAE:  2.7369883\n",
      "MdAE:  2.5411162\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.89\n",
      "-\n",
      " Average eval MAE loss: 11.53\n",
      "===============================\n",
      "MAE:  2.7122464\n",
      "MdAE:  2.3589764\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.49\n",
      "-\n",
      " Average eval MAE loss: 10.64\n",
      "===============================\n",
      "MAE:  2.7803588\n",
      "MdAE:  2.5395262\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 10.96\n",
      "===============================\n",
      "MAE:  2.7615538\n",
      "MdAE:  2.5660024\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.30\n",
      "-\n",
      " Average eval MAE loss: 10.59\n",
      "===============================\n",
      "MAE:  2.7177165\n",
      "MdAE:  2.4766402\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 10.60\n",
      "===============================\n",
      "MAE:  2.7124228\n",
      "MdAE:  2.478599\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.17\n",
      "-\n",
      " Average eval MAE loss: 10.70\n",
      "===============================\n",
      "MAE:  2.7472332\n",
      "MdAE:  2.3900564\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 10.75\n",
      "===============================\n",
      "MAE:  2.7161062\n",
      "MdAE:  2.473321\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 10.61\n",
      "===============================\n",
      "MAE:  2.7328346\n",
      "MdAE:  2.4335124\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.06\n",
      "-\n",
      " Average eval MAE loss: 10.68\n",
      "===============================\n",
      "MAE:  2.7289279\n",
      "MdAE:  2.4338007\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 10.63\n",
      "===============================\n",
      "MAE:  2.7285404\n",
      "MdAE:  2.4616632\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 10.64\n",
      "===============================\n",
      "MAE:  2.7302277\n",
      "MdAE:  2.4736972\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569cb1bca7f84698a99d845225d87adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['mulestudio'], 'test': ['mulestudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 29.08\n",
      "-\n",
      " Average eval MAE loss: 58.22\n",
      "===============================\n",
      "MAE:  3.7876222\n",
      "MdAE:  2.814314\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 17.22\n",
      "-\n",
      " Average eval MAE loss: 58.61\n",
      "===============================\n",
      "MAE:  3.7621157\n",
      "MdAE:  2.908661\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 9.32\n",
      "-\n",
      " Average eval MAE loss: 59.57\n",
      "===============================\n",
      "MAE:  3.887725\n",
      "MdAE:  2.758008\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.92\n",
      "-\n",
      " Average eval MAE loss: 54.79\n",
      "===============================\n",
      "MAE:  3.9521832\n",
      "MdAE:  3.171423\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.22\n",
      "-\n",
      " Average eval MAE loss: 58.05\n",
      "===============================\n",
      "MAE:  3.7948647\n",
      "MdAE:  2.7362623\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.05\n",
      "-\n",
      " Average eval MAE loss: 63.04\n",
      "===============================\n",
      "MAE:  3.9938076\n",
      "MdAE:  2.820116\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.47\n",
      "-\n",
      " Average eval MAE loss: 59.14\n",
      "===============================\n",
      "MAE:  3.9162436\n",
      "MdAE:  2.8746734\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.98\n",
      "-\n",
      " Average eval MAE loss: 59.57\n",
      "===============================\n",
      "MAE:  3.945675\n",
      "MdAE:  3.0679283\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.93\n",
      "-\n",
      " Average eval MAE loss: 57.68\n",
      "===============================\n",
      "MAE:  3.9414842\n",
      "MdAE:  3.0221195\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.81\n",
      "-\n",
      " Average eval MAE loss: 60.04\n",
      "===============================\n",
      "MAE:  4.0007854\n",
      "MdAE:  2.8517003\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.80\n",
      "-\n",
      " Average eval MAE loss: 58.01\n",
      "===============================\n",
      "MAE:  3.9176772\n",
      "MdAE:  2.8773818\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.70\n",
      "-\n",
      " Average eval MAE loss: 60.24\n",
      "===============================\n",
      "MAE:  3.9675975\n",
      "MdAE:  2.9062643\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.66\n",
      "-\n",
      " Average eval MAE loss: 58.20\n",
      "===============================\n",
      "MAE:  3.9087982\n",
      "MdAE:  2.9546728\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.43\n",
      "-\n",
      " Average eval MAE loss: 60.33\n",
      "===============================\n",
      "MAE:  3.9852111\n",
      "MdAE:  2.7280483\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.42\n",
      "-\n",
      " Average eval MAE loss: 58.32\n",
      "===============================\n",
      "MAE:  3.9393485\n",
      "MdAE:  2.9015331\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.23\n",
      "-\n",
      " Average eval MAE loss: 58.69\n",
      "===============================\n",
      "MAE:  3.9683895\n",
      "MdAE:  2.8877208\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.15\n",
      "-\n",
      " Average eval MAE loss: 59.17\n",
      "===============================\n",
      "MAE:  3.9362228\n",
      "MdAE:  2.8702593\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 59.15\n",
      "===============================\n",
      "MAE:  3.9541576\n",
      "MdAE:  2.8790884\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 58.97\n",
      "===============================\n",
      "MAE:  3.9461462\n",
      "MdAE:  2.8816068\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 59.08\n",
      "===============================\n",
      "MAE:  3.9403777\n",
      "MdAE:  2.862255\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb1a185060741ff90724f6bc936fb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['springxd'], 'test': ['springxd']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 12.68\n",
      "-\n",
      " Average eval MAE loss: 5.11\n",
      "===============================\n",
      "MAE:  2.0789132\n",
      "MdAE:  1.877987\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 9.24\n",
      "-\n",
      " Average eval MAE loss: 8.59\n",
      "===============================\n",
      "MAE:  2.711019\n",
      "MdAE:  2.531328\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 5.97\n",
      "-\n",
      " Average eval MAE loss: 7.72\n",
      "===============================\n",
      "MAE:  2.3767464\n",
      "MdAE:  2.0808008\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.52\n",
      "-\n",
      " Average eval MAE loss: 5.57\n",
      "===============================\n",
      "MAE:  1.9125504\n",
      "MdAE:  1.4966627\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.60\n",
      "-\n",
      " Average eval MAE loss: 8.24\n",
      "===============================\n",
      "MAE:  2.3805702\n",
      "MdAE:  1.8280075\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.89\n",
      "-\n",
      " Average eval MAE loss: 6.37\n",
      "===============================\n",
      "MAE:  2.0439177\n",
      "MdAE:  1.5189184\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.52\n",
      "-\n",
      " Average eval MAE loss: 6.52\n",
      "===============================\n",
      "MAE:  2.085857\n",
      "MdAE:  1.6977963\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.43\n",
      "-\n",
      " Average eval MAE loss: 6.63\n",
      "===============================\n",
      "MAE:  2.1871524\n",
      "MdAE:  1.7837683\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.89\n",
      "-\n",
      " Average eval MAE loss: 5.49\n",
      "===============================\n",
      "MAE:  1.9046968\n",
      "MdAE:  1.4168689\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.81\n",
      "-\n",
      " Average eval MAE loss: 6.73\n",
      "===============================\n",
      "MAE:  2.249293\n",
      "MdAE:  1.8064177\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.59\n",
      "-\n",
      " Average eval MAE loss: 5.91\n",
      "===============================\n",
      "MAE:  2.0562007\n",
      "MdAE:  1.64969\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.39\n",
      "-\n",
      " Average eval MAE loss: 5.77\n",
      "===============================\n",
      "MAE:  1.9583112\n",
      "MdAE:  1.5587924\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.28\n",
      "-\n",
      " Average eval MAE loss: 5.82\n",
      "===============================\n",
      "MAE:  2.051545\n",
      "MdAE:  1.6686858\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.23\n",
      "-\n",
      " Average eval MAE loss: 5.89\n",
      "===============================\n",
      "MAE:  2.0079618\n",
      "MdAE:  1.5849667\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 5.82\n",
      "===============================\n",
      "MAE:  2.0036407\n",
      "MdAE:  1.6156116\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.14\n",
      "-\n",
      " Average eval MAE loss: 5.81\n",
      "===============================\n",
      "MAE:  2.0219932\n",
      "MdAE:  1.6465797\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 5.61\n",
      "===============================\n",
      "MAE:  1.9710501\n",
      "MdAE:  1.6041532\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 5.73\n",
      "===============================\n",
      "MAE:  2.0162723\n",
      "MdAE:  1.6485144\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 5.73\n",
      "===============================\n",
      "MAE:  2.005737\n",
      "MdAE:  1.6225669\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 5.72\n",
      "===============================\n",
      "MAE:  1.9967221\n",
      "MdAE:  1.5977191\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8317281b0dc34a81827f8b5d6431bfa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['talenddataquality'], 'test': ['talenddataquality']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 31.92\n",
      "-\n",
      " Average eval MAE loss: 26.76\n",
      "===============================\n",
      "MAE:  4.2486567\n",
      "MdAE:  4.464247\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 19.69\n",
      "-\n",
      " Average eval MAE loss: 30.52\n",
      "===============================\n",
      "MAE:  5.2487473\n",
      "MdAE:  4.9714565\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 12.02\n",
      "-\n",
      " Average eval MAE loss: 25.79\n",
      "===============================\n",
      "MAE:  3.8518445\n",
      "MdAE:  3.7937565\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 6.62\n",
      "-\n",
      " Average eval MAE loss: 26.64\n",
      "===============================\n",
      "MAE:  4.5436387\n",
      "MdAE:  4.3907194\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 4.14\n",
      "-\n",
      " Average eval MAE loss: 25.88\n",
      "===============================\n",
      "MAE:  4.4124956\n",
      "MdAE:  4.193137\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.86\n",
      "-\n",
      " Average eval MAE loss: 27.02\n",
      "===============================\n",
      "MAE:  4.5354385\n",
      "MdAE:  4.3904114\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.42\n",
      "-\n",
      " Average eval MAE loss: 24.19\n",
      "===============================\n",
      "MAE:  3.9337409\n",
      "MdAE:  3.801639\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.79\n",
      "-\n",
      " Average eval MAE loss: 26.70\n",
      "===============================\n",
      "MAE:  4.7445674\n",
      "MdAE:  4.637642\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.17\n",
      "-\n",
      " Average eval MAE loss: 24.98\n",
      "===============================\n",
      "MAE:  4.36573\n",
      "MdAE:  4.278036\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.53\n",
      "-\n",
      " Average eval MAE loss: 24.13\n",
      "===============================\n",
      "MAE:  4.0000005\n",
      "MdAE:  3.6912212\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.12\n",
      "-\n",
      " Average eval MAE loss: 25.33\n",
      "===============================\n",
      "MAE:  3.909934\n",
      "MdAE:  3.5680237\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.05\n",
      "-\n",
      " Average eval MAE loss: 24.69\n",
      "===============================\n",
      "MAE:  4.1374846\n",
      "MdAE:  4.0726495\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.54\n",
      "-\n",
      " Average eval MAE loss: 25.67\n",
      "===============================\n",
      "MAE:  4.488684\n",
      "MdAE:  4.3982797\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.49\n",
      "-\n",
      " Average eval MAE loss: 24.90\n",
      "===============================\n",
      "MAE:  4.155739\n",
      "MdAE:  4.041761\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.37\n",
      "-\n",
      " Average eval MAE loss: 25.45\n",
      "===============================\n",
      "MAE:  4.288286\n",
      "MdAE:  4.141087\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 24.56\n",
      "===============================\n",
      "MAE:  4.1609797\n",
      "MdAE:  4.096937\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.21\n",
      "-\n",
      " Average eval MAE loss: 24.98\n",
      "===============================\n",
      "MAE:  4.2213817\n",
      "MdAE:  4.1270456\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.21\n",
      "-\n",
      " Average eval MAE loss: 24.87\n",
      "===============================\n",
      "MAE:  4.184584\n",
      "MdAE:  4.0646253\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.14\n",
      "-\n",
      " Average eval MAE loss: 25.05\n",
      "===============================\n",
      "MAE:  4.2603297\n",
      "MdAE:  4.1644583\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 24.93\n",
      "===============================\n",
      "MAE:  4.2050457\n",
      "MdAE:  4.0945606\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9addd91d1db4a779d2535d36ce4007a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['talendesb'], 'test': ['talendesb']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 3.14\n",
      "-\n",
      " Average eval MAE loss: 2.53\n",
      "===============================\n",
      "MAE:  0.9476132\n",
      "MdAE:  0.6733115\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 1.55\n",
      "-\n",
      " Average eval MAE loss: 2.69\n",
      "===============================\n",
      "MAE:  0.98689026\n",
      "MdAE:  0.70084685\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.01\n",
      "-\n",
      " Average eval MAE loss: 2.81\n",
      "===============================\n",
      "MAE:  1.0082505\n",
      "MdAE:  0.69129074\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 0.52\n",
      "-\n",
      " Average eval MAE loss: 2.89\n",
      "===============================\n",
      "MAE:  0.9756031\n",
      "MdAE:  0.6125437\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 0.33\n",
      "-\n",
      " Average eval MAE loss: 2.78\n",
      "===============================\n",
      "MAE:  0.9786424\n",
      "MdAE:  0.7171731\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 0.39\n",
      "-\n",
      " Average eval MAE loss: 2.72\n",
      "===============================\n",
      "MAE:  0.9490034\n",
      "MdAE:  0.68289924\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 2.98\n",
      "===============================\n",
      "MAE:  1.0647031\n",
      "MdAE:  0.76908696\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.26\n",
      "-\n",
      " Average eval MAE loss: 2.48\n",
      "===============================\n",
      "MAE:  0.91706294\n",
      "MdAE:  0.64348507\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.21\n",
      "-\n",
      " Average eval MAE loss: 2.59\n",
      "===============================\n",
      "MAE:  0.9469242\n",
      "MdAE:  0.67161417\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.10\n",
      "-\n",
      " Average eval MAE loss: 2.76\n",
      "===============================\n",
      "MAE:  0.9386249\n",
      "MdAE:  0.6816269\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 2.70\n",
      "===============================\n",
      "MAE:  1.0336717\n",
      "MdAE:  0.78939354\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.06\n",
      "-\n",
      " Average eval MAE loss: 2.66\n",
      "===============================\n",
      "MAE:  0.9691359\n",
      "MdAE:  0.6727144\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 2.58\n",
      "===============================\n",
      "MAE:  1.0046176\n",
      "MdAE:  0.79124063\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 2.64\n",
      "===============================\n",
      "MAE:  0.9796876\n",
      "MdAE:  0.7215665\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.61\n",
      "===============================\n",
      "MAE:  0.96137106\n",
      "MdAE:  0.67409647\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 2.67\n",
      "===============================\n",
      "MAE:  1.0366473\n",
      "MdAE:  0.81504285\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 2.65\n",
      "===============================\n",
      "MAE:  1.010604\n",
      "MdAE:  0.7527659\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.62\n",
      "===============================\n",
      "MAE:  0.9921241\n",
      "MdAE:  0.7352569\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.01\n",
      "-\n",
      " Average eval MAE loss: 2.62\n",
      "===============================\n",
      "MAE:  0.9724532\n",
      "MdAE:  0.7056922\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.01\n",
      "-\n",
      " Average eval MAE loss: 2.62\n",
      "===============================\n",
      "MAE:  0.98728156\n",
      "MdAE:  0.72819185\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2826c65df35d45cfad9ba46acf8e66f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['titanium'], 'test': ['titanium']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 35.24\n",
      "-\n",
      " Average eval MAE loss: 15.98\n",
      "===============================\n",
      "MAE:  3.0147767\n",
      "MdAE:  2.5986547\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 21.38\n",
      "-\n",
      " Average eval MAE loss: 14.99\n",
      "===============================\n",
      "MAE:  2.7407074\n",
      "MdAE:  2.1813853\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 14.45\n",
      "-\n",
      " Average eval MAE loss: 22.17\n",
      "===============================\n",
      "MAE:  3.5770543\n",
      "MdAE:  3.1264205\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 9.84\n",
      "-\n",
      " Average eval MAE loss: 18.45\n",
      "===============================\n",
      "MAE:  2.7557676\n",
      "MdAE:  2.0875545\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 5.42\n",
      "-\n",
      " Average eval MAE loss: 19.56\n",
      "===============================\n",
      "MAE:  2.9054227\n",
      "MdAE:  2.2184887\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.81\n",
      "-\n",
      " Average eval MAE loss: 16.87\n",
      "===============================\n",
      "MAE:  2.7108357\n",
      "MdAE:  2.06039\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 3.47\n",
      "-\n",
      " Average eval MAE loss: 17.36\n",
      "===============================\n",
      "MAE:  2.7333567\n",
      "MdAE:  2.049952\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.22\n",
      "-\n",
      " Average eval MAE loss: 17.30\n",
      "===============================\n",
      "MAE:  2.8078113\n",
      "MdAE:  2.116334\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.98\n",
      "-\n",
      " Average eval MAE loss: 20.53\n",
      "===============================\n",
      "MAE:  3.0036213\n",
      "MdAE:  2.3949375\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.52\n",
      "-\n",
      " Average eval MAE loss: 16.50\n",
      "===============================\n",
      "MAE:  2.5625257\n",
      "MdAE:  1.8641992\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.22\n",
      "-\n",
      " Average eval MAE loss: 17.29\n",
      "===============================\n",
      "MAE:  2.7564857\n",
      "MdAE:  2.2237926\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.65\n",
      "-\n",
      " Average eval MAE loss: 18.07\n",
      "===============================\n",
      "MAE:  2.753646\n",
      "MdAE:  2.1872706\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.81\n",
      "-\n",
      " Average eval MAE loss: 18.04\n",
      "===============================\n",
      "MAE:  2.7232466\n",
      "MdAE:  2.0743766\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.48\n",
      "-\n",
      " Average eval MAE loss: 18.26\n",
      "===============================\n",
      "MAE:  2.7429154\n",
      "MdAE:  2.1177142\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.47\n",
      "-\n",
      " Average eval MAE loss: 17.24\n",
      "===============================\n",
      "MAE:  2.7109451\n",
      "MdAE:  2.1557035\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.35\n",
      "-\n",
      " Average eval MAE loss: 17.20\n",
      "===============================\n",
      "MAE:  2.6658976\n",
      "MdAE:  2.0693207\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.22\n",
      "-\n",
      " Average eval MAE loss: 17.95\n",
      "===============================\n",
      "MAE:  2.7585275\n",
      "MdAE:  2.1519508\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 17.70\n",
      "===============================\n",
      "MAE:  2.735666\n",
      "MdAE:  2.1420355\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 17.95\n",
      "===============================\n",
      "MAE:  2.7321715\n",
      "MdAE:  2.1838126\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 17.79\n",
      "===============================\n",
      "MAE:  2.7246838\n",
      "MdAE:  2.1882496\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b63c39a0b6d43099edbe17f07946b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "Start training for  {'train': ['usergrid'], 'test': ['usergrid']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 10.88\n",
      "-\n",
      " Average eval MAE loss: 2.63\n",
      "===============================\n",
      "MAE:  1.4812906\n",
      "MdAE:  1.2713976\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 1.49\n",
      "-\n",
      " Average eval MAE loss: 2.56\n",
      "===============================\n",
      "MAE:  1.4864702\n",
      "MdAE:  1.1901793\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.20\n",
      "-\n",
      " Average eval MAE loss: 2.65\n",
      "===============================\n",
      "MAE:  1.4564929\n",
      "MdAE:  1.1896538\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 0.59\n",
      "-\n",
      " Average eval MAE loss: 2.58\n",
      "===============================\n",
      "MAE:  1.4871763\n",
      "MdAE:  1.2894163\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 2.51\n",
      "===============================\n",
      "MAE:  1.4898107\n",
      "MdAE:  1.2936151\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 0.21\n",
      "-\n",
      " Average eval MAE loss: 2.43\n",
      "===============================\n",
      "MAE:  1.4644948\n",
      "MdAE:  1.2801867\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.18\n",
      "-\n",
      " Average eval MAE loss: 2.65\n",
      "===============================\n",
      "MAE:  1.493079\n",
      "MdAE:  1.2983723\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 2.39\n",
      "===============================\n",
      "MAE:  1.4096755\n",
      "MdAE:  1.2168028\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 2.45\n",
      "===============================\n",
      "MAE:  1.485852\n",
      "MdAE:  1.268296\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 2.41\n",
      "===============================\n",
      "MAE:  1.4864166\n",
      "MdAE:  1.3454869\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 2.38\n",
      "===============================\n",
      "MAE:  1.4242867\n",
      "MdAE:  1.2390742\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.08\n",
      "-\n",
      " Average eval MAE loss: 2.39\n",
      "===============================\n",
      "MAE:  1.4546845\n",
      "MdAE:  1.3058597\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 2.43\n",
      "===============================\n",
      "MAE:  1.4753267\n",
      "MdAE:  1.392683\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.39\n",
      "===============================\n",
      "MAE:  1.4274331\n",
      "MdAE:  1.2516863\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.41\n",
      "===============================\n",
      "MAE:  1.4592919\n",
      "MdAE:  1.3293438\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 2.45\n",
      "===============================\n",
      "MAE:  1.4588449\n",
      "MdAE:  1.3152609\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 2.41\n",
      "===============================\n",
      "MAE:  1.4339229\n",
      "MdAE:  1.3212242\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.41\n",
      "===============================\n",
      "MAE:  1.4376289\n",
      "MdAE:  1.3285055\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.42\n",
      "===============================\n",
      "MAE:  1.4425153\n",
      "MdAE:  1.332983\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.41\n",
      "===============================\n",
      "MAE:  1.4421767\n",
      "MdAE:  1.366035\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    }
   ],
   "source": [
    "global WITHIN_PROJECT, BATCH_SIZE_RATIO\n",
    "WITHIN_PROJECT = True\n",
    "BATCH_SIZE_RATIO = 0.3\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['bamboo'], 'test': ['bamboo']},\n",
    "                        {'train': ['clover'], 'test': ['clover']},\n",
    "                        {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
    "                        {'train': ['duracloud'], 'test': ['duracloud']},\n",
    "                        {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
    "                        {'train': ['mesos'], 'test': ['mesos']},\n",
    "                        {'train': ['moodle'], 'test': ['moodle']},\n",
    "                        {'train': ['mule'], 'test': ['mule']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['springxd'], 'test': ['springxd']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
    "                        {'train': ['talendesb'], 'test': ['talendesb']},\n",
    "                        {'train': ['titanium'], 'test': ['titanium']},\n",
    "                        {'train': ['usergrid'], 'test': ['usergrid']},\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME, HF_MODEL_NAME\n",
    "\n",
    "    # Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
    "    # Quantization Config (for QLORA)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16, # Changed to float16 for lower memory usage\n",
    "    )\n",
    "    # Lora Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Reduced from 16 to 8 for lower memory usage\n",
    "        lora_alpha=16,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        optimize_memory()\n",
    "\n",
    "        # Config for Lama3 model\n",
    "        config = AutoConfig.from_pretrained(HF_MODEL_NAME, num_labels=1)\n",
    "        if MODEL_NAME == 'llama3':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'llama3sp':\n",
    "            MODEL = LLAMA3SP.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'deepseek':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'qwen':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        # prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "        MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "        # get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model\n",
    "        MODEL = get_peft_model(MODEL, lora_config)\n",
    "\n",
    "        # additional memory optimizations\n",
    "        MODEL.gradient_checkpointing_enable()  # Reduce memory usage during training\n",
    "        MODEL.enable_input_require_grads()\n",
    "\n",
    "        if TOKENIZER == 'wordlevel':\n",
    "            MODEL.config.pad_token_id = 3\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        \n",
    "        MODEL.cuda()\n",
    "\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        optimize_memory()\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Within Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f549826aba4d739a6927bee26c0ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['mesos'], 'test': ['usergrid']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 8.72\n",
      "-\n",
      " Average eval MAE loss: 3.25\n",
      "===============================\n",
      "MAE:  1.2458876\n",
      "MdAE:  0.9863001\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 5.26\n",
      "-\n",
      " Average eval MAE loss: 4.30\n",
      "===============================\n",
      "MAE:  1.5236213\n",
      "MdAE:  1.1998624\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 2.85\n",
      "-\n",
      " Average eval MAE loss: 3.69\n",
      "===============================\n",
      "MAE:  1.2884929\n",
      "MdAE:  0.98581505\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 2.19\n",
      "-\n",
      " Average eval MAE loss: 5.23\n",
      "===============================\n",
      "MAE:  1.5182556\n",
      "MdAE:  1.1842268\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.49\n",
      "-\n",
      " Average eval MAE loss: 4.79\n",
      "===============================\n",
      "MAE:  1.4671437\n",
      "MdAE:  1.1367322\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.27\n",
      "-\n",
      " Average eval MAE loss: 3.71\n",
      "===============================\n",
      "MAE:  1.4234837\n",
      "MdAE:  1.1407456\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.83\n",
      "-\n",
      " Average eval MAE loss: 3.85\n",
      "===============================\n",
      "MAE:  1.3966287\n",
      "MdAE:  1.0445935\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.81\n",
      "-\n",
      " Average eval MAE loss: 3.79\n",
      "===============================\n",
      "MAE:  1.3954684\n",
      "MdAE:  1.1219132\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.50\n",
      "-\n",
      " Average eval MAE loss: 3.72\n",
      "===============================\n",
      "MAE:  1.3823622\n",
      "MdAE:  1.0787255\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.09\n",
      "-\n",
      " Average eval MAE loss: 3.76\n",
      "===============================\n",
      "MAE:  1.4068259\n",
      "MdAE:  1.1009071\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.46\n",
      "-\n",
      " Average eval MAE loss: 4.43\n",
      "===============================\n",
      "MAE:  1.4962988\n",
      "MdAE:  1.160392\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.52\n",
      "-\n",
      " Average eval MAE loss: 3.99\n",
      "===============================\n",
      "MAE:  1.386983\n",
      "MdAE:  1.0938694\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.23\n",
      "-\n",
      " Average eval MAE loss: 3.98\n",
      "===============================\n",
      "MAE:  1.4168416\n",
      "MdAE:  1.0985233\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.28\n",
      "-\n",
      " Average eval MAE loss: 3.70\n",
      "===============================\n",
      "MAE:  1.3945738\n",
      "MdAE:  1.1264833\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.19\n",
      "-\n",
      " Average eval MAE loss: 3.96\n",
      "===============================\n",
      "MAE:  1.4303796\n",
      "MdAE:  1.1244047\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 3.81\n",
      "===============================\n",
      "MAE:  1.4199212\n",
      "MdAE:  1.1273856\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 3.87\n",
      "===============================\n",
      "MAE:  1.4046973\n",
      "MdAE:  1.0891125\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 3.90\n",
      "===============================\n",
      "MAE:  1.3958675\n",
      "MdAE:  1.066441\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 3.88\n",
      "===============================\n",
      "MAE:  1.4023014\n",
      "MdAE:  1.0742121\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 3.85\n",
      "===============================\n",
      "MAE:  1.4032996\n",
      "MdAE:  1.0831349\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8a7c8d899945628eafbe6aec7710a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['usergrid'], 'test': ['mesos']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 11.28\n",
      "-\n",
      " Average eval MAE loss: 3.10\n",
      "===============================\n",
      "MAE:  1.7041667\n",
      "MdAE:  1.198221\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 1.47\n",
      "-\n",
      " Average eval MAE loss: 2.68\n",
      "===============================\n",
      "MAE:  1.6804981\n",
      "MdAE:  1.3477235\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 1.30\n",
      "-\n",
      " Average eval MAE loss: 2.55\n",
      "===============================\n",
      "MAE:  1.6504104\n",
      "MdAE:  1.3013399\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 0.54\n",
      "-\n",
      " Average eval MAE loss: 2.69\n",
      "===============================\n",
      "MAE:  1.6573416\n",
      "MdAE:  1.2434778\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 0.23\n",
      "-\n",
      " Average eval MAE loss: 2.69\n",
      "===============================\n",
      "MAE:  1.6681812\n",
      "MdAE:  1.3330007\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 0.18\n",
      "-\n",
      " Average eval MAE loss: 2.66\n",
      "===============================\n",
      "MAE:  1.7107381\n",
      "MdAE:  1.4061103\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 2.81\n",
      "===============================\n",
      "MAE:  1.7306852\n",
      "MdAE:  1.3983037\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.10\n",
      "-\n",
      " Average eval MAE loss: 2.60\n",
      "===============================\n",
      "MAE:  1.6395961\n",
      "MdAE:  1.2812127\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.15\n",
      "-\n",
      " Average eval MAE loss: 2.64\n",
      "===============================\n",
      "MAE:  1.6928271\n",
      "MdAE:  1.3829521\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 2.80\n",
      "===============================\n",
      "MAE:  1.7382089\n",
      "MdAE:  1.4150709\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.08\n",
      "-\n",
      " Average eval MAE loss: 2.64\n",
      "===============================\n",
      "MAE:  1.6806383\n",
      "MdAE:  1.3582275\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 2.64\n",
      "===============================\n",
      "MAE:  1.659965\n",
      "MdAE:  1.3169998\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 2.65\n",
      "===============================\n",
      "MAE:  1.6676072\n",
      "MdAE:  1.3314359\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 2.56\n",
      "===============================\n",
      "MAE:  1.6483568\n",
      "MdAE:  1.3165433\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.66\n",
      "===============================\n",
      "MAE:  1.6682463\n",
      "MdAE:  1.3460486\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.58\n",
      "===============================\n",
      "MAE:  1.6394125\n",
      "MdAE:  1.2907652\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.56\n",
      "===============================\n",
      "MAE:  1.6422503\n",
      "MdAE:  1.3032761\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.60\n",
      "===============================\n",
      "MAE:  1.6596965\n",
      "MdAE:  1.3359267\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.01\n",
      "-\n",
      " Average eval MAE loss: 2.62\n",
      "===============================\n",
      "MAE:  1.6615154\n",
      "MdAE:  1.3351597\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.01\n",
      "-\n",
      " Average eval MAE loss: 2.61\n",
      "===============================\n",
      "MAE:  1.6571828\n",
      "MdAE:  1.3285526\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb49ed573506412bac0e98e8a4f695ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['aptanastudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 16.39\n",
      "-\n",
      " Average eval MAE loss: 4.00\n",
      "===============================\n",
      "MAE:  4.630319\n",
      "MdAE:  3.4143634\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 13.13\n",
      "-\n",
      " Average eval MAE loss: 7.09\n",
      "===============================\n",
      "MAE:  3.9511602\n",
      "MdAE:  2.7448945\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 8.39\n",
      "-\n",
      " Average eval MAE loss: 5.40\n",
      "===============================\n",
      "MAE:  4.1327276\n",
      "MdAE:  2.9493499\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 4.93\n",
      "-\n",
      " Average eval MAE loss: 6.62\n",
      "===============================\n",
      "MAE:  4.0519123\n",
      "MdAE:  2.6906977\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.10\n",
      "-\n",
      " Average eval MAE loss: 6.15\n",
      "===============================\n",
      "MAE:  4.345239\n",
      "MdAE:  3.1425018\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.28\n",
      "-\n",
      " Average eval MAE loss: 7.88\n",
      "===============================\n",
      "MAE:  3.9523427\n",
      "MdAE:  2.6837535\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.91\n",
      "-\n",
      " Average eval MAE loss: 8.85\n",
      "===============================\n",
      "MAE:  3.967901\n",
      "MdAE:  2.6466503\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.87\n",
      "-\n",
      " Average eval MAE loss: 6.64\n",
      "===============================\n",
      "MAE:  3.9087799\n",
      "MdAE:  2.5884871\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.12\n",
      "-\n",
      " Average eval MAE loss: 7.02\n",
      "===============================\n",
      "MAE:  3.915617\n",
      "MdAE:  2.711392\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.02\n",
      "-\n",
      " Average eval MAE loss: 7.02\n",
      "===============================\n",
      "MAE:  3.9280753\n",
      "MdAE:  2.6122565\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.78\n",
      "-\n",
      " Average eval MAE loss: 6.69\n",
      "===============================\n",
      "MAE:  3.9197142\n",
      "MdAE:  2.6776314\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.61\n",
      "-\n",
      " Average eval MAE loss: 5.68\n",
      "===============================\n",
      "MAE:  3.9325256\n",
      "MdAE:  2.6889153\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.58\n",
      "-\n",
      " Average eval MAE loss: 6.43\n",
      "===============================\n",
      "MAE:  3.857052\n",
      "MdAE:  2.5447564\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.29\n",
      "-\n",
      " Average eval MAE loss: 6.61\n",
      "===============================\n",
      "MAE:  3.8465395\n",
      "MdAE:  2.558415\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.40\n",
      "-\n",
      " Average eval MAE loss: 6.90\n",
      "===============================\n",
      "MAE:  3.8321774\n",
      "MdAE:  2.502089\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.16\n",
      "-\n",
      " Average eval MAE loss: 6.49\n",
      "===============================\n",
      "MAE:  3.8689764\n",
      "MdAE:  2.555729\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 6.69\n",
      "===============================\n",
      "MAE:  3.839094\n",
      "MdAE:  2.5141716\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.08\n",
      "-\n",
      " Average eval MAE loss: 6.26\n",
      "===============================\n",
      "MAE:  3.8571455\n",
      "MdAE:  2.6149464\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 6.44\n",
      "===============================\n",
      "MAE:  3.86354\n",
      "MdAE:  2.5703602\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 6.33\n",
      "===============================\n",
      "MAE:  3.8543243\n",
      "MdAE:  2.5763094\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67949bdb45054d10b8e75c0544de3978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['titanium']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 19.69\n",
      "-\n",
      " Average eval MAE loss: 5.73\n",
      "===============================\n",
      "MAE:  3.5479658\n",
      "MdAE:  2.5609531\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 13.52\n",
      "-\n",
      " Average eval MAE loss: 4.72\n",
      "===============================\n",
      "MAE:  3.2817395\n",
      "MdAE:  2.2235017\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 9.45\n",
      "-\n",
      " Average eval MAE loss: 5.21\n",
      "===============================\n",
      "MAE:  3.4180686\n",
      "MdAE:  2.271946\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 7.08\n",
      "-\n",
      " Average eval MAE loss: 6.63\n",
      "===============================\n",
      "MAE:  3.4712348\n",
      "MdAE:  2.2989922\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 5.00\n",
      "-\n",
      " Average eval MAE loss: 6.43\n",
      "===============================\n",
      "MAE:  3.5035808\n",
      "MdAE:  2.5060797\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.67\n",
      "-\n",
      " Average eval MAE loss: 5.35\n",
      "===============================\n",
      "MAE:  3.458964\n",
      "MdAE:  2.2358723\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.24\n",
      "-\n",
      " Average eval MAE loss: 7.60\n",
      "===============================\n",
      "MAE:  3.504374\n",
      "MdAE:  2.5334654\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.79\n",
      "-\n",
      " Average eval MAE loss: 5.63\n",
      "===============================\n",
      "MAE:  3.4730692\n",
      "MdAE:  2.4192963\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.88\n",
      "-\n",
      " Average eval MAE loss: 5.93\n",
      "===============================\n",
      "MAE:  3.4254746\n",
      "MdAE:  2.3398743\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.35\n",
      "-\n",
      " Average eval MAE loss: 6.10\n",
      "===============================\n",
      "MAE:  3.4479494\n",
      "MdAE:  2.355027\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.60\n",
      "-\n",
      " Average eval MAE loss: 5.33\n",
      "===============================\n",
      "MAE:  3.405174\n",
      "MdAE:  2.2469096\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.06\n",
      "-\n",
      " Average eval MAE loss: 7.40\n",
      "===============================\n",
      "MAE:  3.5000415\n",
      "MdAE:  2.5298357\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.76\n",
      "-\n",
      " Average eval MAE loss: 6.76\n",
      "===============================\n",
      "MAE:  3.5470524\n",
      "MdAE:  2.5637507\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.48\n",
      "-\n",
      " Average eval MAE loss: 7.21\n",
      "===============================\n",
      "MAE:  3.4748285\n",
      "MdAE:  2.4808137\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.34\n",
      "-\n",
      " Average eval MAE loss: 6.82\n",
      "===============================\n",
      "MAE:  3.51146\n",
      "MdAE:  2.5262246\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.38\n",
      "-\n",
      " Average eval MAE loss: 6.34\n",
      "===============================\n",
      "MAE:  3.4598658\n",
      "MdAE:  2.4446611\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.19\n",
      "-\n",
      " Average eval MAE loss: 6.44\n",
      "===============================\n",
      "MAE:  3.462416\n",
      "MdAE:  2.4628983\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 6.63\n",
      "===============================\n",
      "MAE:  3.4806263\n",
      "MdAE:  2.4905005\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 6.59\n",
      "===============================\n",
      "MAE:  3.4665828\n",
      "MdAE:  2.49407\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.06\n",
      "-\n",
      " Average eval MAE loss: 6.48\n",
      "===============================\n",
      "MAE:  3.4667149\n",
      "MdAE:  2.488068\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2115f8f345934dcea9fa5cda8d0c6381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['titanium'], 'test': ['appceleratorstudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 31.40\n",
      "-\n",
      " Average eval MAE loss: 15.80\n",
      "===============================\n",
      "MAE:  2.637276\n",
      "MdAE:  2.082324\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 21.50\n",
      "-\n",
      " Average eval MAE loss: 15.24\n",
      "===============================\n",
      "MAE:  2.587139\n",
      "MdAE:  1.9294453\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 13.26\n",
      "-\n",
      " Average eval MAE loss: 21.07\n",
      "===============================\n",
      "MAE:  2.7973397\n",
      "MdAE:  2.067708\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 8.38\n",
      "-\n",
      " Average eval MAE loss: 15.37\n",
      "===============================\n",
      "MAE:  2.6518965\n",
      "MdAE:  2.015575\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 5.46\n",
      "-\n",
      " Average eval MAE loss: 17.63\n",
      "===============================\n",
      "MAE:  2.6345232\n",
      "MdAE:  1.954257\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 4.85\n",
      "-\n",
      " Average eval MAE loss: 18.81\n",
      "===============================\n",
      "MAE:  2.7492201\n",
      "MdAE:  2.0748968\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.88\n",
      "-\n",
      " Average eval MAE loss: 17.72\n",
      "===============================\n",
      "MAE:  2.6555035\n",
      "MdAE:  2.0259576\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.85\n",
      "-\n",
      " Average eval MAE loss: 15.14\n",
      "===============================\n",
      "MAE:  2.6499631\n",
      "MdAE:  1.9442859\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.05\n",
      "-\n",
      " Average eval MAE loss: 16.80\n",
      "===============================\n",
      "MAE:  2.7505221\n",
      "MdAE:  2.0516024\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.24\n",
      "-\n",
      " Average eval MAE loss: 15.83\n",
      "===============================\n",
      "MAE:  2.6643763\n",
      "MdAE:  1.9906588\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.27\n",
      "-\n",
      " Average eval MAE loss: 16.10\n",
      "===============================\n",
      "MAE:  2.6274297\n",
      "MdAE:  1.9820371\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.88\n",
      "-\n",
      " Average eval MAE loss: 16.00\n",
      "===============================\n",
      "MAE:  2.6196752\n",
      "MdAE:  1.9536514\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.75\n",
      "-\n",
      " Average eval MAE loss: 16.59\n",
      "===============================\n",
      "MAE:  2.679367\n",
      "MdAE:  1.9701166\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.46\n",
      "-\n",
      " Average eval MAE loss: 15.95\n",
      "===============================\n",
      "MAE:  2.625961\n",
      "MdAE:  1.9611712\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.44\n",
      "-\n",
      " Average eval MAE loss: 15.70\n",
      "===============================\n",
      "MAE:  2.62571\n",
      "MdAE:  1.9379103\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.33\n",
      "-\n",
      " Average eval MAE loss: 16.37\n",
      "===============================\n",
      "MAE:  2.614771\n",
      "MdAE:  1.969522\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.16\n",
      "-\n",
      " Average eval MAE loss: 15.91\n",
      "===============================\n",
      "MAE:  2.6256657\n",
      "MdAE:  1.9279699\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.17\n",
      "-\n",
      " Average eval MAE loss: 16.07\n",
      "===============================\n",
      "MAE:  2.6203604\n",
      "MdAE:  1.9369879\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 16.06\n",
      "===============================\n",
      "MAE:  2.6222966\n",
      "MdAE:  1.9390516\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 16.04\n",
      "===============================\n",
      "MAE:  2.620167\n",
      "MdAE:  1.9356127\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8833174929804986a7165f3eeb062f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['aptanastudio'], 'test': ['titanium']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 39.36\n",
      "-\n",
      " Average eval MAE loss: 34.41\n",
      "===============================\n",
      "MAE:  3.5362182\n",
      "MdAE:  2.5565157\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 29.20\n",
      "-\n",
      " Average eval MAE loss: 34.14\n",
      "===============================\n",
      "MAE:  3.6454935\n",
      "MdAE:  2.806098\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 18.89\n",
      "-\n",
      " Average eval MAE loss: 34.21\n",
      "===============================\n",
      "MAE:  3.7638392\n",
      "MdAE:  2.8902502\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 8.76\n",
      "-\n",
      " Average eval MAE loss: 34.41\n",
      "===============================\n",
      "MAE:  4.611096\n",
      "MdAE:  4.0659943\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 5.60\n",
      "-\n",
      " Average eval MAE loss: 34.59\n",
      "===============================\n",
      "MAE:  3.9136565\n",
      "MdAE:  3.0465822\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 4.86\n",
      "-\n",
      " Average eval MAE loss: 32.91\n",
      "===============================\n",
      "MAE:  4.532384\n",
      "MdAE:  3.9551806\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.29\n",
      "-\n",
      " Average eval MAE loss: 32.02\n",
      "===============================\n",
      "MAE:  4.33021\n",
      "MdAE:  3.7094507\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.26\n",
      "-\n",
      " Average eval MAE loss: 33.60\n",
      "===============================\n",
      "MAE:  4.8807616\n",
      "MdAE:  4.4287415\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.01\n",
      "-\n",
      " Average eval MAE loss: 32.52\n",
      "===============================\n",
      "MAE:  4.408179\n",
      "MdAE:  3.814825\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.39\n",
      "-\n",
      " Average eval MAE loss: 31.97\n",
      "===============================\n",
      "MAE:  4.187707\n",
      "MdAE:  3.5412922\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.07\n",
      "-\n",
      " Average eval MAE loss: 33.25\n",
      "===============================\n",
      "MAE:  4.5489836\n",
      "MdAE:  4.014188\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.82\n",
      "-\n",
      " Average eval MAE loss: 32.77\n",
      "===============================\n",
      "MAE:  4.4812384\n",
      "MdAE:  3.9441433\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.86\n",
      "-\n",
      " Average eval MAE loss: 31.79\n",
      "===============================\n",
      "MAE:  4.390872\n",
      "MdAE:  3.8776283\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.55\n",
      "-\n",
      " Average eval MAE loss: 32.47\n",
      "===============================\n",
      "MAE:  4.546355\n",
      "MdAE:  3.9739075\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.44\n",
      "-\n",
      " Average eval MAE loss: 32.23\n",
      "===============================\n",
      "MAE:  4.428341\n",
      "MdAE:  3.8581276\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.35\n",
      "-\n",
      " Average eval MAE loss: 32.14\n",
      "===============================\n",
      "MAE:  4.414971\n",
      "MdAE:  3.8603797\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 32.46\n",
      "===============================\n",
      "MAE:  4.526568\n",
      "MdAE:  3.986557\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 32.31\n",
      "===============================\n",
      "MAE:  4.510465\n",
      "MdAE:  4.0069427\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 32.44\n",
      "===============================\n",
      "MAE:  4.51745\n",
      "MdAE:  3.9708376\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.08\n",
      "-\n",
      " Average eval MAE loss: 32.26\n",
      "===============================\n",
      "MAE:  4.4695067\n",
      "MdAE:  3.9196277\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630c5e63f69c4f7b9c848d0a48c75b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['mule'], 'test': ['mulestudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 13.81\n",
      "-\n",
      " Average eval MAE loss: 10.22\n",
      "===============================\n",
      "MAE:  3.5305417\n",
      "MdAE:  2.5355532\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 8.69\n",
      "-\n",
      " Average eval MAE loss: 10.11\n",
      "===============================\n",
      "MAE:  3.6041436\n",
      "MdAE:  2.3004165\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 5.84\n",
      "-\n",
      " Average eval MAE loss: 10.64\n",
      "===============================\n",
      "MAE:  3.7072618\n",
      "MdAE:  2.288129\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.15\n",
      "-\n",
      " Average eval MAE loss: 10.55\n",
      "===============================\n",
      "MAE:  3.6083584\n",
      "MdAE:  2.5387995\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 1.59\n",
      "-\n",
      " Average eval MAE loss: 10.44\n",
      "===============================\n",
      "MAE:  3.6716852\n",
      "MdAE:  2.5071654\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.40\n",
      "-\n",
      " Average eval MAE loss: 11.63\n",
      "===============================\n",
      "MAE:  3.7419276\n",
      "MdAE:  2.6820593\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.28\n",
      "-\n",
      " Average eval MAE loss: 10.40\n",
      "===============================\n",
      "MAE:  3.5965905\n",
      "MdAE:  2.5059829\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.61\n",
      "-\n",
      " Average eval MAE loss: 10.56\n",
      "===============================\n",
      "MAE:  3.6184876\n",
      "MdAE:  2.5647755\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.56\n",
      "-\n",
      " Average eval MAE loss: 10.35\n",
      "===============================\n",
      "MAE:  3.6388547\n",
      "MdAE:  2.5885274\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.64\n",
      "-\n",
      " Average eval MAE loss: 10.99\n",
      "===============================\n",
      "MAE:  3.64815\n",
      "MdAE:  2.6111\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.47\n",
      "-\n",
      " Average eval MAE loss: 10.08\n",
      "===============================\n",
      "MAE:  3.6123214\n",
      "MdAE:  2.4889526\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 10.53\n",
      "===============================\n",
      "MAE:  3.623601\n",
      "MdAE:  2.5941193\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 10.45\n",
      "===============================\n",
      "MAE:  3.6052272\n",
      "MdAE:  2.5361047\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 10.32\n",
      "===============================\n",
      "MAE:  3.6295185\n",
      "MdAE:  2.540606\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 10.65\n",
      "===============================\n",
      "MAE:  3.6388552\n",
      "MdAE:  2.6113431\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.10\n",
      "-\n",
      " Average eval MAE loss: 10.37\n",
      "===============================\n",
      "MAE:  3.6124268\n",
      "MdAE:  2.5602784\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 10.51\n",
      "===============================\n",
      "MAE:  3.6105096\n",
      "MdAE:  2.5800917\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.06\n",
      "-\n",
      " Average eval MAE loss: 10.40\n",
      "===============================\n",
      "MAE:  3.6061184\n",
      "MdAE:  2.5772774\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 10.39\n",
      "===============================\n",
      "MAE:  3.6189485\n",
      "MdAE:  2.5787115\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 10.36\n",
      "===============================\n",
      "MAE:  3.6136425\n",
      "MdAE:  2.5805154\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5c4447e94e41c996225f0f3ee6c8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['mulestudio'], 'test': ['mule']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 26.45\n",
      "-\n",
      " Average eval MAE loss: 45.33\n",
      "===============================\n",
      "MAE:  2.7474277\n",
      "MdAE:  2.4203367\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 16.94\n",
      "-\n",
      " Average eval MAE loss: 38.08\n",
      "===============================\n",
      "MAE:  3.0882294\n",
      "MdAE:  2.6277776\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 9.90\n",
      "-\n",
      " Average eval MAE loss: 44.77\n",
      "===============================\n",
      "MAE:  3.0148852\n",
      "MdAE:  2.315507\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 5.64\n",
      "-\n",
      " Average eval MAE loss: 44.39\n",
      "===============================\n",
      "MAE:  2.7244713\n",
      "MdAE:  2.2782896\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.85\n",
      "-\n",
      " Average eval MAE loss: 40.51\n",
      "===============================\n",
      "MAE:  2.982663\n",
      "MdAE:  2.4238725\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.72\n",
      "-\n",
      " Average eval MAE loss: 38.16\n",
      "===============================\n",
      "MAE:  2.9816957\n",
      "MdAE:  2.614366\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.90\n",
      "-\n",
      " Average eval MAE loss: 38.75\n",
      "===============================\n",
      "MAE:  2.926465\n",
      "MdAE:  2.4452267\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.38\n",
      "-\n",
      " Average eval MAE loss: 39.50\n",
      "===============================\n",
      "MAE:  2.8179557\n",
      "MdAE:  2.379856\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.89\n",
      "-\n",
      " Average eval MAE loss: 39.47\n",
      "===============================\n",
      "MAE:  2.8775144\n",
      "MdAE:  2.4399347\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.94\n",
      "-\n",
      " Average eval MAE loss: 39.37\n",
      "===============================\n",
      "MAE:  2.8055108\n",
      "MdAE:  2.3859673\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.62\n",
      "-\n",
      " Average eval MAE loss: 40.76\n",
      "===============================\n",
      "MAE:  2.8230186\n",
      "MdAE:  2.260979\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.45\n",
      "-\n",
      " Average eval MAE loss: 39.54\n",
      "===============================\n",
      "MAE:  2.7823112\n",
      "MdAE:  2.37333\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.26\n",
      "-\n",
      " Average eval MAE loss: 39.66\n",
      "===============================\n",
      "MAE:  2.8355958\n",
      "MdAE:  2.3697352\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.22\n",
      "-\n",
      " Average eval MAE loss: 39.91\n",
      "===============================\n",
      "MAE:  2.7803056\n",
      "MdAE:  2.3470714\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.34\n",
      "-\n",
      " Average eval MAE loss: 40.79\n",
      "===============================\n",
      "MAE:  2.793285\n",
      "MdAE:  2.2670069\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.15\n",
      "-\n",
      " Average eval MAE loss: 39.32\n",
      "===============================\n",
      "MAE:  2.7904906\n",
      "MdAE:  2.4159794\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.18\n",
      "-\n",
      " Average eval MAE loss: 40.60\n",
      "===============================\n",
      "MAE:  2.8002365\n",
      "MdAE:  2.246325\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 39.62\n",
      "===============================\n",
      "MAE:  2.7887154\n",
      "MdAE:  2.3353057\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.08\n",
      "-\n",
      " Average eval MAE loss: 39.19\n",
      "===============================\n",
      "MAE:  2.807917\n",
      "MdAE:  2.3705492\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.04\n",
      "-\n",
      " Average eval MAE loss: 39.57\n",
      "===============================\n",
      "MAE:  2.8012173\n",
      "MdAE:  2.3324494\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    }
   ],
   "source": [
    "global WITHIN_PROJECT, BATCH_SIZE_RATIO\n",
    "WITHIN_PROJECT = False\n",
    "BATCH_SIZE_RATIO = 0.4\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
    "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
    "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
    "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME, HF_MODEL_NAME\n",
    "    \n",
    "    # Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
    "    # Quantization Config (for QLORA)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16, # Changed to float16 for lower memory usage\n",
    "    )\n",
    "    # Lora Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Reduced from 16 to 8 for lower memory usage\n",
    "        lora_alpha=16,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        optimize_memory()\n",
    "\n",
    "        # Config for Lama3 model\n",
    "        config = AutoConfig.from_pretrained(HF_MODEL_NAME, num_labels=1)\n",
    "        if MODEL_NAME == 'llama3':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'llama3sp':\n",
    "            MODEL = LLAMA3SP.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'deepseek':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'qwen':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        # prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "        MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "        # get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model\n",
    "        MODEL = get_peft_model(MODEL, lora_config)\n",
    "\n",
    "        # additional memory optimizations\n",
    "        MODEL.gradient_checkpointing_enable()  # Reduce memory usage during training\n",
    "        MODEL.enable_input_require_grads()\n",
    "\n",
    "        if TOKENIZER == 'wordlevel':\n",
    "            MODEL.config.pad_token_id = 3\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "\n",
    "        MODEL.cuda()\n",
    "\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        optimize_memory()\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Cross Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1190ad01af2e450db5975659f4eda2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['clover'], 'test': ['usergrid']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 46.19\n",
      "-\n",
      " Average eval MAE loss: 49.65\n",
      "===============================\n",
      "MAE:  3.4119787\n",
      "MdAE:  3.4438124\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 30.37\n",
      "-\n",
      " Average eval MAE loss: 35.78\n",
      "===============================\n",
      "MAE:  2.0512733\n",
      "MdAE:  1.8475847\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 20.71\n",
      "-\n",
      " Average eval MAE loss: 41.82\n",
      "===============================\n",
      "MAE:  1.9108717\n",
      "MdAE:  1.6488316\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 15.13\n",
      "-\n",
      " Average eval MAE loss: 43.41\n",
      "===============================\n",
      "MAE:  2.6068974\n",
      "MdAE:  2.309262\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 6.75\n",
      "-\n",
      " Average eval MAE loss: 42.18\n",
      "===============================\n",
      "MAE:  2.6494951\n",
      "MdAE:  2.2329254\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.21\n",
      "-\n",
      " Average eval MAE loss: 40.39\n",
      "===============================\n",
      "MAE:  2.019535\n",
      "MdAE:  1.7288733\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.01\n",
      "-\n",
      " Average eval MAE loss: 40.51\n",
      "===============================\n",
      "MAE:  2.0863597\n",
      "MdAE:  1.8308818\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.37\n",
      "-\n",
      " Average eval MAE loss: 40.65\n",
      "===============================\n",
      "MAE:  2.1377559\n",
      "MdAE:  1.9161088\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.26\n",
      "-\n",
      " Average eval MAE loss: 42.15\n",
      "===============================\n",
      "MAE:  2.350024\n",
      "MdAE:  2.0155504\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.52\n",
      "-\n",
      " Average eval MAE loss: 40.78\n",
      "===============================\n",
      "MAE:  1.8563856\n",
      "MdAE:  1.6208049\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.21\n",
      "-\n",
      " Average eval MAE loss: 43.97\n",
      "===============================\n",
      "MAE:  2.54871\n",
      "MdAE:  2.1992707\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.86\n",
      "-\n",
      " Average eval MAE loss: 40.96\n",
      "===============================\n",
      "MAE:  1.9032817\n",
      "MdAE:  1.6222723\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.53\n",
      "-\n",
      " Average eval MAE loss: 42.27\n",
      "===============================\n",
      "MAE:  2.3969483\n",
      "MdAE:  2.0635939\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.78\n",
      "-\n",
      " Average eval MAE loss: 41.09\n",
      "===============================\n",
      "MAE:  2.02695\n",
      "MdAE:  1.7538834\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.44\n",
      "-\n",
      " Average eval MAE loss: 42.01\n",
      "===============================\n",
      "MAE:  2.2481942\n",
      "MdAE:  1.9349794\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.19\n",
      "-\n",
      " Average eval MAE loss: 42.02\n",
      "===============================\n",
      "MAE:  2.2335145\n",
      "MdAE:  1.9377534\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.25\n",
      "-\n",
      " Average eval MAE loss: 41.38\n",
      "===============================\n",
      "MAE:  2.0271618\n",
      "MdAE:  1.750812\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.31\n",
      "-\n",
      " Average eval MAE loss: 42.12\n",
      "===============================\n",
      "MAE:  2.2612062\n",
      "MdAE:  1.9885937\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.21\n",
      "-\n",
      " Average eval MAE loss: 41.85\n",
      "===============================\n",
      "MAE:  2.2270498\n",
      "MdAE:  1.9646779\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.10\n",
      "-\n",
      " Average eval MAE loss: 41.47\n",
      "===============================\n",
      "MAE:  2.1321545\n",
      "MdAE:  1.8741394\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4813ce131442feb6639aa2e6ef6b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['talendesb'], 'test': ['mesos']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 3.92\n",
      "-\n",
      " Average eval MAE loss: 2.06\n",
      "===============================\n",
      "MAE:  1.635124\n",
      "MdAE:  1.0246979\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 1.53\n",
      "-\n",
      " Average eval MAE loss: 2.05\n",
      "===============================\n",
      "MAE:  1.5145098\n",
      "MdAE:  0.9832066\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 0.98\n",
      "-\n",
      " Average eval MAE loss: 2.72\n",
      "===============================\n",
      "MAE:  1.5977587\n",
      "MdAE:  1.0607721\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 0.53\n",
      "-\n",
      " Average eval MAE loss: 2.29\n",
      "===============================\n",
      "MAE:  1.77549\n",
      "MdAE:  1.2070217\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 0.42\n",
      "-\n",
      " Average eval MAE loss: 2.06\n",
      "===============================\n",
      "MAE:  1.6380045\n",
      "MdAE:  1.0139602\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 0.27\n",
      "-\n",
      " Average eval MAE loss: 2.05\n",
      "===============================\n",
      "MAE:  1.5984111\n",
      "MdAE:  0.99618363\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 0.18\n",
      "-\n",
      " Average eval MAE loss: 2.14\n",
      "===============================\n",
      "MAE:  1.6095941\n",
      "MdAE:  1.0466139\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 0.17\n",
      "-\n",
      " Average eval MAE loss: 1.98\n",
      "===============================\n",
      "MAE:  1.5698415\n",
      "MdAE:  0.98485875\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.14\n",
      "-\n",
      " Average eval MAE loss: 2.03\n",
      "===============================\n",
      "MAE:  1.6078286\n",
      "MdAE:  1.0235531\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 2.13\n",
      "===============================\n",
      "MAE:  1.6010667\n",
      "MdAE:  1.0683047\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 2.01\n",
      "===============================\n",
      "MAE:  1.5993992\n",
      "MdAE:  1.0043035\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 2.07\n",
      "===============================\n",
      "MAE:  1.5790062\n",
      "MdAE:  1.0222058\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 2.00\n",
      "===============================\n",
      "MAE:  1.6080375\n",
      "MdAE:  1.0217844\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 2.04\n",
      "===============================\n",
      "MAE:  1.5648949\n",
      "MdAE:  1.0062137\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.03\n",
      "===============================\n",
      "MAE:  1.5664492\n",
      "MdAE:  0.9963742\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 2.00\n",
      "===============================\n",
      "MAE:  1.5905187\n",
      "MdAE:  0.9894943\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.01\n",
      "-\n",
      " Average eval MAE loss: 2.05\n",
      "===============================\n",
      "MAE:  1.5608549\n",
      "MdAE:  0.99829715\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 2.01\n",
      "===============================\n",
      "MAE:  1.5718966\n",
      "MdAE:  0.9929687\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.01\n",
      "-\n",
      " Average eval MAE loss: 2.01\n",
      "===============================\n",
      "MAE:  1.5711062\n",
      "MdAE:  0.9869375\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.00\n",
      "-\n",
      " Average eval MAE loss: 2.00\n",
      "===============================\n",
      "MAE:  1.5709352\n",
      "MdAE:  0.9844969\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c931584835da496e8ca488b39c6f51be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['talenddataquality'], 'test': ['aptanastudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 36.61\n",
      "-\n",
      " Average eval MAE loss: 25.45\n",
      "===============================\n",
      "MAE:  4.0535917\n",
      "MdAE:  2.8079658\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 21.23\n",
      "-\n",
      " Average eval MAE loss: 28.13\n",
      "===============================\n",
      "MAE:  4.1533237\n",
      "MdAE:  3.1360626\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 14.01\n",
      "-\n",
      " Average eval MAE loss: 22.43\n",
      "===============================\n",
      "MAE:  4.207697\n",
      "MdAE:  3.0496073\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 8.09\n",
      "-\n",
      " Average eval MAE loss: 25.08\n",
      "===============================\n",
      "MAE:  4.316\n",
      "MdAE:  3.194418\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 4.35\n",
      "-\n",
      " Average eval MAE loss: 31.31\n",
      "===============================\n",
      "MAE:  4.19203\n",
      "MdAE:  3.3543873\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.12\n",
      "-\n",
      " Average eval MAE loss: 25.88\n",
      "===============================\n",
      "MAE:  4.2701716\n",
      "MdAE:  3.2054095\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.99\n",
      "-\n",
      " Average eval MAE loss: 24.47\n",
      "===============================\n",
      "MAE:  4.2278743\n",
      "MdAE:  3.1193695\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.32\n",
      "-\n",
      " Average eval MAE loss: 29.37\n",
      "===============================\n",
      "MAE:  4.2887897\n",
      "MdAE:  3.2156143\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.13\n",
      "-\n",
      " Average eval MAE loss: 28.66\n",
      "===============================\n",
      "MAE:  4.2150707\n",
      "MdAE:  3.2374\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.33\n",
      "-\n",
      " Average eval MAE loss: 26.24\n",
      "===============================\n",
      "MAE:  4.1929326\n",
      "MdAE:  3.1676884\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.18\n",
      "-\n",
      " Average eval MAE loss: 25.53\n",
      "===============================\n",
      "MAE:  4.2356462\n",
      "MdAE:  3.1913357\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 23.06\n",
      "===============================\n",
      "MAE:  4.3123283\n",
      "MdAE:  3.1702538\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.00\n",
      "-\n",
      " Average eval MAE loss: 26.80\n",
      "===============================\n",
      "MAE:  4.199213\n",
      "MdAE:  3.206067\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.46\n",
      "-\n",
      " Average eval MAE loss: 26.49\n",
      "===============================\n",
      "MAE:  4.214272\n",
      "MdAE:  3.1638818\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.36\n",
      "-\n",
      " Average eval MAE loss: 25.47\n",
      "===============================\n",
      "MAE:  4.220789\n",
      "MdAE:  3.1807718\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.43\n",
      "-\n",
      " Average eval MAE loss: 24.55\n",
      "===============================\n",
      "MAE:  4.228083\n",
      "MdAE:  3.1055546\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.27\n",
      "-\n",
      " Average eval MAE loss: 24.93\n",
      "===============================\n",
      "MAE:  4.213683\n",
      "MdAE:  3.1169853\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.22\n",
      "-\n",
      " Average eval MAE loss: 25.97\n",
      "===============================\n",
      "MAE:  4.2174144\n",
      "MdAE:  3.1344242\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.18\n",
      "-\n",
      " Average eval MAE loss: 25.50\n",
      "===============================\n",
      "MAE:  4.21696\n",
      "MdAE:  3.155136\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.15\n",
      "-\n",
      " Average eval MAE loss: 25.13\n",
      "===============================\n",
      "MAE:  4.2263975\n",
      "MdAE:  3.1594362\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6f23c7cdb34103b69df51c13fa5002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['mule'], 'test': ['titanium']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 20.44\n",
      "-\n",
      " Average eval MAE loss: 10.69\n",
      "===============================\n",
      "MAE:  3.398951\n",
      "MdAE:  2.2323399\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 10.05\n",
      "-\n",
      " Average eval MAE loss: 10.36\n",
      "===============================\n",
      "MAE:  3.6782262\n",
      "MdAE:  2.2390995\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 6.14\n",
      "-\n",
      " Average eval MAE loss: 11.21\n",
      "===============================\n",
      "MAE:  3.4111068\n",
      "MdAE:  2.339006\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 3.57\n",
      "-\n",
      " Average eval MAE loss: 10.38\n",
      "===============================\n",
      "MAE:  3.592048\n",
      "MdAE:  2.1889277\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 2.60\n",
      "-\n",
      " Average eval MAE loss: 10.70\n",
      "===============================\n",
      "MAE:  3.5026834\n",
      "MdAE:  2.3197064\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 1.75\n",
      "-\n",
      " Average eval MAE loss: 10.40\n",
      "===============================\n",
      "MAE:  3.4687011\n",
      "MdAE:  2.309823\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.35\n",
      "-\n",
      " Average eval MAE loss: 9.99\n",
      "===============================\n",
      "MAE:  3.506853\n",
      "MdAE:  2.2047472\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.51\n",
      "-\n",
      " Average eval MAE loss: 10.81\n",
      "===============================\n",
      "MAE:  3.501364\n",
      "MdAE:  2.2948413\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.72\n",
      "-\n",
      " Average eval MAE loss: 10.40\n",
      "===============================\n",
      "MAE:  3.4244027\n",
      "MdAE:  2.2497168\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 0.67\n",
      "-\n",
      " Average eval MAE loss: 10.38\n",
      "===============================\n",
      "MAE:  3.4073658\n",
      "MdAE:  2.251655\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.70\n",
      "-\n",
      " Average eval MAE loss: 10.80\n",
      "===============================\n",
      "MAE:  3.4605703\n",
      "MdAE:  2.3107777\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.29\n",
      "-\n",
      " Average eval MAE loss: 10.74\n",
      "===============================\n",
      "MAE:  3.414706\n",
      "MdAE:  2.301422\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.30\n",
      "-\n",
      " Average eval MAE loss: 10.57\n",
      "===============================\n",
      "MAE:  3.51606\n",
      "MdAE:  2.2462966\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.28\n",
      "-\n",
      " Average eval MAE loss: 10.50\n",
      "===============================\n",
      "MAE:  3.4913871\n",
      "MdAE:  2.2645016\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.10\n",
      "-\n",
      " Average eval MAE loss: 10.64\n",
      "===============================\n",
      "MAE:  3.463665\n",
      "MdAE:  2.3000183\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 10.37\n",
      "===============================\n",
      "MAE:  3.4595466\n",
      "MdAE:  2.2546372\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.08\n",
      "-\n",
      " Average eval MAE loss: 10.60\n",
      "===============================\n",
      "MAE:  3.44472\n",
      "MdAE:  2.268168\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 10.48\n",
      "===============================\n",
      "MAE:  3.4357152\n",
      "MdAE:  2.246273\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 10.49\n",
      "===============================\n",
      "MAE:  3.442855\n",
      "MdAE:  2.2515695\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.02\n",
      "-\n",
      " Average eval MAE loss: 10.49\n",
      "===============================\n",
      "MAE:  3.4479346\n",
      "MdAE:  2.252759\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aacd32ea181433aafafa00ab57a860d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['talenddataquality'], 'test': ['appceleratorstudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 35.53\n",
      "-\n",
      " Average eval MAE loss: 26.57\n",
      "===============================\n",
      "MAE:  2.6246564\n",
      "MdAE:  1.9418707\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 22.48\n",
      "-\n",
      " Average eval MAE loss: 21.21\n",
      "===============================\n",
      "MAE:  2.4628508\n",
      "MdAE:  1.9015484\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 17.52\n",
      "-\n",
      " Average eval MAE loss: 23.04\n",
      "===============================\n",
      "MAE:  2.661877\n",
      "MdAE:  2.0798879\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 9.40\n",
      "-\n",
      " Average eval MAE loss: 29.93\n",
      "===============================\n",
      "MAE:  3.33324\n",
      "MdAE:  2.804533\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 5.16\n",
      "-\n",
      " Average eval MAE loss: 24.84\n",
      "===============================\n",
      "MAE:  2.7217054\n",
      "MdAE:  2.162005\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 3.79\n",
      "-\n",
      " Average eval MAE loss: 24.67\n",
      "===============================\n",
      "MAE:  2.9540203\n",
      "MdAE:  2.415741\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 3.49\n",
      "-\n",
      " Average eval MAE loss: 23.62\n",
      "===============================\n",
      "MAE:  2.6543376\n",
      "MdAE:  2.0664978\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 2.33\n",
      "-\n",
      " Average eval MAE loss: 29.60\n",
      "===============================\n",
      "MAE:  3.1772509\n",
      "MdAE:  2.6823606\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 2.35\n",
      "-\n",
      " Average eval MAE loss: 29.98\n",
      "===============================\n",
      "MAE:  3.2291095\n",
      "MdAE:  2.6980705\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.61\n",
      "-\n",
      " Average eval MAE loss: 26.37\n",
      "===============================\n",
      "MAE:  2.8107624\n",
      "MdAE:  2.2571602\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 1.34\n",
      "-\n",
      " Average eval MAE loss: 28.23\n",
      "===============================\n",
      "MAE:  3.0681982\n",
      "MdAE:  2.5122538\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 1.10\n",
      "-\n",
      " Average eval MAE loss: 26.48\n",
      "===============================\n",
      "MAE:  2.8328938\n",
      "MdAE:  2.3256578\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 1.05\n",
      "-\n",
      " Average eval MAE loss: 28.02\n",
      "===============================\n",
      "MAE:  3.0509884\n",
      "MdAE:  2.5295372\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.53\n",
      "-\n",
      " Average eval MAE loss: 28.64\n",
      "===============================\n",
      "MAE:  3.1285348\n",
      "MdAE:  2.643067\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.61\n",
      "-\n",
      " Average eval MAE loss: 25.40\n",
      "===============================\n",
      "MAE:  2.7454336\n",
      "MdAE:  2.2253752\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.34\n",
      "-\n",
      " Average eval MAE loss: 27.60\n",
      "===============================\n",
      "MAE:  3.0016654\n",
      "MdAE:  2.4875112\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.26\n",
      "-\n",
      " Average eval MAE loss: 27.66\n",
      "===============================\n",
      "MAE:  2.9892614\n",
      "MdAE:  2.4693108\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 27.44\n",
      "===============================\n",
      "MAE:  2.9527724\n",
      "MdAE:  2.4468083\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.16\n",
      "-\n",
      " Average eval MAE loss: 27.38\n",
      "===============================\n",
      "MAE:  2.9677355\n",
      "MdAE:  2.460157\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 27.73\n",
      "===============================\n",
      "MAE:  2.9950922\n",
      "MdAE:  2.4742408\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2946b83bac1b43eab17aba21adea62d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['mulestudio'], 'test': ['titanium']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 27.08\n",
      "-\n",
      " Average eval MAE loss: 39.10\n",
      "===============================\n",
      "MAE:  3.46333\n",
      "MdAE:  2.3932767\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 15.89\n",
      "-\n",
      " Average eval MAE loss: 43.31\n",
      "===============================\n",
      "MAE:  3.4286234\n",
      "MdAE:  2.2223644\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 7.63\n",
      "-\n",
      " Average eval MAE loss: 40.02\n",
      "===============================\n",
      "MAE:  3.5787332\n",
      "MdAE:  2.532837\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 4.49\n",
      "-\n",
      " Average eval MAE loss: 41.13\n",
      "===============================\n",
      "MAE:  3.575898\n",
      "MdAE:  2.555266\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.03\n",
      "-\n",
      " Average eval MAE loss: 38.67\n",
      "===============================\n",
      "MAE:  3.834474\n",
      "MdAE:  2.880488\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.96\n",
      "-\n",
      " Average eval MAE loss: 38.55\n",
      "===============================\n",
      "MAE:  3.7675242\n",
      "MdAE:  2.7828727\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 1.57\n",
      "-\n",
      " Average eval MAE loss: 39.09\n",
      "===============================\n",
      "MAE:  3.643671\n",
      "MdAE:  2.5980368\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.31\n",
      "-\n",
      " Average eval MAE loss: 42.38\n",
      "===============================\n",
      "MAE:  3.5927074\n",
      "MdAE:  2.366293\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 0.96\n",
      "-\n",
      " Average eval MAE loss: 42.86\n",
      "===============================\n",
      "MAE:  3.5977473\n",
      "MdAE:  2.3419619\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.03\n",
      "-\n",
      " Average eval MAE loss: 42.22\n",
      "===============================\n",
      "MAE:  3.5544124\n",
      "MdAE:  2.3408804\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 2.06\n",
      "-\n",
      " Average eval MAE loss: 40.56\n",
      "===============================\n",
      "MAE:  3.7041593\n",
      "MdAE:  2.5430617\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.97\n",
      "-\n",
      " Average eval MAE loss: 39.95\n",
      "===============================\n",
      "MAE:  3.6428583\n",
      "MdAE:  2.4983506\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.29\n",
      "-\n",
      " Average eval MAE loss: 40.76\n",
      "===============================\n",
      "MAE:  3.6070976\n",
      "MdAE:  2.4322262\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.20\n",
      "-\n",
      " Average eval MAE loss: 41.36\n",
      "===============================\n",
      "MAE:  3.6346743\n",
      "MdAE:  2.417984\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.28\n",
      "-\n",
      " Average eval MAE loss: 39.93\n",
      "===============================\n",
      "MAE:  3.5827239\n",
      "MdAE:  2.4349675\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.13\n",
      "-\n",
      " Average eval MAE loss: 41.70\n",
      "===============================\n",
      "MAE:  3.611995\n",
      "MdAE:  2.4014995\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.12\n",
      "-\n",
      " Average eval MAE loss: 41.23\n",
      "===============================\n",
      "MAE:  3.5682595\n",
      "MdAE:  2.3870583\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 41.49\n",
      "===============================\n",
      "MAE:  3.5802834\n",
      "MdAE:  2.3733525\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.06\n",
      "-\n",
      " Average eval MAE loss: 40.71\n",
      "===============================\n",
      "MAE:  3.5956059\n",
      "MdAE:  2.4347563\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.03\n",
      "-\n",
      " Average eval MAE loss: 40.74\n",
      "===============================\n",
      "MAE:  3.5998304\n",
      "MdAE:  2.4291544\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70b0b34e7734d878a802b0252f1b61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['mulestudio']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 19.97\n",
      "-\n",
      " Average eval MAE loss: 4.98\n",
      "===============================\n",
      "MAE:  3.6080663\n",
      "MdAE:  2.4341588\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 12.74\n",
      "-\n",
      " Average eval MAE loss: 7.36\n",
      "===============================\n",
      "MAE:  3.8913705\n",
      "MdAE:  3.126961\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 8.31\n",
      "-\n",
      " Average eval MAE loss: 6.09\n",
      "===============================\n",
      "MAE:  3.6601543\n",
      "MdAE:  2.7717974\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 4.79\n",
      "-\n",
      " Average eval MAE loss: 6.20\n",
      "===============================\n",
      "MAE:  3.68953\n",
      "MdAE:  2.548896\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.85\n",
      "-\n",
      " Average eval MAE loss: 10.05\n",
      "===============================\n",
      "MAE:  4.1147146\n",
      "MdAE:  3.2314801\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.64\n",
      "-\n",
      " Average eval MAE loss: 5.47\n",
      "===============================\n",
      "MAE:  3.5572743\n",
      "MdAE:  2.545262\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.10\n",
      "-\n",
      " Average eval MAE loss: 6.12\n",
      "===============================\n",
      "MAE:  3.6967616\n",
      "MdAE:  2.6115093\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.61\n",
      "-\n",
      " Average eval MAE loss: 6.83\n",
      "===============================\n",
      "MAE:  3.8136938\n",
      "MdAE:  2.9300525\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.30\n",
      "-\n",
      " Average eval MAE loss: 6.32\n",
      "===============================\n",
      "MAE:  3.8152342\n",
      "MdAE:  2.8239617\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.29\n",
      "-\n",
      " Average eval MAE loss: 6.10\n",
      "===============================\n",
      "MAE:  3.7045112\n",
      "MdAE:  2.7190588\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.78\n",
      "-\n",
      " Average eval MAE loss: 5.90\n",
      "===============================\n",
      "MAE:  3.7215736\n",
      "MdAE:  2.686063\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.69\n",
      "-\n",
      " Average eval MAE loss: 7.37\n",
      "===============================\n",
      "MAE:  3.8714783\n",
      "MdAE:  3.0019279\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.46\n",
      "-\n",
      " Average eval MAE loss: 6.01\n",
      "===============================\n",
      "MAE:  3.7185934\n",
      "MdAE:  2.6584718\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.26\n",
      "-\n",
      " Average eval MAE loss: 6.31\n",
      "===============================\n",
      "MAE:  3.7642136\n",
      "MdAE:  2.8162508\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.39\n",
      "-\n",
      " Average eval MAE loss: 6.18\n",
      "===============================\n",
      "MAE:  3.7436123\n",
      "MdAE:  2.784341\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.25\n",
      "-\n",
      " Average eval MAE loss: 6.81\n",
      "===============================\n",
      "MAE:  3.7871428\n",
      "MdAE:  2.9096482\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.17\n",
      "-\n",
      " Average eval MAE loss: 6.26\n",
      "===============================\n",
      "MAE:  3.7509885\n",
      "MdAE:  2.8008623\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.10\n",
      "-\n",
      " Average eval MAE loss: 6.42\n",
      "===============================\n",
      "MAE:  3.779938\n",
      "MdAE:  2.8823168\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.07\n",
      "-\n",
      " Average eval MAE loss: 6.14\n",
      "===============================\n",
      "MAE:  3.7424135\n",
      "MdAE:  2.7663388\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 6.28\n",
      "===============================\n",
      "MAE:  3.7561843\n",
      "MdAE:  2.8090603\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede3fb37b812413ab34988b64902cddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen3-1.7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross project split!\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "using pretrained Qwen tokenizer\n",
      "cross project data processing!\n",
      "Start training for  {'train': ['appceleratorstudio'], 'test': ['mule']} .....\n",
      ">>> epoch  0\n",
      " Average training MAE loss: 16.83\n",
      "-\n",
      " Average eval MAE loss: 9.04\n",
      "===============================\n",
      "MAE:  3.5204232\n",
      "MdAE:  3.4285717\n",
      ">>> epoch  1\n",
      " Average training MAE loss: 12.45\n",
      "-\n",
      " Average eval MAE loss: 5.28\n",
      "===============================\n",
      "MAE:  2.7992587\n",
      "MdAE:  2.4242382\n",
      ">>> epoch  2\n",
      " Average training MAE loss: 8.97\n",
      "-\n",
      " Average eval MAE loss: 8.57\n",
      "===============================\n",
      "MAE:  3.15766\n",
      "MdAE:  2.8181162\n",
      ">>> epoch  3\n",
      " Average training MAE loss: 5.34\n",
      "-\n",
      " Average eval MAE loss: 8.50\n",
      "===============================\n",
      "MAE:  3.4273376\n",
      "MdAE:  2.9539943\n",
      ">>> epoch  4\n",
      " Average training MAE loss: 3.95\n",
      "-\n",
      " Average eval MAE loss: 7.39\n",
      "===============================\n",
      "MAE:  2.9087496\n",
      "MdAE:  2.4951525\n",
      ">>> epoch  5\n",
      " Average training MAE loss: 2.43\n",
      "-\n",
      " Average eval MAE loss: 6.92\n",
      "===============================\n",
      "MAE:  2.928387\n",
      "MdAE:  2.438219\n",
      ">>> epoch  6\n",
      " Average training MAE loss: 2.11\n",
      "-\n",
      " Average eval MAE loss: 5.71\n",
      "===============================\n",
      "MAE:  2.817133\n",
      "MdAE:  2.444713\n",
      ">>> epoch  7\n",
      " Average training MAE loss: 1.81\n",
      "-\n",
      " Average eval MAE loss: 6.70\n",
      "===============================\n",
      "MAE:  2.9409783\n",
      "MdAE:  2.5398302\n",
      ">>> epoch  8\n",
      " Average training MAE loss: 1.43\n",
      "-\n",
      " Average eval MAE loss: 6.70\n",
      "===============================\n",
      "MAE:  2.953978\n",
      "MdAE:  2.504305\n",
      ">>> epoch  9\n",
      " Average training MAE loss: 1.08\n",
      "-\n",
      " Average eval MAE loss: 6.78\n",
      "===============================\n",
      "MAE:  2.9627988\n",
      "MdAE:  2.5758154\n",
      ">>> epoch  10\n",
      " Average training MAE loss: 0.83\n",
      "-\n",
      " Average eval MAE loss: 5.93\n",
      "===============================\n",
      "MAE:  2.8476546\n",
      "MdAE:  2.5000112\n",
      ">>> epoch  11\n",
      " Average training MAE loss: 0.85\n",
      "-\n",
      " Average eval MAE loss: 7.19\n",
      "===============================\n",
      "MAE:  3.0298824\n",
      "MdAE:  2.6292448\n",
      ">>> epoch  12\n",
      " Average training MAE loss: 0.90\n",
      "-\n",
      " Average eval MAE loss: 5.54\n",
      "===============================\n",
      "MAE:  2.7893682\n",
      "MdAE:  2.391131\n",
      ">>> epoch  13\n",
      " Average training MAE loss: 0.41\n",
      "-\n",
      " Average eval MAE loss: 5.91\n",
      "===============================\n",
      "MAE:  2.8575351\n",
      "MdAE:  2.4576364\n",
      ">>> epoch  14\n",
      " Average training MAE loss: 0.33\n",
      "-\n",
      " Average eval MAE loss: 6.86\n",
      "===============================\n",
      "MAE:  2.9628413\n",
      "MdAE:  2.5803828\n",
      ">>> epoch  15\n",
      " Average training MAE loss: 0.17\n",
      "-\n",
      " Average eval MAE loss: 5.72\n",
      "===============================\n",
      "MAE:  2.842634\n",
      "MdAE:  2.488688\n",
      ">>> epoch  16\n",
      " Average training MAE loss: 0.17\n",
      "-\n",
      " Average eval MAE loss: 6.11\n",
      "===============================\n",
      "MAE:  2.8760238\n",
      "MdAE:  2.483964\n",
      ">>> epoch  17\n",
      " Average training MAE loss: 0.11\n",
      "-\n",
      " Average eval MAE loss: 6.07\n",
      "===============================\n",
      "MAE:  2.8850853\n",
      "MdAE:  2.5149417\n",
      ">>> epoch  18\n",
      " Average training MAE loss: 0.09\n",
      "-\n",
      " Average eval MAE loss: 6.23\n",
      "===============================\n",
      "MAE:  2.8915303\n",
      "MdAE:  2.500928\n",
      ">>> epoch  19\n",
      " Average training MAE loss: 0.05\n",
      "-\n",
      " Average eval MAE loss: 6.26\n",
      "===============================\n",
      "MAE:  2.8976197\n",
      "MdAE:  2.5251322\n",
      "all done for one project\n",
      "results have been written into a text file!\n"
     ]
    }
   ],
   "source": [
    "global WITHIN_PROJECT, BATCH_SIZE_RATIO\n",
    "WITHIN_PROJECT = False\n",
    "BATCH_SIZE_RATIO = 0.4\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['clover'], 'test': ['usergrid']},\n",
    "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['mule'], 'test': ['titanium']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME, HF_MODEL_NAME\n",
    "    \n",
    "    # Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
    "    # Quantization Config (for QLORA)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # Cambiado a float16 para menor uso de memoria\n",
    "    )\n",
    "    # Lora Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Reduced from 16 to 8 for lower memory usage\n",
    "        lora_alpha=16,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        optimize_memory()\n",
    "\n",
    "        # Config for Lama3 model\n",
    "        config = AutoConfig.from_pretrained(HF_MODEL_NAME, num_labels=1)\n",
    "        if MODEL_NAME == 'llama3':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'llama3sp':\n",
    "            MODEL = LLAMA3SP.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'deepseek':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        elif MODEL_NAME == 'qwen':\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "        # prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "        MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "        # get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model\n",
    "        MODEL = get_peft_model(MODEL, lora_config)\n",
    "\n",
    "        # additional memory optimizations\n",
    "        MODEL.gradient_checkpointing_enable()  # Reduce memory usage during training\n",
    "        MODEL.enable_input_require_grads()\n",
    "\n",
    "        if TOKENIZER == 'wordlevel':\n",
    "            MODEL.config.pad_token_id = 3\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "\n",
    "        MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL \n",
    "        optimize_memory()\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
