{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from Llama3SP import LlamaForSequenceClassification as LLAMA3SP\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    XLNetTokenizer,\n",
    "    BertTokenizer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to huggingface hub to put your Llama token so we can access Llama 3.2 1B Param Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global EPOCHS, BATCH_SIZE_RATIO, SEQUENCE_LEN, LEARNING_RATE, TOKENIZER, MODEL_NAME, DEVICE\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE_RATIO = 0.3 # within proj: 0.3 / cross proj: 0.4\n",
    "SEQUENCE_LEN = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "TOKENIZER = 'wordpiece' # available: llama3, wordlevel, sentencepiece, wordpiece, gpt\n",
    "MODEL_NAME = 'llama3' # available: llama3\n",
    "HF_MODEL_NAME = 'meta-llama/Llama-3.2-1B'\n",
    "\n",
    "# define device\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    # set up to release cache memory when possible\n",
    "    torch.cuda.empty_cache()\n",
    "    # set up more conservative memory limits  \n",
    "    torch.cuda.set_per_process_memory_fraction(0.8)  # Use only 80% of GPU memory\n",
    "\n",
    "# define files to be used\n",
    "global DATA_PATH \n",
    "DATA_PATH = './sp_dataset/marked_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dynamic memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static Methods and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT = ''\n",
    "MODEL = None\n",
    "DYNAMIC_BATCH = True\n",
    "BATCH_SIZE = None\n",
    "WITHIN_PROJECT = None\n",
    "MAE_RECORDS = []\n",
    "MDAE_RECORDS = []\n",
    "\n",
    "\n",
    "def optimize_memory():\n",
    "    \"\"\"Aux function to optimize memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def data_processing(file_pair):\n",
    "    global BATCH_SIZE, BATCH_SIZE_RATIO, DATA_PATH, WITHIN_PROJECT, DYNAMIC_BATCH, MODEL_NAME\n",
    "\n",
    "    optimize_memory()\n",
    "\n",
    "    train_data = pd.DataFrame(columns=['text', 'label'])\n",
    "    for train_file_name in file_pair['train']:\n",
    "        fname = DATA_PATH + train_file_name + '.csv'\n",
    "        df = prepare_dataframe(fname)\n",
    "        train_data = train_data.append(df)\n",
    "        \n",
    "    # data split\n",
    "    if WITHIN_PROJECT:\n",
    "        train_text, train_labels, val_text, val_labels, test_text, test_labels = within_project_split(train_data)\n",
    "    else:\n",
    "        train_text, train_labels, val_text, val_labels = train_val_split(train_data, 0.6)\n",
    "    # define batch size dynamically based on training length\n",
    "    if DYNAMIC_BATCH:\n",
    "        # BATCH_SIZE = int(len(train_text) * BATCH_SIZE_RATIO)\n",
    "        BATCH_SIZE = min(int(len(train_text) * BATCH_SIZE_RATIO), 32)\n",
    "\n",
    "    optimize_memory()\n",
    "\n",
    "    # process data in chunks for tokenization\n",
    "    def process_in_chunks(texts, chunk_size=1000):\n",
    "        all_tokens = {'input_ids': []}\n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i + chunk_size].tolist()\n",
    "            tokens = tokenization(chunk)\n",
    "            all_tokens['input_ids'].extend(tokens['input_ids'])\n",
    "            optimize_memory()\n",
    "        return all_tokens\n",
    "    \n",
    "    # tokenization\n",
    "    tokens_train = process_in_chunks(train_text)\n",
    "    tokens_val = process_in_chunks(val_text)\n",
    " \n",
    "    train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "    train_y = torch.tensor(train_labels.tolist()).type(torch.LongTensor)\n",
    "    train_dataloader = prepare_dataloader(train_seq, train_y, sampler_type='random')\n",
    "\n",
    "    val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "    val_y = torch.tensor(val_labels.tolist()).type(torch.LongTensor)\n",
    "    val_dataloader = prepare_dataloader(val_seq, val_y, sampler_type='sequential')\n",
    "    \n",
    "    # prepare testing datasets\n",
    "    all_test_dataloader = []\n",
    "    test_file_names = []\n",
    "\n",
    "    if WITHIN_PROJECT:\n",
    "        tokens_test = process_in_chunks(test_text)\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(file_pair['test'][0])\n",
    "        return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "    for test_file_name in file_pair['test']:\n",
    "        fname = DATA_PATH + test_file_name + '.csv'\n",
    "        test_data = prepare_dataframe(fname)\n",
    "\n",
    "        test_text = test_data['text']\n",
    "        test_labels = test_data['label']\n",
    "\n",
    "        # tokenization\n",
    "        tokens_test = process_in_chunks(test_text)\n",
    "        test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "        test_y = torch.tensor(test_labels.tolist()).type(torch.LongTensor)\n",
    "        test_dataloader = prepare_dataloader(test_seq, test_y, sampler_type='sequential')\n",
    "\n",
    "        all_test_dataloader.append(test_dataloader)\n",
    "        test_file_names.append(test_file_name)\n",
    "\n",
    "        optimize_memory()\n",
    "    print('cross project data processing!')\n",
    "    return file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names\n",
    "\n",
    "\n",
    "def train_val_split(data, split_ratio):\n",
    "    print('cross project split!')\n",
    "    split_point = int(len(data) * split_ratio)\n",
    "    train_text = data['text'][:split_point]\n",
    "    train_labels = data['label'][:split_point]\n",
    "    val_text = data['text'][split_point:]\n",
    "    val_labels = data['label'][split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels\n",
    "\n",
    "\n",
    "def tokenization(text_list):\n",
    "    global TOKENIZER, SEQUENCE_LEN, MODEL\n",
    "\n",
    "    if TOKENIZER == 'wordpiece':\n",
    "        print('using wordpiece tokenizer!')\n",
    "        tokenizer = BertTokenizer('all_tokenizers/sp_word_piece/vocab.txt')\n",
    "    elif TOKENIZER == 'sentencepiece':\n",
    "        print('using sentencepiece tokenizer!')\n",
    "        tokenizer = XLNetTokenizer('all_tokenizers/sp_sentence_piece/spm_tokenizer.model', padding_side='right')\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    elif TOKENIZER == 'wordlevel':\n",
    "        print('using wordlevel tokenizer!')\n",
    "        tokenizer = Tokenizer.from_file('all_tokenizers/sp_word_level/wordlevel.json')\n",
    "        encoded_sentences = {'input_ids':[]}\n",
    "        for sentence in text_list:\n",
    "            encoded = tokenizer.encode(sentence)\n",
    "            encoded = encoded.ids\n",
    "            if len(encoded) > SEQUENCE_LEN:\n",
    "                encoded = encoded[:SEQUENCE_LEN]\n",
    "            elif len(encoded) < SEQUENCE_LEN:\n",
    "                padding = SEQUENCE_LEN - len(encoded)\n",
    "                for _ in range(padding):\n",
    "                    encoded.append(3)\n",
    "            encoded_sentences['input_ids'].append(encoded)\n",
    "        return encoded_sentences\n",
    "    elif TOKENIZER == 'llama3':\n",
    "        print('using pretrained llama3 tokenizer')\n",
    "        tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME, add_prefix_space=True)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # update some model configs\n",
    "    # must use .cache = False as below or it crashes from my experience\n",
    "    MODEL.config.pad_token_id = tokenizer.pad_token_id\n",
    "    MODEL.config.use_cache = False\n",
    "    MODEL.config.pretraining_tp = 1\n",
    "    return tokenizer.batch_encode_plus(text_list, truncation=True, max_length=SEQUENCE_LEN, padding='max_length')\n",
    "\n",
    "\n",
    "def prepare_dataframe(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    # some rows have no description, fill blank to avoid Null\n",
    "    data = data.fillna(' ')\n",
    "    d = {'text': (data['title']).tolist(), 'label': data['storypoint']}\n",
    "    return pd.DataFrame(data=d)\n",
    "\n",
    "\n",
    "def prepare_dataloader(seq, y, sampler_type):\n",
    "    global BATCH_SIZE\n",
    "    tensor_dataset = TensorDataset(seq, y)\n",
    "    if sampler_type == 'random':\n",
    "        sampler = RandomSampler(tensor_dataset)\n",
    "    elif sampler_type == 'sequential':\n",
    "        sampler = SequentialSampler(tensor_dataset)\n",
    "    dataloader = DataLoader(tensor_dataset, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def within_project_split(data):\n",
    "    print('within project split!')\n",
    "    train_val_split_point = int(len(data) * 0.6)\n",
    "    val_test_split_point = int(len(data) * 0.8)\n",
    "    train_text = data['text'][:train_val_split_point]\n",
    "    train_labels = data['label'][:train_val_split_point]\n",
    "    val_text = data['text'][train_val_split_point:val_test_split_point]\n",
    "    val_labels = data['label'][train_val_split_point:val_test_split_point]\n",
    "    test_text = data['text'][val_test_split_point:]\n",
    "    test_labels = data['label'][val_test_split_point:]\n",
    "    return train_text, train_labels, val_text, val_labels, test_text, test_labels   \n",
    "\n",
    "\n",
    "def train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, model, test_file_names):\n",
    "    global LEARNING_RATE, EPOCHS, MAE_RECORDS, MDAE_RECORDS, DEVICE\n",
    "    optimizer = torch.optim.AdamW(MODEL.parameters(), lr=LEARNING_RATE)    \n",
    "    # total number of training steps is [number of batches] x [number of epochs]\n",
    "    total_steps = len(train_dataloader) * EPOCHS\n",
    "    # create the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "    print(\"Start training for \", file_pair, \".....\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # tensorboard writer\n",
    "    writer_path = 'tb/' + str(file_pair['train'][0]) + '_' + str(file_pair['test'][0])\n",
    "    writer = SummaryWriter(writer_path)\n",
    "    \n",
    "    # vars for model selection\n",
    "    min_eval_loss_epoch = [10000, 0]\n",
    "    \n",
    "    time_records = []\n",
    "    MAE_RECORDS = []\n",
    "    MDAE_RECORDS = []\n",
    "    \n",
    "    loss_fct = nn.L1Loss()\n",
    "    for e in range(EPOCHS):\n",
    "        # ---TRAINING---\n",
    "        # clean GPU memory\n",
    "        optimize_memory()\n",
    "        print(\">>> epoch \", e)\n",
    "        # set model into train mode\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):            \n",
    "            b_input_ids = batch[0].to(torch.long).to(DEVICE)\n",
    "            b_labels = batch[1].to(torch.float).to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward() \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # clean memory\n",
    "            del step, batch, b_input_ids, b_labels, result, loss, logits\n",
    "            optimize_memory()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(\" Average training MAE loss: {0:.2f}\".format(avg_train_loss))\n",
    "        writer.add_scalar('loss/train', avg_train_loss, e)\n",
    "        # clean memory\n",
    "        del avg_train_loss, total_train_loss\n",
    "        optimize_memory()\n",
    "        \n",
    "        time_records.append(time.time() - start_time)\n",
    "        \n",
    "        # ---EVAL---\n",
    "        print(\"-\")\n",
    "        # set model into eval mode\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for batch in val_dataloader:            \n",
    "            b_input_ids = batch[0].to(torch.long).to(DEVICE)\n",
    "            b_labels = batch[1].to(torch.float).to(DEVICE)\n",
    "            model.zero_grad()\n",
    "            result = model(b_input_ids, \n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            total_eval_loss += loss.item()  \n",
    "            # clean memory\n",
    "            del b_input_ids, b_labels, batch, result, loss, logits\n",
    "            optimize_memory()\n",
    "        avg_eval_loss = total_eval_loss / len(val_dataloader)\n",
    "        print(\" Average eval MAE loss: {0:.2f}\".format(avg_eval_loss))\n",
    "        \n",
    "        if avg_eval_loss <= min_eval_loss_epoch[0]:\n",
    "            min_eval_loss_epoch[0] = avg_eval_loss\n",
    "            min_eval_loss_epoch[1] = e\n",
    "\n",
    "        optimize_memory()\n",
    "        \n",
    "        writer.add_scalar('loss/eval', avg_eval_loss, e)\n",
    "        # clean memory\n",
    "        del avg_eval_loss, total_eval_loss\n",
    "        optimize_memory()\n",
    "        # save model state to dict\n",
    "        torch.save(model.state_dict(), './models/' + 'epo_' + str(e))\n",
    "        \n",
    "        print(\"===============================\")\n",
    "        \n",
    "        # testing on holdout data\n",
    "        index = 0\n",
    "        for test_dataloader in all_test_dataloader:\n",
    "            test_file_name = test_file_names[index]\n",
    "            index += 1\n",
    "            testing_start_time = time.time()\n",
    "            predictions = []\n",
    "            true_labels = []\n",
    "            for batch in test_dataloader:\n",
    "                batch = tuple(t.to(DEVICE) for t in batch)\n",
    "                b_input_ids, b_labels = batch\n",
    "                with torch.no_grad():\n",
    "                    logits = model(b_input_ids)\n",
    "                logits = logits['logits'].detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                predictions.append(logits)\n",
    "                true_labels.append(label_ids)\n",
    "            # calculate errors\n",
    "            distance_records = []\n",
    "            for i in range(len(predictions)):\n",
    "                for j in range(len(predictions[i])):\n",
    "                    distance = abs(predictions[i][j] - true_labels[i][j])\n",
    "                    distance_records.append(distance)\n",
    "\n",
    "            ## MAE = mean value of all absolute errors (stored in distance_records)\n",
    "            MAE = np.mean(np.array(distance_records)) \n",
    "            ## MdAE = median value of all absolute errors (stored in distance_records)\n",
    "            MdAE = np.median(np.array(distance_records)) \n",
    "\n",
    "            MAE_RECORDS.append(MAE)\n",
    "            MDAE_RECORDS.append(MdAE)\n",
    "            \n",
    "            global OUTPUT\n",
    "            OUTPUT +=  'Epochs ' + str(e) + '\\n'\n",
    "            OUTPUT += 'MAE: ' + str(MAE) + '\\n'\n",
    "            OUTPUT += 'MdAE: ' + str(MdAE) + '\\n\\n'\n",
    "            print('MAE: ', MAE)\n",
    "            print('MdAE: ', MdAE)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "    # select model\n",
    "    os.rename('models/epo_' + str(min_eval_loss_epoch[1]), \n",
    "              'models/' + str(file_pair['train'][0]) + '_' \n",
    "              + str(file_pair['test'][0]) + '_epo_' + str(min_eval_loss_epoch[1]))\n",
    "    \n",
    "    # del unwanted models\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            os.remove(\"models/epo_\" + str(i))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    OUTPUT += 'MAE: ' + str(MAE_RECORDS[min_eval_loss_epoch[1]]) \\\n",
    "                + '  MdAE: ' + str(MDAE_RECORDS[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'training time: ' + str(time_records[min_eval_loss_epoch[1]]) + '\\n'\n",
    "    OUTPUT += 'Epochs: ' + str(min_eval_loss_epoch[1]) +'\\n'\n",
    "    global BATCH_SIZE\n",
    "    OUTPUT += 'batch size: ' + str(BATCH_SIZE)\n",
    "    print('all done for one project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT, BATCH_SIZE_RATIO\n",
    "WITHIN_PROJECT = True\n",
    "BATCH_SIZE_RATIO = 0.3\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['bamboo'], 'test': ['bamboo']},\n",
    "                        {'train': ['clover'], 'test': ['clover']},\n",
    "                        {'train': ['datamanagement'], 'test': ['datamanagement']},\n",
    "                        {'train': ['duracloud'], 'test': ['duracloud']},\n",
    "                        {'train': ['jirasoftware'], 'test': ['jirasoftware']},\n",
    "                        {'train': ['mesos'], 'test': ['mesos']},\n",
    "                        {'train': ['moodle'], 'test': ['moodle']},\n",
    "                        {'train': ['mule'], 'test': ['mule']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['springxd'], 'test': ['springxd']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['talenddataquality']},\n",
    "                        {'train': ['talendesb'], 'test': ['talendesb']},\n",
    "                        {'train': ['titanium'], 'test': ['titanium']},\n",
    "                        {'train': ['usergrid'], 'test': ['usergrid']},\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME, HF_MODEL_NAME\n",
    "\n",
    "    # Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
    "    # Quantization Config (for QLORA)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16, # Changed to float16 for lower memory usage\n",
    "    )\n",
    "    # Lora Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Reduced from 16 to 8 for lower memory usage\n",
    "        lora_alpha=16,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        optimize_memory()\n",
    "\n",
    "        if MODEL_NAME == 'llama3':\n",
    "            # Config for LLama3 model\n",
    "            config = AutoConfig.from_pretrained(HF_MODEL_NAME, num_labels=1)\n",
    "\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "            # prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "            MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "            # get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model\n",
    "            MODEL = get_peft_model(MODEL, lora_config)\n",
    "\n",
    "            # additional memory optimizations\n",
    "            MODEL.gradient_checkpointing_enable()  # Reduce memory usage during training\n",
    "            MODEL.enable_input_require_grads()\n",
    "        \n",
    "        elif MODEL_NAME == 'llama3sp':\n",
    "            # First load the base model\n",
    "            MODEL = LLAMA3SP.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                num_labels=1, # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "\n",
    "            # Prepare the model for training with LoRA\n",
    "            MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "            # get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model\n",
    "            MODEL = get_peft_model(MODEL, lora_config)\n",
    "\n",
    "            # Memory optimizations\n",
    "            MODEL.gradient_checkpointing_enable()\n",
    "            MODEL.enable_input_require_grads()\n",
    "\n",
    "        if TOKENIZER == 'wordlevel':\n",
    "            MODEL.config.pad_token_id = 3\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        \n",
    "        MODEL.cuda()\n",
    "\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        optimize_memory()\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Within Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT, BATCH_SIZE_RATIO\n",
    "WITHIN_PROJECT = False\n",
    "BATCH_SIZE_RATIO = 0.4\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['mesos'], 'test': ['usergrid']},\n",
    "                        {'train': ['usergrid'], 'test': ['mesos']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['titanium']},\n",
    "                        {'train': ['titanium'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['aptanastudio'], 'test': ['titanium']},\n",
    "                        {'train': ['mule'], 'test': ['mulestudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME, HF_MODEL_NAME\n",
    "    \n",
    "    # Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
    "    # Quantization Config (for QLORA)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16, # Changed to float16 for lower memory usage\n",
    "    )\n",
    "    # Lora Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Reduced from 16 to 8 for lower memory usage\n",
    "        lora_alpha=16,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        optimize_memory()\n",
    "\n",
    "        if MODEL_NAME == 'llama3':\n",
    "            # Config for LLama3 model\n",
    "            config = AutoConfig.from_pretrained(HF_MODEL_NAME, num_labels=1)\n",
    "\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1,  # Para regresiÃ³n\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "            # prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "            MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "            # get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model\n",
    "            MODEL = get_peft_model(MODEL, lora_config)\n",
    "\n",
    "            # additional memory optimizations\n",
    "            MODEL.gradient_checkpointing_enable()  # Reduce memory usage during training\n",
    "            MODEL.enable_input_require_grads()\n",
    "\n",
    "        if TOKENIZER == 'wordlevel':\n",
    "            MODEL.config.pad_token_id = 3\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "\n",
    "        MODEL.cuda()\n",
    "\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL\n",
    "        optimize_memory()\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Project Training Script - Cross Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global WITHIN_PROJECT, BATCH_SIZE_RATIO\n",
    "WITHIN_PROJECT = False\n",
    "BATCH_SIZE_RATIO = 0.4\n",
    "\n",
    "TRAIN_TEST_FILE_PAIRS = [\n",
    "                        {'train': ['clover'], 'test': ['usergrid']},\n",
    "                        {'train': ['talendesb'], 'test': ['mesos']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['aptanastudio']},\n",
    "                        {'train': ['mule'], 'test': ['titanium']},\n",
    "                        {'train': ['talenddataquality'], 'test': ['appceleratorstudio']},\n",
    "                        {'train': ['mulestudio'], 'test': ['titanium']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mulestudio']},\n",
    "                        {'train': ['appceleratorstudio'], 'test': ['mule']}\n",
    "                        ]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global TRAIN_TEST_FILE_PAIRS, MODEL, TOKENIZER, MODEL_NAME, HF_MODEL_NAME\n",
    "    \n",
    "    # Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
    "    # Quantization Config (for QLORA)\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # Cambiado a float16 para menor uso de memoria\n",
    "    )\n",
    "    # Lora Config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # Reduced from 16 to 8 for lower memory usage\n",
    "        lora_alpha=16,\n",
    "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout=0.1,\n",
    "        bias='none',\n",
    "        task_type='SEQ_CLS'\n",
    "    )\n",
    "\n",
    "    for file in TRAIN_TEST_FILE_PAIRS:\n",
    "        optimize_memory()\n",
    "\n",
    "        if MODEL_NAME == 'llama3':\n",
    "            # Config for LLama3 model\n",
    "            config = AutoConfig.from_pretrained(HF_MODEL_NAME, num_labels=1)\n",
    "\n",
    "            MODEL = AutoModelForSequenceClassification.from_pretrained(\n",
    "                HF_MODEL_NAME,\n",
    "                quantization_config=quantization_config,\n",
    "                # num_labels=1,  # For regression\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto',\n",
    "                low_cpu_mem_usage=True,\n",
    "                config=config,\n",
    "            )\n",
    "            # prepare_model_for_kbit_training() function to preprocess the quantized model for training.\n",
    "            MODEL = prepare_model_for_kbit_training(MODEL)\n",
    "            # get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model\n",
    "            MODEL = get_peft_model(MODEL, lora_config)\n",
    "\n",
    "            # additional memory optimizations\n",
    "            MODEL.gradient_checkpointing_enable()  # Reduce memory usage during training\n",
    "            MODEL.enable_input_require_grads()\n",
    "\n",
    "        if TOKENIZER == 'wordlevel':\n",
    "            MODEL.config.pad_token_id = 3\n",
    "        elif TOKENIZER == 'sentencepiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "        elif TOKENIZER == 'wordpiece':\n",
    "            MODEL.config.pad_token_id = 0\n",
    "\n",
    "        MODEL.cuda()\n",
    "        file_pair, train_dataloader, val_dataloader, all_test_dataloader, test_file_names = data_processing(file_pair=file)\n",
    "        train_eval_test(file_pair, train_dataloader, val_dataloader, all_test_dataloader, MODEL, test_file_names)\n",
    "        del MODEL \n",
    "        optimize_memory()\n",
    "        torch.cuda.empty_cache()            \n",
    "        global OUTPUT\n",
    "        with open('./results/' + str(file['train'][0]) + '_' + str(file['test'][0]) +'.txt', 'w+') as f:\n",
    "            f.writelines(OUTPUT)\n",
    "            print('results have been written into a text file!')\n",
    "            OUTPUT = \"\"\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
